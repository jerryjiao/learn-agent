{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "64f7fbb4-e992-4d5d-b7c7-c2309fc0bc31",
      "metadata": {
        "id": "64f7fbb4-e992-4d5d-b7c7-c2309fc0bc31"
      },
      "source": [
        "# ä½¿ç”¨LLaMA Factoryå¾®è°ƒQwen3æ¨¡å‹\n",
        "\n",
        "[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)æ˜¯ä¸€æ¬¾å¼€æºä½ä»£ç å¤§æ¨¡å‹å¾®è°ƒæ¡†æ¶ï¼Œé›†æˆäº†ä¸šç•Œæœ€å¹¿æ³›ä½¿ç”¨çš„å¾®è°ƒæŠ€æœ¯ï¼Œæ”¯æŒé€šè¿‡Web UIç•Œé¢é›¶ä»£ç å¾®è°ƒå¤§æ¨¡å‹ï¼Œç›®å‰å·²ç»æˆä¸ºå¼€æºç¤¾åŒºå†…æœ€å—æ¬¢è¿çš„å¾®è°ƒæ¡†æ¶ï¼ŒGitHubæ˜Ÿæ ‡è¶…è¿‡2ä¸‡ã€‚\n",
        "æœ¬æ•™ç¨‹å°†åŸºäºå¼€æºçš„Qwen3 8Bæ¨¡å‹ï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨PAIå¹³å°åŠLLaMA Factoryè®­ç»ƒæ¡†æ¶å®Œæˆä¸­æ–‡åŒ»ç–—é¢†åŸŸçš„å¾®è°ƒå’Œè¯„ä¼°ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe138c06-db8f-4422-b502-62bfcfca19e2",
      "metadata": {
        "tags": [],
        "id": "fe138c06-db8f-4422-b502-62bfcfca19e2"
      },
      "source": [
        "## è¿è¡Œç¯å¢ƒè¦æ±‚\n",
        "\n",
        "### 1. é˜¿é‡Œäº‘ PAI-DSW ç¯å¢ƒé…ç½®\n",
        "\n",
        "æœ¬æ•™ç¨‹åœ¨é˜¿é‡Œäº‘ PAI-DSW å¹³å°ä¸Šå¼€å‘å’Œæµ‹è¯•ï¼Œæ¨èä½¿ç”¨ä»¥ä¸‹é…ç½®ï¼š\n",
        "\n",
        "| é…ç½®é¡¹ | è§„æ ¼è¯´æ˜ | å¤‡æ³¨ |\n",
        "|--------|---------|------|\n",
        "| **GPU ç±»å‹** | NVIDIA A10 | å®ä¾‹è§„æ ¼ï¼š`ecs.gn7i-c8g1.2xlarge` |\n",
        "| **GPU æ˜¾å­˜** | 24GB | ç”¨äºåŠ è½½æ¨¡å‹å’Œè®­ç»ƒæ•°æ® |\n",
        "| **å®˜æ–¹é•œåƒ** | `pytorch:2.6.0-gpu-py310-cu124-ubuntu22.04` | PAI-DSW é¢„é…ç½®é•œåƒ |\n",
        "| **æ“ä½œç³»ç»Ÿ** | Ubuntu 22.04 LTS | é•¿æœŸæ”¯æŒç‰ˆæœ¬ |\n",
        "| **CUDA ç‰ˆæœ¬** | 12.4 | ä¸ PyTorch 2.6.0 å…¼å®¹ |\n",
        "| **Python ç‰ˆæœ¬** | 3.10.x | æ¨èä½¿ç”¨ Python 3.10 æˆ– 3.11 |\n",
        "| **PyTorch ç‰ˆæœ¬** | 2.6.0 | æ·±åº¦å­¦ä¹ æ¡†æ¶ |\n",
        "| **ç£ç›˜ç©ºé—´** | â‰¥ 100GB | ç”¨äºå­˜å‚¨æ¨¡å‹ã€æ•°æ®é›†å’Œæ£€æŸ¥ç‚¹ |\n",
        "\n",
        "### 2. ç§æœ‰ç¯å¢ƒéƒ¨ç½²è¦æ±‚\n",
        "\n",
        "å¦‚æœæ‚¨éœ€è¦åœ¨æœ¬åœ°æœåŠ¡å™¨æˆ–ç§æœ‰äº‘ç¯å¢ƒä¸­éƒ¨ç½²ï¼Œè¯·ç¡®ä¿æ»¡è¶³ä»¥ä¸‹è½¯ç¡¬ä»¶è¦æ±‚ï¼š\n",
        "\n",
        "#### 2.1 ç¡¬ä»¶è¦æ±‚\n",
        "\n",
        "| ç¡¬ä»¶é…ç½® | æœ€ä½è¦æ±‚ | æ¨èé…ç½® | è¯´æ˜ |\n",
        "|---------|---------|---------|------|\n",
        "| **GPU** | NVIDIA RTX 3090 (24GB) | NVIDIA A10/A100 (â‰¥24GB) | å¿…é¡»æ”¯æŒ CUDAï¼Œæ˜¾å­˜è‡³å°‘ 24GB |\n",
        "| **GPU æ˜¾å­˜** | 24GB | 40GB+ | Qwen3 8B æ¨¡å‹ä½¿ç”¨ LoRA å¾®è°ƒéœ€è¦çº¦ 20-24GB |\n",
        "| **CPU** | 8æ ¸å¿ƒ | 16æ ¸å¿ƒ+ | ç”¨äºæ•°æ®é¢„å¤„ç†å’Œæ¨¡å‹æ¨ç† |\n",
        "| **å†…å­˜ (RAM)** | 32GB | 64GB+ | é¿å…æ•°æ®åŠ è½½æ—¶çš„å†…å­˜æº¢å‡º |\n",
        "| **ç£ç›˜ç©ºé—´** | 100GB | 500GB+ SSD | æ¨¡å‹æ–‡ä»¶çº¦ 15GBï¼Œæ•°æ®é›†å’Œæ—¥å¿—éœ€é¢å¤–ç©ºé—´ |\n",
        "\n",
        "#### 2.2 è½¯ä»¶è¦æ±‚\n",
        "\n",
        "| è½¯ä»¶ç»„ä»¶ | ç‰ˆæœ¬è¦æ±‚ | å®‰è£…å‘½ä»¤/è¯´æ˜ |\n",
        "|---------|---------|--------------|\n",
        "| **æ“ä½œç³»ç»Ÿ** | Ubuntu 20.04 / 22.04 LTS<br>CentOS 7/8<br>Windows 10/11 (WSL2) | æ¨èä½¿ç”¨ Ubuntu 22.04 LTS |\n",
        "| **Python** | 3.10.x / 3.11.x | `python --version` æ£€æŸ¥ç‰ˆæœ¬ |\n",
        "| **CUDA Toolkit** | 12.1 / 12.2 / 12.4 | [NVIDIA å®˜æ–¹ä¸‹è½½](https://developer.nvidia.com/cuda-downloads) |\n",
        "| **cuDNN** | ä¸ CUDA ç‰ˆæœ¬åŒ¹é… | é€šå¸¸éš CUDA Toolkit å®‰è£… |\n",
        "| **PyTorch** | â‰¥ 2.3.0 | `pip install torch>=2.3.0` |\n",
        "| **NVIDIA Driver** | â‰¥ 525.x (for CUDA 12.x) | `nvidia-smi` æ£€æŸ¥é©±åŠ¨ç‰ˆæœ¬ |\n",
        "| **Git** | ä»»æ„ç‰ˆæœ¬ | ç”¨äºå…‹éš† LLaMA Factory ä»“åº“ |\n",
        "\n",
        "#### 2.3 ç¯å¢ƒéªŒè¯å‘½ä»¤\n",
        "\n",
        "åœ¨å¼€å§‹å¾®è°ƒä¹‹å‰ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤éªŒè¯ç¯å¢ƒé…ç½®ï¼š\n",
        "\n",
        "```bash\n",
        "# æ£€æŸ¥ NVIDIA é©±åŠ¨å’Œ GPU\n",
        "nvidia-smi\n",
        "\n",
        "# æ£€æŸ¥ CUDA ç‰ˆæœ¬\n",
        "nvcc --version\n",
        "\n",
        "# æ£€æŸ¥ Python ç‰ˆæœ¬\n",
        "python --version\n",
        "\n",
        "# æ£€æŸ¥ PyTorch å’Œ CUDA æ˜¯å¦æ­£ç¡®å®‰è£…\n",
        "python -c \"import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA Available: {torch.cuda.is_available()}'); print(f'CUDA Version: {torch.version.cuda}')\"\n",
        "```\n",
        "\n",
        "#### 2.4 ç½‘ç»œè¦æ±‚\n",
        "\n",
        "| ç½‘ç»œéœ€æ±‚ | è¯´æ˜ |\n",
        "|---------|------|\n",
        "| **æ¨¡å‹ä¸‹è½½** | éœ€è®¿é—® Hugging Face Hub æˆ– ModelScope<br>Qwen3-8B æ¨¡å‹çº¦ 15GB |\n",
        "| **ä¾èµ–å®‰è£…** | éœ€è®¿é—® PyPI é•œåƒæº (å¯ä½¿ç”¨é˜¿é‡Œäº‘/æ¸…åé•œåƒåŠ é€Ÿ) |\n",
        "| **é˜²ç«å¢™é…ç½®** | å¦‚ä½¿ç”¨ SSH è¿œç¨‹è®¿é—®ï¼Œéœ€å¼€æ”¾ç›¸åº”ç«¯å£ |\n",
        "\n",
        "---\n",
        "\n",
        "> **ğŸ’¡ æç¤º**ï¼šå¦‚æœæ‚¨çš„ GPU æ˜¾å­˜ä¸è¶³ 24GBï¼Œå¯ä»¥è€ƒè™‘ï¼š\n",
        "> - ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚ Qwen3-1.8B æˆ– Qwen3-4Bï¼‰\n",
        "> - å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰é™ä½æ˜¾å­˜å ç”¨\n",
        "> - å‡å°æ‰¹æ¬¡å¤§å°ï¼ˆBatch Sizeï¼‰\n",
        "> - ä½¿ç”¨ QLoRAï¼ˆé‡åŒ– LoRAï¼‰è¿›ä¸€æ­¥é™ä½æ˜¾å­˜éœ€æ±‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd9fdf00-1e2c-45b9-8d56-6302d943f0f4",
      "metadata": {
        "tags": [],
        "id": "bd9fdf00-1e2c-45b9-8d56-6302d943f0f4"
      },
      "source": [
        "## ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd803045-f131-448a-8023-de15d7574774",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-05T08:19:37.788954Z",
          "iopub.status.busy": "2025-12-05T08:19:37.788776Z",
          "iopub.status.idle": "2025-12-05T08:19:38.827325Z",
          "shell.execute_reply": "2025-12-05T08:19:38.826695Z",
          "shell.execute_reply.started": "2025-12-05T08:19:37.788932Z"
        },
        "tags": [],
        "id": "fd803045-f131-448a-8023-de15d7574774",
        "outputId": "c0220e43-8e0f-41c1-a139-9354e55b4c70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.10/site-packages (2.2.2)\n",
            "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/site-packages (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/site-packages (from pandas==2.2.2) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "### ç¯å¢ƒä¿¡æ¯\n",
            "| é¡¹ç›®         | ä¿¡æ¯                                                                              |\n",
            "|:-------------|:----------------------------------------------------------------------------------|\n",
            "| æ“ä½œç³»ç»Ÿ     | Linux 5.10.134-18.0.4.lifsea8.x86_64                                              |\n",
            "| CPU ä¿¡æ¯     | Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz (4 physical cores, 8 logical cores) |\n",
            "| å†…å­˜ä¿¡æ¯     | 29.37 GB (Available: 27.89 GB)                                                    |\n",
            "| GPU ä¿¡æ¯     | NVIDIA A10 (24564 MiB)                                                            |\n",
            "| CUDA ä¿¡æ¯    | 12.4                                                                              |\n",
            "| Python ç‰ˆæœ¬  | 3.10.19                                                                           |\n",
            "| Conda ç‰ˆæœ¬   | Conda not found                                                                   |\n",
            "| ç‰©ç†ç£ç›˜ç©ºé—´ | Total: 97.87 GB, Used: 24.70 GB, Free: 73.15 GB                                   |\n"
          ]
        }
      ],
      "source": [
        "# ğŸ” ç¯å¢ƒä¿¡æ¯æ£€æŸ¥è„šæœ¬\n",
        "#\n",
        "# æœ¬è„šæœ¬çš„ä½œç”¨ï¼š\n",
        "# 1. å®‰è£… pandas åº“ç”¨äºæ•°æ®è¡¨æ ¼å±•ç¤º\n",
        "# 2. æ£€æŸ¥ç³»ç»Ÿçš„å„é¡¹é…ç½®ä¿¡æ¯\n",
        "# 3. ç”Ÿæˆè¯¦ç»†çš„ç¯å¢ƒæŠ¥å‘Šè¡¨æ ¼\n",
        "#\n",
        "# å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œè¿™ä¸ªæ­¥éª¤å¸®åŠ©ä½ ï¼š\n",
        "# - äº†è§£å½“å‰è¿è¡Œç¯å¢ƒçš„ç¡¬ä»¶é…ç½®\n",
        "# - ç¡®è®¤æ˜¯å¦æ»¡è¶³æ¨¡å‹è¿è¡Œçš„æœ€ä½è¦æ±‚\n",
        "# - å­¦ä¹ å¦‚ä½•é€šè¿‡ä»£ç è·å–ç³»ç»Ÿä¿¡æ¯\n",
        "\n",
        "# å®‰è£… pandas åº“ - ç”¨äºåˆ›å»ºå’Œå±•ç¤ºæ•°æ®è¡¨æ ¼\n",
        "# pandas æ˜¯ Python ä¸­æœ€æµè¡Œçš„æ•°æ®å¤„ç†å’Œåˆ†æåº“\n",
        "%pip install pandas==2.2.2 tabulate==0.9.0\n",
        "\n",
        "import platform # å¯¼å…¥ platform æ¨¡å—ä»¥è·å–ç³»ç»Ÿä¿¡æ¯\n",
        "import os # å¯¼å…¥ os æ¨¡å—ä»¥ä¸æ“ä½œç³»ç»Ÿäº¤äº’\n",
        "import subprocess # å¯¼å…¥ subprocess æ¨¡å—ä»¥è¿è¡Œå¤–éƒ¨å‘½ä»¤\n",
        "import pandas as pd # å¯¼å…¥ pandas æ¨¡å—ï¼Œé€šå¸¸ç”¨äºæ•°æ®å¤„ç†ï¼Œè¿™é‡Œç”¨äºåˆ›å»ºè¡¨æ ¼\n",
        "import shutil # å¯¼å…¥ shutil æ¨¡å—ä»¥è·å–ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
        "\n",
        "# è·å– CPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ ¸å¿ƒæ•°é‡\n",
        "def get_cpu_info():\n",
        "    cpu_info = \"\" # åˆå§‹åŒ– CPU ä¿¡æ¯å­—ç¬¦ä¸²\n",
        "    physical_cores = \"N/A\"\n",
        "    logical_cores = \"N/A\"\n",
        "\n",
        "    if platform.system() == \"Windows\": # å¦‚æœæ˜¯ Windows ç³»ç»Ÿ\n",
        "        cpu_info = platform.processor() # ä½¿ç”¨ platform.processor() è·å– CPU ä¿¡æ¯\n",
        "        try:\n",
        "            # è·å– Windows ä¸Šçš„æ ¸å¿ƒæ•°é‡ (éœ€è¦ WMI)\n",
        "            import wmi\n",
        "            c = wmi.WMI()\n",
        "            for proc in c.Win32_Processor():\n",
        "                physical_cores = proc.NumberOfCores\n",
        "                logical_cores = proc.NumberOfLogicalProcessors\n",
        "        except:\n",
        "            pass # å¦‚æœ WMI ä¸å¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯\n",
        "\n",
        "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
        "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
        "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # æ›´æ–° PATH ç¯å¢ƒå˜é‡\n",
        "        try:\n",
        "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            stdout_brand, stderr_brand = process_brand.communicate()\n",
        "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
        "\n",
        "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            stdout_physical, stderr_physical = process_physical.communicate()\n",
        "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
        "\n",
        "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            stdout_logical, stderr_logical = process_logical.communicate()\n",
        "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
        "\n",
        "        except:\n",
        "            cpu_info = \"Could not retrieve CPU info\"\n",
        "            physical_cores = \"N/A\"\n",
        "            logical_cores = \"N/A\"\n",
        "\n",
        "    else:  # Linux ç³»ç»Ÿ\n",
        "        try:\n",
        "            # åœ¨ Linux ä¸Šè¯»å– /proc/cpuinfo æ–‡ä»¶è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
        "            with open('/proc/cpuinfo') as f:\n",
        "                physical_cores_count = 0\n",
        "                logical_cores_count = 0\n",
        "                cpu_info_lines = []\n",
        "                for line in f:\n",
        "                    if line.startswith('model name'): # æŸ¥æ‰¾ä»¥ 'model name'å¼€å¤´çš„è¡Œ\n",
        "                        if not cpu_info: # åªè·å–ç¬¬ä¸€ä¸ª model name\n",
        "                            cpu_info = line.split(': ')[1].strip()\n",
        "                    elif line.startswith('cpu cores'): # æŸ¥æ‰¾ä»¥ 'cpu cores' å¼€å¤´çš„è¡Œ\n",
        "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
        "                    elif line.startswith('processor'): # æŸ¥æ‰¾ä»¥ 'processor' å¼€å¤´çš„è¡Œ\n",
        "                        logical_cores_count += 1\n",
        "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
        "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
        "                if not cpu_info:\n",
        "                     cpu_info = \"Could not retrieve CPU info\"\n",
        "\n",
        "        except:\n",
        "            cpu_info = \"Could not retrieve CPU info\"\n",
        "            physical_cores = \"N/A\"\n",
        "            logical_cores = \"N/A\"\n",
        "\n",
        "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # è¿”å› CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
        "\n",
        "\n",
        "# è·å–å†…å­˜ä¿¡æ¯çš„å‡½æ•°\n",
        "def get_memory_info():\n",
        "    mem_info = \"\" # åˆå§‹åŒ–å†…å­˜ä¿¡æ¯å­—ç¬¦ä¸²\n",
        "    if platform.system() == \"Windows\":\n",
        "        # åœ¨ Windows ä¸Šä¸å®¹æ˜“é€šè¿‡æ ‡å‡†åº“è·å–ï¼Œéœ€è¦å¤–éƒ¨åº“æˆ– PowerShell\n",
        "        mem_info = \"Requires external tools on Windows\" # è®¾ç½®æç¤ºä¿¡æ¯\n",
        "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
        "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å–å†…å­˜å¤§å°\n",
        "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # è¿è¡Œ sysctl å‘½ä»¤\n",
        "        stdout, stderr = process.communicate() # è·å–æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯\n",
        "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # è§£æè¾“å‡ºï¼Œè·å–å†…å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰\n",
        "        mem_gb = mem_bytes / (1024**3) # è½¬æ¢ä¸º GB\n",
        "        mem_info = f\"{mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
        "    else:  # Linux ç³»ç»Ÿ\n",
        "        try:\n",
        "            # åœ¨ Linux ä¸Šè¯»å– /proc/meminfo æ–‡ä»¶è·å–å†…å­˜ä¿¡æ¯\n",
        "            with open('/proc/meminfo') as f:\n",
        "                total_mem_kb = 0\n",
        "                available_mem_kb = 0\n",
        "                for line in f:\n",
        "                    if line.startswith('MemTotal'): # æŸ¥æ‰¾ä»¥ 'MemTotal' å¼€å¤´çš„è¡Œ\n",
        "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–æ€»å†…å­˜ï¼ˆKBï¼‰\n",
        "                    elif line.startswith('MemAvailable'): # æŸ¥æ‰¾ä»¥ 'MemAvailable' å¼€å¤´çš„è¡Œ\n",
        "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–å¯ç”¨å†…å­˜ï¼ˆKBï¼‰\n",
        "\n",
        "                if total_mem_kb > 0:\n",
        "                    total_mem_gb = total_mem_kb / (1024**2) # è½¬æ¢ä¸º GB\n",
        "                    mem_info = f\"{total_mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡ºæ€»å†…å­˜\n",
        "                    if available_mem_kb > 0:\n",
        "                        available_mem_gb = available_mem_kb / (1024**2)\n",
        "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # æ·»åŠ å¯ç”¨å†…å­˜ä¿¡æ¯\n",
        "                else:\n",
        "                     mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
        "\n",
        "        except:\n",
        "            mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
        "    return mem_info # è¿”å›å†…å­˜ä¿¡æ¯\n",
        "\n",
        "# è·å– GPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ˜¾å­˜\n",
        "def get_gpu_info():\n",
        "    try:\n",
        "        # å°è¯•ä½¿ç”¨ nvidia-smi è·å– NVIDIA GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
        "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
        "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
        "            gpu_lines = result.stdout.strip().split('\\n') # è§£æè¾“å‡ºï¼Œè·å– GPU åç§°å’Œæ˜¾å­˜\n",
        "            gpu_info_list = []\n",
        "            for line in gpu_lines:\n",
        "                name, memory = line.split(', ')\n",
        "                gpu_info_list.append(f\"{name} ({memory})\") # æ ¼å¼åŒ– GPU ä¿¡æ¯\n",
        "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # è¿”å› GPU ä¿¡æ¯æˆ–æç¤ºä¿¡æ¯\n",
        "        else:\n",
        "             # å°è¯•ä½¿ç”¨ lshw è·å–å…¶ä»– GPU ä¿¡æ¯ (éœ€è¦å®‰è£… lshw)\n",
        "            try:\n",
        "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
        "                if result_lshw.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
        "                     # ç®€å•è§£æè¾“å‡ºä¸­çš„ product åç§°å’Œæ˜¾å­˜\n",
        "                    gpu_info_lines = []\n",
        "                    current_gpu = {}\n",
        "                    for line in result_lshw.stdout.splitlines():\n",
        "                        if 'product:' in line:\n",
        "                             if current_gpu:\n",
        "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
        "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
        "                        elif 'size:' in line and 'memory' in line:\n",
        "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
        "\n",
        "                    if current_gpu: # æ·»åŠ æœ€åä¸€ä¸ª GPU çš„ä¿¡æ¯\n",
        "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
        "\n",
        "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # å¦‚æœæ‰¾åˆ° GPU ä½†ä¿¡æ¯æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "                else:\n",
        "                    return \"No GPU found (checked nvidia-smi and lshw)\" # å¦‚æœä¸¤ä¸ªå‘½ä»¤éƒ½æ‰¾ä¸åˆ° GPUï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "            except FileNotFoundError:\n",
        "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # å¦‚æœæ‰¾ä¸åˆ° lshw å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "    except FileNotFoundError:\n",
        "        return \"No GPU found (nvidia-smi not found)\" # å¦‚æœæ‰¾ä¸åˆ° nvidia-smi å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "\n",
        "\n",
        "# è·å– CUDA ç‰ˆæœ¬çš„å‡½æ•°\n",
        "def get_cuda_version():\n",
        "    try:\n",
        "        # å°è¯•ä½¿ç”¨ nvcc --version è·å– CUDA ç‰ˆæœ¬\n",
        "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
        "            for line in result.stdout.splitlines():\n",
        "                if 'release' in line: # æŸ¥æ‰¾åŒ…å« 'release' çš„è¡Œ\n",
        "                    return line.split('release ')[1].split(',')[0] # è§£æè¡Œï¼Œæå–ç‰ˆæœ¬å·\n",
        "        return \"CUDA not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° CUDA æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "    except FileNotFoundError:\n",
        "        return \"CUDA not found\" # å¦‚æœæ‰¾ä¸åˆ° nvcc å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "\n",
        "# è·å– Python ç‰ˆæœ¬çš„å‡½æ•°\n",
        "def get_python_version():\n",
        "    return platform.python_version() # è·å– Python ç‰ˆæœ¬\n",
        "\n",
        "# è·å– Conda ç‰ˆæœ¬çš„å‡½æ•°\n",
        "def get_conda_version():\n",
        "    try:\n",
        "        # å°è¯•ä½¿ç”¨ conda --version è·å– Conda ç‰ˆæœ¬\n",
        "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
        "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
        "            return result.stdout.strip() # è¿”å› Conda ç‰ˆæœ¬\n",
        "        return \"Conda not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° Conda æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "    except FileNotFoundError:\n",
        "        return \"Conda not found\" # å¦‚æœæ‰¾ä¸åˆ° conda å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
        "\n",
        "# è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯çš„å‡½æ•°\n",
        "def get_disk_space():\n",
        "    try:\n",
        "        total, used, free = shutil.disk_usage(\"/\") # è·å–æ ¹ç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ\n",
        "        total_gb = total / (1024**3) # è½¬æ¢ä¸º GB\n",
        "        used_gb = used / (1024**3) # è½¬æ¢ä¸º GB\n",
        "        free_gb = free / (1024**3) # è½¬æ¢ä¸º GB\n",
        "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
        "    except Exception as e:\n",
        "        return f\"Could not retrieve disk info: {e}\" # å¦‚æœè·å–ä¿¡æ¯å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
        "\n",
        "# è·å–ç¯å¢ƒä¿¡æ¯\n",
        "os_name = platform.system() # è·å–æ“ä½œç³»ç»Ÿåç§°\n",
        "os_version = platform.release() # è·å–æ“ä½œç³»ç»Ÿç‰ˆæœ¬\n",
        "if os_name == \"Linux\":\n",
        "    try:\n",
        "        # åœ¨ Linux ä¸Šå°è¯•è·å–å‘è¡Œç‰ˆå’Œç‰ˆæœ¬\n",
        "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
        "        if lsb_info.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
        "            for line in lsb_info.stdout.splitlines():\n",
        "                if 'Description:' in line: # æŸ¥æ‰¾åŒ…å« 'Description:' çš„è¡Œ\n",
        "                    os_version = line.split('Description:')[1].strip() # æå–æè¿°ä¿¡æ¯ä½œä¸ºç‰ˆæœ¬\n",
        "                    break # æ‰¾åˆ°åé€€å‡ºå¾ªç¯\n",
        "                elif 'Release:' in line: # æŸ¥æ‰¾åŒ…å« 'Release:' çš„è¡Œ\n",
        "                     os_version = line.split('Release:')[1].strip() # æå–ç‰ˆæœ¬å·\n",
        "                     # å°è¯•è·å– codename\n",
        "                     try:\n",
        "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
        "                         if codename_info.returncode == 0:\n",
        "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # å°† codename æ·»åŠ åˆ°ç‰ˆæœ¬ä¿¡æ¯ä¸­\n",
        "                     except:\n",
        "                         pass # å¦‚æœè·å– codename å¤±è´¥åˆ™å¿½ç•¥\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        pass # lsb_release å¯èƒ½æœªå®‰è£…ï¼Œå¿½ç•¥é”™è¯¯\n",
        "\n",
        "full_os_info = f\"{os_name} {os_version}\" # ç»„åˆå®Œæ•´çš„æ“ä½œç³»ç»Ÿä¿¡æ¯\n",
        "cpu_info = get_cpu_info() # è°ƒç”¨å‡½æ•°è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
        "memory_info = get_memory_info() # è°ƒç”¨å‡½æ•°è·å–å†…å­˜ä¿¡æ¯\n",
        "gpu_info = get_gpu_info() # è°ƒç”¨å‡½æ•°è·å– GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
        "cuda_version = get_cuda_version() # è°ƒç”¨å‡½æ•°è·å– CUDA ç‰ˆæœ¬\n",
        "python_version = get_python_version() # è°ƒç”¨å‡½æ•°è·å– Python ç‰ˆæœ¬\n",
        "conda_version = get_conda_version() # è°ƒç”¨å‡½æ•°è·å– Conda ç‰ˆæœ¬\n",
        "disk_info = get_disk_space() # è°ƒç”¨å‡½æ•°è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
        "\n",
        "\n",
        "# åˆ›å»ºç”¨äºå­˜å‚¨æ•°æ®çš„å­—å…¸\n",
        "env_data = {\n",
        "    \"é¡¹ç›®\": [ # é¡¹ç›®åç§°åˆ—è¡¨\n",
        "        \"æ“ä½œç³»ç»Ÿ\",\n",
        "        \"CPU ä¿¡æ¯\",\n",
        "        \"å†…å­˜ä¿¡æ¯\",\n",
        "        \"GPU ä¿¡æ¯\",\n",
        "        \"CUDA ä¿¡æ¯\",\n",
        "        \"Python ç‰ˆæœ¬\",\n",
        "        \"Conda ç‰ˆæœ¬\",\n",
        "        \"ç‰©ç†ç£ç›˜ç©ºé—´\" # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´\n",
        "    ],\n",
        "    \"ä¿¡æ¯\": [ # å¯¹åº”çš„ä¿¡æ¯åˆ—è¡¨\n",
        "        full_os_info,\n",
        "        cpu_info,\n",
        "        memory_info,\n",
        "        gpu_info,\n",
        "        cuda_version,\n",
        "        python_version,\n",
        "        conda_version,\n",
        "        disk_info # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
        "    ]\n",
        "}\n",
        "\n",
        "# åˆ›å»ºä¸€ä¸ª pandas DataFrame\n",
        "df = pd.DataFrame(env_data)\n",
        "\n",
        "# æ‰“å°è¡¨æ ¼\n",
        "print(\"### ç¯å¢ƒä¿¡æ¯\") # æ‰“å°æ ‡é¢˜\n",
        "print(df.to_markdown(index=False)) # å°† DataFrame è½¬æ¢ä¸º Markdown æ ¼å¼å¹¶æ‰“å°ï¼Œä¸åŒ…å«ç´¢å¼•"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a50ba54e-fade-4d44-ad4b-6545643bc15c",
      "metadata": {
        "tags": [],
        "id": "a50ba54e-fade-4d44-ad4b-6545643bc15c"
      },
      "source": [
        "## æ£€æŸ¥è½¯ç¡¬ä»¶ä¾èµ–\n",
        "\n",
        "| å¿…éœ€é¡¹       | è‡³å°‘   | æ¨è   |\n",
        "| ------------ | ------ | ------ |\n",
        "| python       | 3.9    | 3.10   |\n",
        "| torch        | 2.0.0  | 2.6.0  |\n",
        "| torchvision  | 0.15.0 | 0.21.0 |\n",
        "| transformers | 4.45.0 | 4.50.0 |\n",
        "| datasets     | 2.16.0 | 3.2.0  |\n",
        "| accelerate   | 0.34.0 | 1.2.1  |\n",
        "| peft         | 0.14.0 | 0.15.1 |\n",
        "| trl          | 0.8.6  | 0.9.6  |\n",
        "\n",
        "| å¯é€‰é¡¹       | è‡³å°‘   | æ¨è   |\n",
        "| ------------ | ------ | ------ |\n",
        "| CUDA         | 11.6   | 12.2   |\n",
        "| deepspeed    | 0.10.0 | 0.16.4 |\n",
        "| bitsandbytes | 0.39.0 | 0.43.1 |\n",
        "| vllm         | 0.4.3  | 0.8.2  |\n",
        "| flash-attn   | 2.5.6  | 2.7.2  |\n",
        "\n",
        "### ç¡¬ä»¶ä¾èµ–\n",
        "\n",
        "\\* *ä¼°ç®—å€¼*\n",
        "\n",
        "| æ–¹æ³•                            | ç²¾åº¦ | 7B    | 14B   | 30B   | 70B    | `x`B    |\n",
        "| ------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |\n",
        "| Full (`bf16` or `fp16`)         | 32   | 120GB | 240GB | 600GB | 1200GB | `18x`GB |\n",
        "| Full (`pure_bf16`)              | 16   | 60GB  | 120GB | 300GB | 600GB  | `8x`GB  |\n",
        "| Freeze/LoRA/GaLore/APOLLO/BAdam | 16   | 16GB  | 32GB  | 64GB  | 160GB  | `2x`GB  |\n",
        "| QLoRA                           | 8    | 10GB  | 20GB  | 40GB  | 80GB   | `x`GB   |\n",
        "| QLoRA                           | 4    | 6GB   | 12GB  | 24GB  | 48GB   | `x/2`GB |\n",
        "| QLoRA                           | 2    | 4GB   | 8GB   | 16GB  | 24GB   | `x/4`GB |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7aab387-96a7-4d4d-ae9d-e0395dfa0f52",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-05T08:19:38.828380Z",
          "iopub.status.busy": "2025-12-05T08:19:38.828153Z",
          "iopub.status.idle": "2025-12-05T08:19:40.079247Z",
          "shell.execute_reply": "2025-12-05T08:19:40.078586Z",
          "shell.execute_reply.started": "2025-12-05T08:19:38.828360Z"
        },
        "id": "c7aab387-96a7-4d4d-ae9d-e0395dfa0f52",
        "outputId": "88f9f402-ab81-4963-d209-e68864422819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===========================================================================\n",
            "ğŸ› ï¸  ç¯å¢ƒä¾èµ–æ£€æµ‹æŠ¥å‘Š (æ ‡å‡†å·²å†æ¬¡æ›´æ–°)\n",
            "===========================================================================\n",
            "[ å¿…éœ€é¡¹ ]\n",
            "\u001b[92mâœ… python          | å½“å‰: å®Œç¾ (3.10.19)    | å¿…éœ€: >=3.9        | æ¨è: >=3.10\u001b[0m\n",
            "\u001b[92mâœ… torch           | å½“å‰: å®Œç¾ (2.6.0+cu124) | å¿…éœ€: >=2.0.0      | æ¨è: >=2.6.0\u001b[0m\n",
            "\u001b[92mâœ… torchvision     | å½“å‰: å®Œç¾ (0.21.0+cu124) | å¿…éœ€: >=0.15.0     | æ¨è: >=0.21.0\u001b[0m\n",
            "\u001b[91mâŒ transformers    | å½“å‰: æœªå®‰è£… !!          | å¿…éœ€: >=4.45.0     | æ¨è: >=4.50.0\u001b[0m\n",
            "\u001b[91mâŒ datasets        | å½“å‰: æœªå®‰è£… !!          | å¿…éœ€: >=2.16.0     | æ¨è: >=3.2.0\u001b[0m\n",
            "\u001b[91mâŒ accelerate      | å½“å‰: æœªå®‰è£… !!          | å¿…éœ€: >=0.34.0     | æ¨è: >=1.2.1\u001b[0m\n",
            "\u001b[91mâŒ peft            | å½“å‰: æœªå®‰è£… !!          | å¿…éœ€: >=0.14.0     | æ¨è: >=0.15.1\u001b[0m\n",
            "\u001b[91mâŒ trl             | å½“å‰: æœªå®‰è£… !!          | å¿…éœ€: >=0.8.6      | æ¨è: >=0.9.6\u001b[0m\n",
            "\n",
            "[ å¯é€‰é¡¹ ]\n",
            "\u001b[92mâœ… cuda            | å½“å‰: å®Œç¾ (12.4)       | å¿…éœ€: >=11.6       | æ¨è: >=12.2\u001b[0m\n",
            "\u001b[90mâšª deepspeed       | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=0.10.0     | æ¨è: >=0.16.4\u001b[0m\n",
            "\u001b[92mâœ… bitsandbytes    | å½“å‰: å®Œç¾ (0.48.2)     | å¿…éœ€: >=0.39.0     | æ¨è: >=0.43.1\u001b[0m\n",
            "\u001b[90mâšª vllm            | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=0.4.3      | æ¨è: >=0.8.2\u001b[0m\n",
            "\u001b[90mâšª flash-attn      | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=2.5.6      | æ¨è: >=2.7.2\u001b[0m\n",
            "\n",
            "[ ç¡¬ä»¶ç®€æµ‹ ]\n",
            "âœ… æ£€æµ‹åˆ° 1 ä¸ª GPU è®¾å¤‡:\n",
            "   - GPU 0: NVIDIA A10 (23.5 GB VRAM)\n",
            "===========================================================================\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import platform\n",
        "import importlib.metadata\n",
        "from packaging import version\n",
        "import torch\n",
        "\n",
        "# ================= æ‚¨çš„æœ€æ–°é…ç½®è¦æ±‚ (å·²å†æ¬¡æ›´æ–°) =================\n",
        "REQUIRED_PACKAGES = {\n",
        "    \"python\":       (\"3.9\",    \"3.10\"),\n",
        "    \"torch\":        (\"2.0.0\",  \"2.6.0\"),\n",
        "    \"torchvision\":  (\"0.15.0\", \"0.21.0\"),\n",
        "    \"transformers\": (\"4.45.0\", \"4.50.0\"),\n",
        "    \"datasets\":     (\"2.16.0\", \"3.2.0\"),\n",
        "    \"accelerate\":   (\"0.34.0\", \"1.2.1\"),\n",
        "    \"peft\":         (\"0.14.0\", \"0.15.1\"),\n",
        "    \"trl\":          (\"0.8.6\",  \"0.9.6\"),\n",
        "}\n",
        "\n",
        "OPTIONAL_PACKAGES = {\n",
        "    \"cuda\":         (\"11.6\",   \"12.2\"),\n",
        "    \"deepspeed\":    (\"0.10.0\", \"0.16.4\"),\n",
        "    \"bitsandbytes\": (\"0.39.0\", \"0.43.1\"),\n",
        "    \"vllm\":         (\"0.4.3\",  \"0.8.2\"),\n",
        "    \"flash-attn\":   (\"2.5.6\",  \"2.7.2\"),\n",
        "}\n",
        "\n",
        "# ================= æ£€æµ‹é€»è¾‘ =================\n",
        "def get_package_version(package_name):\n",
        "    try:\n",
        "        if package_name == \"python\": return platform.python_version()\n",
        "        elif package_name == \"cuda\": return torch.version.cuda if torch.cuda.is_available() else None\n",
        "        elif package_name == \"flash-attn\": return importlib.metadata.version(\"flash_attn\")\n",
        "        else: return importlib.metadata.version(package_name)\n",
        "    except importlib.metadata.PackageNotFoundError: return None\n",
        "\n",
        "def check_requirement(name, constraints, is_optional=False):\n",
        "    min_v, rec_v = constraints\n",
        "    current_v = get_package_version(name)\n",
        "\n",
        "    # å®šä¹‰åˆ—å®½ä»¥ä¿æŒå¯¹é½\n",
        "    col_name = 15\n",
        "    col_curr = 15\n",
        "    col_req = 10\n",
        "\n",
        "    if current_v is None:\n",
        "        status, msg, color = (\"âšª\", \"æœªå®‰è£… (å¯é€‰)\", \"\\033[90m\") if is_optional else (\"âŒ\", \"æœªå®‰è£… !!\", \"\\033[91m\")\n",
        "    else:\n",
        "        try:\n",
        "            curr_p, min_p, rec_p = version.parse(current_v), version.parse(min_v), version.parse(rec_v)\n",
        "            if curr_p >= rec_p:   status, msg, color = (\"âœ…\", f\"å®Œç¾ ({current_v})\", \"\\033[92m\") # Green\n",
        "            elif curr_p >= min_p: status, msg, color = (\"âš ï¸\", f\"è¾¾æ ‡ ({current_v})\", \"\\033[93m\") # Yellow\n",
        "            else:                 status, msg, color = (\"âŒ\", f\"è¿‡ä½ ({current_v})\", \"\\033[91m\") # Red\n",
        "        except Exception as e:\n",
        "            status, msg, color = (\"â“\", f\"è§£æé”™è¯¯\", \"\\033[93m\")\n",
        "\n",
        "    print(f\"{color}{status} {name:<{col_name}} | å½“å‰: {msg:<{col_curr}} | å¿…éœ€: >={min_v:<{col_req}} | æ¨è: >={rec_v}\\033[0m\")\n",
        "\n",
        "def check_gpu():\n",
        "    print(\"\\n[ ç¡¬ä»¶ç®€æµ‹ ]\")\n",
        "    if torch.cuda.is_available():\n",
        "        device_count = torch.cuda.device_count()\n",
        "        print(f\"âœ… æ£€æµ‹åˆ° {device_count} ä¸ª GPU è®¾å¤‡:\")\n",
        "        for i in range(device_count):\n",
        "            gpu_name = torch.cuda.get_device_name(i)\n",
        "            # è½¬æ¢ä¸º GB\n",
        "            total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
        "            print(f\"   - GPU {i}: {gpu_name} ({total_mem:.1f} GB VRAM)\")\n",
        "    else:\n",
        "        print(\"âšª æœªæ£€æµ‹åˆ°å¯ç”¨ GPU (CUDAä¸å¯ç”¨)\")\n",
        "\n",
        "print(\"=\"*75)\n",
        "print(f\"ğŸ› ï¸  ç¯å¢ƒä¾èµ–æ£€æµ‹æŠ¥å‘Š (æ ‡å‡†å·²å†æ¬¡æ›´æ–°)\")\n",
        "print(\"=\"*75)\n",
        "print(\"[ å¿…éœ€é¡¹ ]\")\n",
        "for k, v in REQUIRED_PACKAGES.items(): check_requirement(k, v)\n",
        "print(\"\\n[ å¯é€‰é¡¹ ]\")\n",
        "for k, v in OPTIONAL_PACKAGES.items(): check_requirement(k, v, True)\n",
        "check_gpu()\n",
        "print(\"=\"*75)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbb110e1-2cf0-4ca4-98bd-d8c38322e944",
      "metadata": {
        "id": "fbb110e1-2cf0-4ca4-98bd-d8c38322e944"
      },
      "source": [
        "## 1. å®‰è£…LLaMA Factory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87e76f4d-15a3-4efe-9a32-190b63972e45",
      "metadata": {
        "id": "87e76f4d-15a3-4efe-9a32-190b63972e45"
      },
      "source": [
        "é¦–å…ˆï¼Œæ‹‰å–LLaMA-Factoryé¡¹ç›®åˆ°DSWå®ä¾‹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38481631-eeea-4d5f-abdc-f9aeeee439ae",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2025-12-05T08:19:40.080100Z",
          "iopub.status.busy": "2025-12-05T08:19:40.079915Z",
          "iopub.status.idle": "2025-12-05T08:19:52.132337Z",
          "shell.execute_reply": "2025-12-05T08:19:52.131563Z",
          "shell.execute_reply.started": "2025-12-05T08:19:40.080082Z"
        },
        "tags": [],
        "id": "38481631-eeea-4d5f-abdc-f9aeeee439ae",
        "outputId": "5c0474a8-1e8c-4113-da26-ba9122fe3e52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… ç›®å½• 'LLaMA-Factory' ä¸å­˜åœ¨ï¼Œå‡†å¤‡ä¸‹è½½ä»“åº“...\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 24605, done.\u001b[K\n",
            "remote: Counting objects: 100% (130/130), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 24605 (delta 67), reused 24 (delta 24), pack-reused 24475 (from 2)\u001b[K\n",
            "Receiving objects: 100% (24605/24605), 12.22 MiB | 1.19 MiB/s, done.\n",
            "Resolving deltas: 100% (17696/17696), done.\n",
            "Note: switching to 'ca75f1edf3cb50343ed1c98605141c3e22075b5f'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# å®šä¹‰ä»“åº“æ–‡ä»¶å¤¹åç§°\n",
        "repo_name = \"LLaMA-Factory\"\n",
        "\n",
        "# 1. åˆ¤æ–­æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨\n",
        "if os.path.exists(repo_name):\n",
        "    # è·å–ç”¨æˆ·è¾“å…¥\n",
        "    user_input = input(f\"âš ï¸ æ£€æµ‹åˆ°æ—§ç›®å½• '{repo_name}'ï¼Œæ˜¯å¦åˆ é™¤ï¼Ÿ(è¾“å…¥ y ç¡®è®¤åˆ é™¤ï¼Œå…¶ä»–é”®å–æ¶ˆ): \")\n",
        "\n",
        "    # åˆ¤æ–­ç”¨æˆ·è¾“å…¥ (æ”¯æŒ y æˆ– Y)\n",
        "    if user_input.lower() == 'y':\n",
        "        print(f\"æ­£åœ¨åˆ é™¤ '{repo_name}'...\")\n",
        "        try:\n",
        "            shutil.rmtree(repo_name) # æ‰§è¡Œå®é™…åˆ é™¤\n",
        "            print(\"ğŸ—‘ï¸ æ—§ç›®å½•å·²æ¸…é™¤ï¼\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ åˆ é™¤å¤±è´¥: {e}\")\n",
        "    else:\n",
        "        print(\"ğŸš« æ“ä½œå·²å–æ¶ˆï¼Œä¿ç•™æ—§ç›®å½•ã€‚\")\n",
        "else:\n",
        "    print(f\"âœ… ç›®å½• '{repo_name}' ä¸å­˜åœ¨ï¼Œå‡†å¤‡ä¸‹è½½ä»“åº“...\")\n",
        "    # å…‹éš†ä»“åº“\n",
        "    !git clone -b v0.9.3 https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca0122b-7f5a-4f45-8e8f-5f3dbcd9ff0f",
      "metadata": {
        "id": "fca0122b-7f5a-4f45-8e8f-5f3dbcd9ff0f"
      },
      "source": [
        "æ¥ç€ï¼Œæˆ‘ä»¬å®‰è£…LLaMA-Factoryä¾èµ–ç¯å¢ƒã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a159cf2-7e19-44e5-8b86-7e8eb437a992",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2025-12-05T08:19:52.133372Z",
          "iopub.status.busy": "2025-12-05T08:19:52.133150Z",
          "iopub.status.idle": "2025-12-05T08:25:25.017274Z",
          "shell.execute_reply": "2025-12-05T08:25:25.016467Z",
          "shell.execute_reply.started": "2025-12-05T08:19:52.133352Z"
        },
        "tags": [],
        "id": "3a159cf2-7e19-44e5-8b86-7e8eb437a992",
        "outputId": "f0a4b8c8-376c-4997-ad15-dc614d492c7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/mnt/workspace/LLaMA-Factory\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (25.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
            "Obtaining file:///mnt/workspace/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/96/f2/25b27b396af03d5b64e61976b14f7209e2939e9e806c10749b6d277c273e/transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting datasets<=3.6.0,>=2.16.0 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/20/34/a08b0ee99715eaba118cbe19a71f7b5e2425c2718ef96007c325944a1152/datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "Collecting accelerate<=1.7.0,>=0.34.0 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/f8/bb/be8146c196ad6e4dec78385d91e92591f8a433576c4e04c342a636fcd811/accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "Collecting peft<=0.15.2,>=0.14.0 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/68/85/8e6ea3d1089f2b6de3c1cd34bbbd7560912af9d34b057be3b8b8fefe1da3/peft-0.15.2-py3-none-any.whl (411 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/a5/c3/6565c2c376a829f99da20d39c2912405195ec1fa6aae068dc45c46793e72/trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "Collecting tokenizers<=0.21.1,>=0.19.0 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/8a/63/38be071b0c8e06840bc6046991636bcb30c27f6bb1e670f4f4bc87cf49cc/tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting gradio<=5.31.0,>=4.38.0 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/86/a5/630f31e90e1a39c8ec3d2a2404e3d4d6a27c9f15faca61cdd54458700757/gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:44\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/8e/6d/41991e503e51fc1134502694c5fa7a1671501a17ffa12716a4a9151af3df/scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:30\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting einops (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/87/62/9773de14fe6c45c23649e98b83231fffd7b9892b6cf863251dc2afa73643/einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Collecting sentencepiece (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/c8/89/8deeafbba2871e8fa10f20f17447786f4ac38085925335728d360eaf4cae/sentencepiece-0.2.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/b2/94/443fab3d4e5ebecac895712abd3849b8da93b7b7dec61c7db5c9c7ebe40c/tiktoken-0.12.0-cp310-cp310-manylinux_2_28_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting protobuf (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/1d/2a/3c5f05a4af06649547027d288747f68525755de692a26a7720dced3652c0/protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "Collecting uvicorn (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/ee/d9/d88e73ca598f4f6ff671fb5fde8a32925c2e08a637303a1d12883c7305fa/uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
            "Collecting fastapi (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/db/15/a785e992a27620e022d0bc61b6c897ec14cff07c5ab7ff9f27651a21570b/fastapi-0.123.9-py3-none-any.whl (111 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/23/a0/984525d19ca5c8a6c33911a0c164b11490dd0f90ff7fd689f704f84e9a11/sse_starlette-3.0.3-py3-none-any.whl (11 kB)\n",
            "Collecting matplotlib>=3.7.0 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/e2/3c/5692a2d9a5ba848fda3f48d2b607037df96460b941a59ef236404b39776b/matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting fire (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/e5/4c/93d0f85318da65923e4b91c1c2ff03d8a458cbefebe3bc612a6693c7906d/fire-0.7.1-py3-none-any.whl (115 kB)\n",
            "Collecting omegaconf (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from llamafactory==0.9.3) (25.0)\n",
            "Collecting pyyaml (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/7a/1e/7acc4f0e74c4b3d9531e24739e0ab832a5edf40e64fbae1a9c01941cabd7/pyyaml-6.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (770 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m770.3/770.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.0.0 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/4b/d7/ecf66c1cd12dc28b4040b15ab4d17b773b87fa9d29ca16125de01adb36cd/numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:14\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<=2.10.6 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/f4/3c/8cc1cc84deffa6e25d2d0c688ebb80635dfdbf1dbea3e30c541c8cf4d860/pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/site-packages (from llamafactory==0.9.3) (2.2.2)\n",
            "Collecting av (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/d4/db/b27bdd20c9dc80de5b8792dae16dd6f4edf16408c0c7b28070c6228a8057/av-16.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (39.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:32\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting librosa (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/b5/ba/c63c5786dfee4c3417094c4b00966e61e4a63efecee22cb7b4c0387dda83/librosa-0.11.0-py3-none-any.whl (260 kB)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/60/ec/e34d546cfd9c5b906d1d534bb75557be9f2b179609d60bb9e97ec07e8ead/tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "Collecting nltk (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/60/90/81ac364ef94209c100e12579629dc92bf7a709a84af32f8c551b02c07e94/nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting jieba (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting rouge-chinese (from llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/03/0f/394cf877be7b903881020ef7217f7dc644dad158d52a9353fcab22e3464d/rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/site-packages (from llamafactory==0.9.3) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.10/site-packages (from llamafactory==0.9.3) (0.21.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/site-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3) (7.1.3)\n",
            "Collecting huggingface-hub>=0.21.0 (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/dd/4f/82e5ab009089a2c48472bf4248391fe4091cf0b9c3e951dbb8afe3b23d76/huggingface_hub-1.1.7-py3-none-any.whl (516 kB)\n",
            "Collecting safetensors>=0.4.3 (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/a0/60/429e9b1cb3fc651937727befe258ea24122d9663e4d5709a48c9cbfceecb/safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (3.19.1)\n",
            "Collecting pyarrow>=15.0.0 (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/46/46/a1d9c24baf21cfd9ce994ac820a24608decf2710521b29223d4334985127/pyarrow-22.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:38\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.9,>=0.3.0 (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (4.67.1)\n",
            "Collecting xxhash (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/2d/4b/55ab404c56cd70a2cf5ecfe484838865d0fea5627365c6c8ca156bd09c8f/xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/bc/f7/7ec7fddc92e50714ea3745631f79bd9c96424cb2702632521028e57d3a36/multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/56/53/eb690efa8513166adef3e0669afd31e95ffde69fb3c52ec2ac7223ed6018/fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/a1/4f/ca2ef819488cbb41844c6cf92ca6dd15b9441e6207c58e5ae0e0fc8d70ad/aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hCollecting aiofiles<25.0,>=22.0 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/a5/45/30bb92d442636f570cb5651bc661f52b610e2eec3f891a5dc3a4c3667db0/aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Collecting anyio<5.0,>=3.0 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/7f/9c/36c5c37947ebfb8c7f22e0eb6e4d188ee2d53aa3880f3f2744fb894f0cb1/anyio-4.12.0-py3-none-any.whl (113 kB)\n",
            "Collecting ffmpy (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/55/56/dd3669eccebb6d8ac81e624542ebd53fe6f08e1b8f2f8d50aeb7e3b83f99/ffmpy-1.0.0-py3-none-any.whl (5.6 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/55/6f/03eb8e0e0ec80eced5ed35a63376dabfc7391b1538502f8e85e9dc5bab02/gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "Collecting groovy~=0.1 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/28/27/3d6dcadc8a3214d8522c1e7f6a19554e33659be44546d44a2f7572ac7d2a/groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (3.1.3)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/c6/3a/b31c8f0182a3e27f48e703f46e61bb769666cd0dac4700a73912d07a1417/orjson-3.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (11.3.0)\n",
            "Collecting pydub (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/96/bc/058fe0aefc0fbf0d19614cb6d1a3e2c048f7dc77ca64957f33b12cfdc5ef/ruff-0.14.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/2e/a3/0f0b7d78e2f1eb9e8e1afbff1d2bff8d60144aee17aca51c065b516743dd/safehttpx-0.1.7-py3-none-any.whl (9.0 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/6a/23/8146aad7d88f4fcb3a6218f41a60f6c2d4e3a72de72da1825dc7c8f7877c/semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/d9/52/1064f510b141bd54025f9b55105e26d1fa970b9be67ad766380a3c9b74b0/starlette-0.50.0-py3-none-any.whl (74 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/bd/75/8539d011f6be8e29f339c42e633aae3cb73bffa95dd0f9adec09b9c58e85/tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
            "Collecting typer<1.0,>=0.12 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/78/64/7713ffe4b5983314e9d436a90d5bd4f63b6054e2aca783a3cfc44cb95bbf/typer-0.20.0-py3-none-any.whl (47 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (4.15.0)\n",
            "Collecting websockets<16.0,>=10.0 (from gradio-client==1.10.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/97/3a/5323a6bb94917af13bbb34009fac01e55c51dfde354f63692bf2533ffbc2/websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (3.11)\n",
            "Collecting annotated-doc>=0.0.2 (from fastapi->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/1e/d3/26bf1008eb3d2daa8ef4cacc7f3bfdc11818d111f7e2d0201bc6e3b49d45/annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.9.3) (2025.2)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<=2.10.6->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/32/90/3b15e31b88ca39e9e626630b4c4a1f5a0dfd09076366f4219429e6786076/pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hCollecting huggingface-hub>=0.21.0 (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/cb/bd/1a875e0d592d447cbc02805fd3fe0f497714d6a2583f59d14fa9ebad96eb/huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m36m-:--:--\u001b[0m\n",
            "\u001b[?25hCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.21.0->accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/9a/92/cf3ab0b652b082e66876d08da57fcc6fa2f0e6c70dfbbafbd470bb73eb47/hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting regex!=2019.12.17 (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/14/62/b56d29e70b03666193369bdbdedfdc23946dbe9f81dd78ce262c74d988ab/regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m791.7/791.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click>=8.0.0 (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/98/78/01c019cdb5d6498122777c1a43056ebb3ebfeef2076d9d026bfe15583b2b/click-8.3.1-py3-none-any.whl (108 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/25/7a/b0178788f8dc6cafce37a212c99565fa1fe7872c70c6c9c1e1a372d9d88f/rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "Collecting docstring-parser>=0.16 (from tyro<0.9.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/55/e2/2537ebcff11c1ee1ff17d8d0b6f4db75873e3b0fb32c2d4a2ee31ecb310a/docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/8e/e1/202a31727b0d096a04380f78e809074d7a1d0a22d9d5a39fea1d2353fd02/shtab-1.8.0-py3-none-any.whl (14 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/fb/76/641ae371508676492379f16e2fa48f4e2c11741bd63c48be4b12a6b09cba/aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/fe/ba/e2081de779ca30d473f21f5b30e0e737c438205440784c7dfc81efc2b029/async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/3a/2a/7cc015f5b9f5db42b7d48157e23356022889fc354a2813c15934b7cb5c0e/attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/5d/ed/c7895fd2fde7f3ee70d248175f9b6cdf792fb741ab92dc59cd9ef3bd241b/frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/5e/4f/9c7992f245554d8b173f6f0a048ad24b3e645d883f096857ec2c0822b8bd/multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/08/57/8c87e93142b2c1fa2408e45695205a7ba05fb5db458c0bf5c06ba0e09ea6/propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/35/9f/06b765d45c0e44e8ecf0fe15c9eacbbde342bb5b7561c46944f107bfb6c3/yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (2025.11.12)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/7e/f5/f66802a942d491edb555dd61e3a9961140fd64c90bce1eafd741609d334d/httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/04/4b/29cac41a4d98d144bf5f6d33995617b185d14b22401f75ca86f384e87ff1/h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/32/5c/1ee32d1c7956923202f00cf8d2a14a62ed7517bdc0ee1e55301227fc273c/contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib>=3.7.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib>=3.7.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/5e/02/51373fa8846bd22bb54e5efb30a824b417b058083f775a194a432f21a45f/fonttools-4.61.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib>=3.7.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/d4/42/0f333164e6307a0687d1eb9ad256215aae2f4bd5d28f4653d6cd319a3ba3/kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing>=3 (from matplotlib>=3.7.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/10/5e/1aa9a93198c6b64513c9d7752de7422c06402de6600a8767da1524f9570b/pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->llamafactory==0.9.3) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (2.5.0)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/94/54/e7d793b573f298e1c9013b8c4dade17d481164aa517d1d7148619c2cedbf/markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (2.19.2)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->llamafactory==0.9.3) (1.3.0)\n",
            "Collecting termcolor (from fire->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/f9/d5/141f53d7c1eb2a80e6d3e9a390228c3222c27705cbe7f048d3623053f3ca/termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
            "Collecting audioread>=2.1.9 (from librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/7e/16/fbe8e1e185a45042f7cd3a282def5bb8d95bb69ab9e9ef6a5368aa17e426/audioread-3.1.0-py3-none-any.whl (23 kB)\n",
            "Collecting numba>=0.51.0 (from librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/a1/13/9a27bcd0baeea236116070c7df458414336f25e9dd5a872b066cf36b74bf/numba-0.62.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.1.0 (from librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/58/0e/8c2a03d518fb6bd0b6b0d4b114c63d5f1db01ff0f9925d8eb10960d01c01/scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.0 (from librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/1e/e8/685f47e0d754320684db4425a0967f7d3fa70126bffd76110b7009a0090f/joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/site-packages (from librosa->llamafactory==0.9.3) (5.2.1)\n",
            "Collecting soundfile>=0.12.1 (from librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/57/5e/70bdd9579b35003a489fc850b5047beeda26328053ebadc1fb60f320f7db/soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting pooch>=1.1 (from librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/a8/87/77cc11c7a9ea9fd05503def69e3d18605852cd0d4b0d3b8f15bbeb3ef1d1/pooch-1.8.2-py3-none-any.whl (64 kB)\n",
            "Collecting soxr>=0.3.2 (from librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/75/34/e18f1003e242aabed44ed8902534814d3e64209e4d1d874f5b9b67d73cde/soxr-1.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (242 kB)\n",
            "Collecting lazy_loader>=0.1 (from librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/83/60/d497a310bde3f01cb805196ac61b7ad6dc5dcf8dce66634dc34364b20b4f/lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Collecting msgpack>=1.0 (from librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/b7/09/2a06956383c0fdebaef5aa9246e2356776f12ea6f2a44bd1368abf0e46c4/msgpack-1.1.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (406 kB)\n",
            "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.0->librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/a6/7b/6d7585998a5991fa74dc925aae57913ba8c7c2efff909de9d34cc1cd3c27/llvmlite-0.45.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m  \u001b[33m0:00:45\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/site-packages (from pooch>=1.1->librosa->llamafactory==0.9.3) (4.5.0)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Collecting cffi>=1.0 (from soundfile>=0.12.1->librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/98/29/9b366e70e243eb3d14a5cb488dfd3a0b6b2f1fb001a203f653b93ccfac88/cffi-2.0.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216 kB)\n",
            "Collecting pycparser (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/a0/e3/59cd50310fc9b59512193629e1984c1f95e5c8ae6e5d8c69532ccc65a7fe/pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->llamafactory==0.9.3)\n",
            "  Downloading https://mirrors.aliyun.com/pypi/packages/3e/38/7859ff46355f76f8d19459005ca000b6e7012f2f1ca597746cbcd1fbfe5e/antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, jieba, antlr4-python3-runtime\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.3-0.editable-py3-none-any.whl size=27459 sha256=be03f37366eca445e28b9ff79b9f7be7e1b3b22b127c3eeff8324a5d2e1a1b64\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tbqiw_ah/wheels/5c/79/7e/72c80d84fb58351f0f8d2e48a93b05787aabc61467c934c42c\n",
            "  Building wheel for jieba (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314509 sha256=d163db8d5e43ae035007e3ca9299f3899d320f68e83ff5b96bd00f551e7bba8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/9c/e7/e8376be764bb36c57db95518a4aa6ff65c4ca470dc8dad24d4\n",
            "  Building wheel for antlr4-python3-runtime (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144591 sha256=efae756946f039e7e2f49db0d9f1601cf02eaa15e13c961101211c6cf0f81490\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/82/b1/b79d6e90f34257cd436860ed4f4a09f9e1ea8cd32da7046ea4\n",
            "Successfully built llamafactory jieba antlr4-python3-runtime\n",
            "Installing collected packages: pydub, jieba, antlr4-python3-runtime, xxhash, websockets, tomlkit, threadpoolctl, termcolor, shtab, shellingham, sentencepiece, semantic-version, safetensors, ruff, rouge-chinese, regex, pyyaml, python-multipart, pyparsing, pydantic-core, pycparser, pyarrow, protobuf, propcache, orjson, numpy, multidict, msgpack, mdurl, llvmlite, lazy_loader, kiwisolver, joblib, hf-xet, h11, groovy, fsspec, frozenlist, fonttools, ffmpy, einops, docstring-parser, dill, cycler, click, av, audioread, attrs, async-timeout, annotated-types, annotated-doc, aiohappyeyeballs, aiofiles, yarl, uvicorn, tiktoken, soxr, scipy, pydantic, pooch, omegaconf, numba, nltk, multiprocess, markdown-it-py, huggingface-hub, httpcore, fire, contourpy, cffi, anyio, aiosignal, tokenizers, starlette, sse-starlette, soundfile, scikit-learn, rich, matplotlib, httpx, aiohttp, tyro, typer, transformers, safehttpx, librosa, gradio-client, fastapi, accelerate, peft, gradio, datasets, trl, llamafactory\n",
            "\u001b[2K  Attempting uninstall: numpy0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22/94\u001b[0m [protobuf]]tipart]\n",
            "\u001b[2K    Found existing installation: numpy 2.1.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22/94\u001b[0m [protobuf]\n",
            "\u001b[2K    Uninstalling numpy-2.1.2:90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22/94\u001b[0m [protobuf]\n",
            "\u001b[2K      Successfully uninstalled numpy-2.1.2â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m25/94\u001b[0m [numpy]\n",
            "\u001b[2K  Attempting uninstall: fsspecmâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/94\u001b[0m [hf-xet]ver]\n",
            "\u001b[2K    Found existing installation: fsspec 2025.9.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/94\u001b[0m [hf-xet]\n",
            "\u001b[2K    Uninstalling fsspec-2025.9.0:[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/94\u001b[0m [hf-xet]\n",
            "\u001b[2K      Successfully uninstalled fsspec-2025.9.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33/94\u001b[0m [hf-xet]\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94/94\u001b[0m [llamafactory][0m [datasets]e]s]ub]\n",
            "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.7.0 aiofiles-24.1.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-doc-0.0.4 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 anyio-4.12.0 async-timeout-5.0.1 attrs-25.4.0 audioread-3.1.0 av-16.0.1 cffi-2.0.0 click-8.3.1 contourpy-1.3.2 cycler-0.12.1 datasets-3.6.0 dill-0.3.8 docstring-parser-0.17.0 einops-0.8.1 fastapi-0.123.9 ffmpy-1.0.0 fire-0.7.1 fonttools-4.61.0 frozenlist-1.8.0 fsspec-2025.3.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 jieba-0.42.1 joblib-1.5.2 kiwisolver-1.4.9 lazy_loader-0.4 librosa-0.11.0 llamafactory-0.9.3 llvmlite-0.45.1 markdown-it-py-4.0.0 matplotlib-3.10.7 mdurl-0.1.2 msgpack-1.1.2 multidict-6.7.0 multiprocess-0.70.16 nltk-3.9.2 numba-0.62.1 numpy-1.26.4 omegaconf-2.3.0 orjson-3.11.4 peft-0.15.2 pooch-1.8.2 propcache-0.4.1 protobuf-6.33.1 pyarrow-22.0.0 pycparser-2.23 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 pyparsing-3.2.5 python-multipart-0.0.20 pyyaml-6.0.3 regex-2025.11.3 rich-14.2.0 rouge-chinese-1.0.3 ruff-0.14.8 safehttpx-0.1.7 safetensors-0.7.0 scikit-learn-1.7.2 scipy-1.15.3 semantic-version-2.10.0 sentencepiece-0.2.1 shellingham-1.5.4 shtab-1.8.0 soundfile-0.13.1 soxr-1.0.0 sse-starlette-3.0.3 starlette-0.50.0 termcolor-3.2.0 threadpoolctl-3.6.0 tiktoken-0.12.0 tokenizers-0.21.1 tomlkit-0.13.3 transformers-4.52.4 trl-0.9.6 typer-0.20.0 tyro-0.8.14 uvicorn-0.38.0 websockets-15.0.1 xxhash-3.6.0 yarl-1.22.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 1. ã€å…³é”®ã€‘ä½¿ç”¨ %cd è¿›å…¥ç›®å½• (ä¸è¦ç”¨ !cd)\n",
        "%cd /mnt/workspace/LLaMA-Factory\n",
        "\n",
        "# 2. å»æ‰æ— å…³ä¾èµ–\n",
        "!pip install --upgrade pip\n",
        "!pip install modelscope>=1.11.0\n",
        "!pip install bitsandbytes>=0.39.0\n",
        "# !pip install vllm==0.8.3\n",
        "\n",
        "# 3. å®‰è£…é¡¹ç›®ä¾èµ–\n",
        "# è¿™ä¼šè‡ªåŠ¨å®‰è£… v0.9.3 æ‰€éœ€çš„å…¶ä»–åº“\n",
        "!pip install -e \".[torch,metrics]\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "204be19d-742d-4e1d-a3fd-a10932e21026",
      "metadata": {
        "id": "204be19d-742d-4e1d-a3fd-a10932e21026"
      },
      "source": [
        "### æ£€éªŒä¾èµ–æ˜¯å¦æ»¡è¶³"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aadae0bb-b256-438e-aecd-b1d77773c7da",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-05T08:25:25.018209Z",
          "iopub.status.busy": "2025-12-05T08:25:25.017997Z",
          "iopub.status.idle": "2025-12-05T08:25:25.037228Z",
          "shell.execute_reply": "2025-12-05T08:25:25.036599Z",
          "shell.execute_reply.started": "2025-12-05T08:25:25.018189Z"
        },
        "tags": [],
        "id": "aadae0bb-b256-438e-aecd-b1d77773c7da",
        "outputId": "d525068b-11c5-433c-df72-ebed882e374f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===========================================================================\n",
            "ğŸ› ï¸  ç¯å¢ƒä¾èµ–æ£€æµ‹æŠ¥å‘Š (æ ‡å‡†å·²å†æ¬¡æ›´æ–°)\n",
            "===========================================================================\n",
            "[ å¿…éœ€é¡¹ ]\n",
            "\u001b[92mâœ… python          | å½“å‰: å®Œç¾ (3.10.19)    | å¿…éœ€: >=3.9        | æ¨è: >=3.10\u001b[0m\n",
            "\u001b[92mâœ… torch           | å½“å‰: å®Œç¾ (2.6.0+cu124) | å¿…éœ€: >=2.0.0      | æ¨è: >=2.6.0\u001b[0m\n",
            "\u001b[92mâœ… torchvision     | å½“å‰: å®Œç¾ (0.21.0+cu124) | å¿…éœ€: >=0.15.0     | æ¨è: >=0.21.0\u001b[0m\n",
            "\u001b[92mâœ… transformers    | å½“å‰: å®Œç¾ (4.52.4)     | å¿…éœ€: >=4.45.0     | æ¨è: >=4.50.0\u001b[0m\n",
            "\u001b[92mâœ… datasets        | å½“å‰: å®Œç¾ (3.6.0)      | å¿…éœ€: >=2.16.0     | æ¨è: >=3.2.0\u001b[0m\n",
            "\u001b[92mâœ… accelerate      | å½“å‰: å®Œç¾ (1.7.0)      | å¿…éœ€: >=0.34.0     | æ¨è: >=1.2.1\u001b[0m\n",
            "\u001b[92mâœ… peft            | å½“å‰: å®Œç¾ (0.15.2)     | å¿…éœ€: >=0.14.0     | æ¨è: >=0.15.1\u001b[0m\n",
            "\u001b[92mâœ… trl             | å½“å‰: å®Œç¾ (0.9.6)      | å¿…éœ€: >=0.8.6      | æ¨è: >=0.9.6\u001b[0m\n",
            "\n",
            "[ å¯é€‰é¡¹ ]\n",
            "\u001b[92mâœ… cuda            | å½“å‰: å®Œç¾ (12.4)       | å¿…éœ€: >=11.6       | æ¨è: >=12.2\u001b[0m\n",
            "\u001b[90mâšª deepspeed       | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=0.10.0     | æ¨è: >=0.16.4\u001b[0m\n",
            "\u001b[92mâœ… bitsandbytes    | å½“å‰: å®Œç¾ (0.48.2)     | å¿…éœ€: >=0.39.0     | æ¨è: >=0.43.1\u001b[0m\n",
            "\u001b[90mâšª vllm            | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=0.4.3      | æ¨è: >=0.8.2\u001b[0m\n",
            "\u001b[90mâšª flash-attn      | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=2.5.6      | æ¨è: >=2.7.2\u001b[0m\n",
            "\n",
            "[ ç¡¬ä»¶ç®€æµ‹ ]\n",
            "âœ… æ£€æµ‹åˆ° 1 ä¸ª GPU è®¾å¤‡:\n",
            "   - GPU 0: NVIDIA A10 (23.5 GB VRAM)\n",
            "===========================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import sys\n",
        "import platform\n",
        "import importlib.metadata\n",
        "from packaging import version\n",
        "import torch\n",
        "\n",
        "# ================= æ‚¨çš„æœ€æ–°é…ç½®è¦æ±‚ (å·²å†æ¬¡æ›´æ–°) =================\n",
        "REQUIRED_PACKAGES = {\n",
        "    \"python\":       (\"3.9\",    \"3.10\"),\n",
        "    \"torch\":        (\"2.0.0\",  \"2.6.0\"),\n",
        "    \"torchvision\":  (\"0.15.0\", \"0.21.0\"),\n",
        "    \"transformers\": (\"4.45.0\", \"4.50.0\"),\n",
        "    \"datasets\":     (\"2.16.0\", \"3.2.0\"),\n",
        "    \"accelerate\":   (\"0.34.0\", \"1.2.1\"),\n",
        "    \"peft\":         (\"0.14.0\", \"0.15.1\"),\n",
        "    \"trl\":          (\"0.8.6\",  \"0.9.6\"),\n",
        "}\n",
        "\n",
        "OPTIONAL_PACKAGES = {\n",
        "    \"cuda\":         (\"11.6\",   \"12.2\"),\n",
        "    \"deepspeed\":    (\"0.10.0\", \"0.16.4\"),\n",
        "    \"bitsandbytes\": (\"0.39.0\", \"0.43.1\"),\n",
        "    \"vllm\":         (\"0.4.3\",  \"0.8.2\"),\n",
        "    \"flash-attn\":   (\"2.5.6\",  \"2.7.2\"),\n",
        "}\n",
        "\n",
        "# ================= æ£€æµ‹é€»è¾‘ =================\n",
        "def get_package_version(package_name):\n",
        "    try:\n",
        "        if package_name == \"python\": return platform.python_version()\n",
        "        elif package_name == \"cuda\": return torch.version.cuda if torch.cuda.is_available() else None\n",
        "        elif package_name == \"flash-attn\": return importlib.metadata.version(\"flash_attn\")\n",
        "        else: return importlib.metadata.version(package_name)\n",
        "    except importlib.metadata.PackageNotFoundError: return None\n",
        "\n",
        "def check_requirement(name, constraints, is_optional=False):\n",
        "    min_v, rec_v = constraints\n",
        "    current_v = get_package_version(name)\n",
        "\n",
        "    # å®šä¹‰åˆ—å®½ä»¥ä¿æŒå¯¹é½\n",
        "    col_name = 15\n",
        "    col_curr = 15\n",
        "    col_req = 10\n",
        "\n",
        "    if current_v is None:\n",
        "        status, msg, color = (\"âšª\", \"æœªå®‰è£… (å¯é€‰)\", \"\\033[90m\") if is_optional else (\"âŒ\", \"æœªå®‰è£… !!\", \"\\033[91m\")\n",
        "    else:\n",
        "        try:\n",
        "            curr_p, min_p, rec_p = version.parse(current_v), version.parse(min_v), version.parse(rec_v)\n",
        "            if curr_p >= rec_p:   status, msg, color = (\"âœ…\", f\"å®Œç¾ ({current_v})\", \"\\033[92m\") # Green\n",
        "            elif curr_p >= min_p: status, msg, color = (\"âš ï¸\", f\"è¾¾æ ‡ ({current_v})\", \"\\033[93m\") # Yellow\n",
        "            else:                 status, msg, color = (\"âŒ\", f\"è¿‡ä½ ({current_v})\", \"\\033[91m\") # Red\n",
        "        except Exception as e:\n",
        "            status, msg, color = (\"â“\", f\"è§£æé”™è¯¯\", \"\\033[93m\")\n",
        "\n",
        "    print(f\"{color}{status} {name:<{col_name}} | å½“å‰: {msg:<{col_curr}} | å¿…éœ€: >={min_v:<{col_req}} | æ¨è: >={rec_v}\\033[0m\")\n",
        "\n",
        "def check_gpu():\n",
        "    print(\"\\n[ ç¡¬ä»¶ç®€æµ‹ ]\")\n",
        "    if torch.cuda.is_available():\n",
        "        device_count = torch.cuda.device_count()\n",
        "        print(f\"âœ… æ£€æµ‹åˆ° {device_count} ä¸ª GPU è®¾å¤‡:\")\n",
        "        for i in range(device_count):\n",
        "            gpu_name = torch.cuda.get_device_name(i)\n",
        "            # è½¬æ¢ä¸º GB\n",
        "            total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
        "            print(f\"   - GPU {i}: {gpu_name} ({total_mem:.1f} GB VRAM)\")\n",
        "    else:\n",
        "        print(\"âšª æœªæ£€æµ‹åˆ°å¯ç”¨ GPU (CUDAä¸å¯ç”¨)\")\n",
        "\n",
        "print(\"=\"*75)\n",
        "print(f\"ğŸ› ï¸  ç¯å¢ƒä¾èµ–æ£€æµ‹æŠ¥å‘Š (æ ‡å‡†å·²å†æ¬¡æ›´æ–°)\")\n",
        "print(\"=\"*75)\n",
        "print(\"[ å¿…éœ€é¡¹ ]\")\n",
        "for k, v in REQUIRED_PACKAGES.items(): check_requirement(k, v)\n",
        "print(\"\\n[ å¯é€‰é¡¹ ]\")\n",
        "for k, v in OPTIONAL_PACKAGES.items(): check_requirement(k, v, True)\n",
        "check_gpu()\n",
        "print(\"=\"*75)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14da143a-1457-4b45-8847-3ad80b98e3b7",
      "metadata": {
        "id": "14da143a-1457-4b45-8847-3ad80b98e3b7"
      },
      "source": [
        "è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¦‚æœæ˜¾ç¤ºllamafactory-cliçš„ç‰ˆæœ¬ï¼Œåˆ™è¡¨ç¤ºå®‰è£…æˆåŠŸã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9865b7f4-016c-441b-94fe-0436a65e398a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-05T08:25:25.037887Z",
          "iopub.status.busy": "2025-12-05T08:25:25.037697Z",
          "iopub.status.idle": "2025-12-05T08:25:33.891566Z",
          "shell.execute_reply": "2025-12-05T08:25:33.890641Z",
          "shell.execute_reply.started": "2025-12-05T08:25:25.037870Z"
        },
        "tags": [],
        "id": "9865b7f4-016c-441b-94fe-0436a65e398a",
        "outputId": "8987ded5-d60f-42e4-ee0f-9b2d8bca01b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------\n",
            "| Welcome to LLaMA Factory, version 0.9.3                |\n",
            "|                                                        |\n",
            "| Project page: https://github.com/hiyouga/LLaMA-Factory |\n",
            "----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!llamafactory-cli version"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3507f4d9-5cef-45a3-96bd-a2bbd4d4a90d",
      "metadata": {
        "id": "3507f4d9-5cef-45a3-96bd-a2bbd4d4a90d"
      },
      "source": [
        "## 2. ä¸‹è½½æ•°æ®é›†"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "464f2e63-865a-47ec-90a8-7e5af46b4d33",
      "metadata": {
        "id": "464f2e63-865a-47ec-90a8-7e5af46b4d33"
      },
      "source": [
        "LLaMA-Factoryé¡¹ç›®å†…ç½®äº†ä¸°å¯Œçš„æ•°æ®é›†ï¼Œæ”¾åœ¨äº†`data`ç›®å½•ä¸‹ã€‚æ‚¨å¯ä»¥è·³è¿‡æœ¬æ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨å†…ç½®æ•°æ®é›†ã€‚æ‚¨ä¹Ÿå¯ä»¥å‡†å¤‡è‡ªå®šä¹‰æ•°æ®é›†ï¼Œå°†æ•°æ®å¤„ç†ä¸ºæ¡†æ¶ç‰¹å®šçš„æ ¼å¼ï¼Œæ”¾åœ¨`data`ä¸‹ï¼Œå¹¶ä¸”ä¿®æ”¹`dataset_info.json`æ–‡ä»¶ã€‚\n",
        "\n",
        "æœ¬æ•™ç¨‹å‡†å¤‡äº†ä¸€ä»½å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œè¿è¡Œä¸‹è¿°å‘½ä»¤ä¸‹è½½æ•°æ®ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "748944fd-0545-4cdc-877a-1a5abcf50298",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2025-12-05T08:25:33.892963Z",
          "iopub.status.busy": "2025-12-05T08:25:33.892613Z",
          "iopub.status.idle": "2025-12-05T08:25:34.395508Z",
          "shell.execute_reply": "2025-12-05T08:25:34.394853Z",
          "shell.execute_reply.started": "2025-12-05T08:25:33.892927Z"
        },
        "tags": [],
        "id": "748944fd-0545-4cdc-877a-1a5abcf50298",
        "outputId": "cf0f5ceb-078e-45cf-f1d1-c6eec0e9adb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-05 16:25:33--  https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/llama_factory/data.zip\n",
            "Resolving atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com (atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com)... 47.101.88.43\n",
            "Connecting to atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com (atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com)|47.101.88.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 402043 (393K) [application/zip]\n",
            "Saving to: â€˜data.zipâ€™\n",
            "\n",
            "data.zip            100%[===================>] 392.62K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-12-05 16:25:34 (3.76 MB/s) - â€˜data.zipâ€™ saved [402043/402043]\n",
            "\n",
            "Archive:  data.zip\n",
            "  inflating: data/eval.json          \n",
            "  inflating: data/__MACOSX/._eval.json  \n",
            "  inflating: data/LICENSE            \n",
            "  inflating: data/__MACOSX/._LICENSE  \n",
            "  inflating: data/dataset_info.json  \n",
            "  inflating: data/__MACOSX/._dataset_info.json  \n",
            "  inflating: data/train.json         \n",
            "  inflating: data/__MACOSX/._train.json  \n"
          ]
        }
      ],
      "source": [
        "!wget https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/llama_factory/data.zip\n",
        "!mv data rawdata && unzip data.zip -d data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7002d0f1-d726-414e-b8df-114b43791379",
      "metadata": {
        "id": "7002d0f1-d726-414e-b8df-114b43791379"
      },
      "source": [
        "ä¸Šä¼ åŒ»ç–—æ•°æ®é›†ï¼Œå°†jsonlæ–‡ä»¶æ‹–æ‹½åˆ°`/mnt/workspace`æ–‡ä»¶ä¸‹ã€‚\n",
        "```\n",
        "root@dsw-523480-fd7f95d6d-cgjwf:/mnt/workspace# pwd\n",
        "/mnt/workspace\n",
        "root@dsw-523480-fd7f95d6d-cgjwf:/mnt/workspace# ll\n",
        "total 63916\n",
        "drwxr-xr-x  7 root root     4096 Dec  3 16:32  ./\n",
        "drwxr-xr-x  1 root root     4096 Dec  3 15:06  ../\n",
        "drwxr-xr-x  2 root root     4096 Dec  3 15:19  _html/\n",
        "drwxr-xr-x 16 root root     4096 Dec  3 15:59  LLaMA-Factory/\n",
        "-rw-r--r--  1 root root    87581 Dec  3 16:32  llama-factory-in-ali-pai.ipynb\n",
        "drwxr-xr-x  3 root root     4096 Dec  3 15:06  .local/\n",
        "-rw-r--r--  1 root root 65325060 Dec  3 16:21  medical_o1_sft_Chinese_alpaca_cot.jsonl\n",
        "drwxr-xr-x  2 root root     4096 Dec  3 15:10  .virtual_documents/\n",
        "```\n",
        "\n",
        "\n",
        "ä¿®æ”¹`/mnt/workspace`æ–‡ä»¶å¤¹ä¸‹â€œdataset_json.infoâ€æ–‡ä»¶ï¼Œåœ¨jsonæ–‡ä»¶å¼€å§‹ä½ç½®æ·»åŠ å¦‚ä¸‹æ‰€ç¤ºè„šæœ¬ã€‚\n",
        "```\n",
        "\"medical_sft\": {\n",
        "    \"file_name\": \"/workspace/LLaMA-Factory/data/medical_o1_sft_Chinese_alpaca_cot.jsonl\",\n",
        "    \"columns\": {\n",
        "      \"prompt\": \"prompt\",\n",
        "      \"response\": \"response\"\n",
        "    }\n",
        "  }\n",
        "```\n",
        "\n",
        "â€œdataset_json.infoâ€æ–‡ä»¶æœ€ç»ˆå¦‚ä¸‹\n",
        "```\n",
        "{\n",
        "  \"medical_sft\": {\n",
        "    \"file_name\": \"/mnt/workspace/medical_o1_sft_Chinese_alpaca_cot.jsonl\",\n",
        "    \"columns\": {\n",
        "      \"prompt\": \"prompt\",\n",
        "      \"response\": \"response\"\n",
        "    }\n",
        "  },\n",
        "  \"train\": {\n",
        "    \"file_name\": \"train.json\",\n",
        "    \"formatting\": \"sharegpt\"\n",
        "  },\n",
        "  \"eval\": {\n",
        "    \"file_name\": \"eval.json\",\n",
        "    \"formatting\": \"sharegpt\"\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8241a6b7-cf72-4014-bd45-a2e5f434b70a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-05T08:25:34.396989Z",
          "iopub.status.busy": "2025-12-05T08:25:34.396622Z",
          "iopub.status.idle": "2025-12-05T08:25:34.403054Z",
          "shell.execute_reply": "2025-12-05T08:25:34.402125Z",
          "shell.execute_reply.started": "2025-12-05T08:25:34.396950Z"
        },
        "id": "8241a6b7-cf72-4014-bd45-a2e5f434b70a",
        "outputId": "c72cc8d7-b25c-41bc-90c1-28dfa014fab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… æˆåŠŸï¼å·²è‡ªåŠ¨æ·»åŠ é€—å·å¹¶æ³¨å†Œ 'medical_sft' åˆ°ï¼š\n",
            "/mnt/workspace/LLaMA-Factory/data/dataset_info.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# 1. ä½ çš„æ•°æ®é›†è·¯å¾„\n",
        "jsonl_file = \"/mnt/workspace/medical_o1_sft_Chinese_alpaca_cot.jsonl\"\n",
        "# 2. LLaMA-Factory çš„é…ç½®æ–‡ä»¶è·¯å¾„\n",
        "config_file = \"/mnt/workspace/LLaMA-Factory/data/dataset_info.json\"\n",
        "\n",
        "# 3. è¦æ·»åŠ çš„é…ç½®å†…å®¹ (Pythonå­—å…¸æ ¼å¼ï¼Œä¸éœ€è¦æ‰‹åŠ¨ç®¡é€—å·)\n",
        "new_data = {\n",
        "    \"medical_sft\": {\n",
        "        \"file_name\": jsonl_file,\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"prompt\",\n",
        "            \"response\": \"response\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# 4. æ‰§è¡Œè‡ªåŠ¨å†™å…¥\n",
        "try:\n",
        "    with open(config_file, 'r', encoding='utf-8') as f:\n",
        "        content = json.load(f)\n",
        "\n",
        "    # æ›´æ–°æ•°æ®ï¼ˆç¨‹åºä¼šè‡ªåŠ¨å¤„ç†é€—å·åˆ†å‰²ï¼‰\n",
        "    content.update(new_data)\n",
        "\n",
        "    with open(config_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(content, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"âœ… æˆåŠŸï¼å·²è‡ªåŠ¨æ·»åŠ é€—å·å¹¶æ³¨å†Œ 'medical_sft' åˆ°ï¼š\\n{config_file}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° LLaMA-Factory çš„é…ç½®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç›®å½•ç»“æ„ã€‚\")\n",
        "except json.JSONDecodeError:\n",
        "    print(\"âŒ é”™è¯¯ï¼šåŸé…ç½®æ–‡ä»¶æ ¼å¼æŸåï¼ˆå¯èƒ½ä¹‹å‰æ‰‹åŠ¨æ”¹åäº†ï¼‰ï¼Œè¯·è¿˜åŸæ–‡ä»¶ã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64c2c80e-4828-4c00-baf2-4d79e056263e",
      "metadata": {
        "id": "64c2c80e-4828-4c00-baf2-4d79e056263e"
      },
      "source": [
        "## 3. æ¨¡å‹å¾®è°ƒ\n",
        "### 3.1 å¯åŠ¨Web UI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36bcd446-eb8d-4a28-9909-98e08e5e57e9",
      "metadata": {
        "id": "36bcd446-eb8d-4a28-9909-98e08e5e57e9"
      },
      "source": [
        "åšå¥½å‰åºå‡†å¤‡å·¥ä½œåï¼Œç›´æ¥è¿è¡Œä¸‹è¿°å‘½ä»¤å°±å¯ä»¥å¯åŠ¨Web UIã€‚è¿™é‡Œç”¨åˆ°çš„ç¯å¢ƒå˜é‡è§£é‡Šå¦‚ä¸‹ï¼š\n",
        "- `USE_MODELSCOPE_HUB`è®¾ä¸º1ï¼Œè¡¨ç¤ºæ¨¡å‹æ¥æºæ˜¯ModelScopeã€‚ä½¿ç”¨HuggingFaceæ¨¡å‹å¯èƒ½ä¼šæœ‰ç½‘ç»œé—®é¢˜ã€‚\n",
        "\n",
        "ç‚¹å‡»è¿”å›çš„URLåœ°å€`http://0.0.0.0:7860`ï¼Œè¿›å…¥Web UIé¡µé¢ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f3d0bf-668b-43e9-a146-33369c69a86a",
      "metadata": {
        "ExecutionIndicator": {
          "show": true
        },
        "execution": {
          "iopub.execute_input": "2025-12-05T08:25:34.403827Z",
          "iopub.status.busy": "2025-12-05T08:25:34.403614Z"
        },
        "tags": [],
        "id": "20f3d0bf-668b-43e9-a146-33369c69a86a",
        "outputId": "2f7b380f-5c10-49a5-99cc-dc533f768762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
            "* Running on local URL:  http://0.0.0.0:7860\n",
            "* To create a public link, set `share=True` in `launch()`.\n",
            "[WARNING|2025-12-05 16:35:32] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
            "[INFO|2025-12-05 16:35:32] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,574 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,574 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,574 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,575 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,575 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,575 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,575 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-12-05 16:35:32,897 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 16:35:32,898 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 16:35:32,900 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,901 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,902 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,902 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,902 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,902 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,902 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 16:35:32,902 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-12-05 16:35:33,230 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-12-05 16:35:33] llamafactory.data.loader:143 >> Loading dataset /mnt/workspace/medical_o1_sft_Chinese_alpaca_cot.jsonl...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 20171 examples [00:00, 206800.45 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100%|â–ˆ| 1000/1000 [00:00<00:00, 4732\n",
            "Running tokenizer on dataset (num_proc=16): 100%|â–ˆ| 1000/1000 [00:02<00:00, 363.\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 872, 198, 100345, 53481, 3837, 46944, 16, 104252, 99657, 18493, 106084, 117405, 100347, 42140, 44290, 30709, 36885, 55502, 3837, 101930, 16530, 100939, 39762, 3837, 100136, 99601, 116066, 26288, 29524, 100711, 3837, 101776, 99577, 88653, 115623, 3837, 39426, 16530, 118609, 3837, 117405, 16872, 18830, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 1773, 100137, 116067, 18493, 104823, 15946, 105262, 100678, 99252, 11319, 220, 14880, 104137, 113272, 62926, 107485, 102349, 8997, 99487, 109661, 18493, 104797, 117405, 17447, 45861, 112312, 30709, 36885, 55502, 3837, 99725, 105562, 52801, 3837, 103957, 105231, 115623, 67279, 3837, 88653, 117633, 115623, 1773, 105839, 104797, 100624, 99259, 3837, 87267, 33108, 100102, 99259, 101063, 1773, 99306, 14777, 92015, 104006, 99406, 3837, 112575, 102347, 102441, 99193, 3837, 104797, 9370, 100102, 99259, 70927, 99308, 80158, 99873, 100585, 34187, 101099, 3407, 11622, 104823, 106413, 100192, 3837, 100347, 30709, 36885, 55502, 5373, 105566, 101930, 16530, 100939, 39762, 3837, 100001, 101368, 104029, 114316, 64355, 116066, 1773, 109661, 111335, 49828, 100001, 116771, 3837, 99558, 99519, 100102, 99259, 18493, 31914, 20742, 100601, 36885, 3407, 77288, 87256, 101997, 3837, 117405, 16872, 100626, 34794, 100743, 3837, 43288, 87267, 106015, 20412, 105172, 64355, 116066, 1773, 104544, 106141, 101174, 105806, 3837, 110090, 115623, 101561, 70927, 117083, 1773, 101893, 99559, 104823, 15946, 104854, 105792, 53556, 123, 116066, 100631, 100102, 116066, 3837, 74763, 104560, 109851, 99559, 3407, 49567, 100158, 3837, 117405, 101913, 34794, 100743, 33108, 102010, 49185, 99696, 118329, 101160, 99461, 100403, 26939, 117405, 16872, 3837, 100346, 99520, 66394, 102410, 20412, 88653, 25074, 57191, 121813, 120201, 11319, 100001, 101419, 38953, 53481, 107200, 57191, 117591, 9370, 101120, 101304, 3837, 104050, 18830, 32108, 115623, 16530, 100939, 39762, 3837, 99518, 101894, 102257, 57191, 34794, 100743, 102072, 3407, 104857, 105839, 3837, 35946, 99494, 100681, 100001, 101368, 33126, 111892, 121813, 120201, 105437, 11319, 101150, 106350, 104661, 106021, 33108, 104797, 106806, 105419, 33071, 100741, 3837, 100102, 99259, 104560, 35568, 62112, 3837, 77288, 87267, 103943, 79599, 99586, 100631, 110776, 100102, 105998, 101742, 99337, 3407, 102112, 99797, 111492, 53481, 108734, 3837, 100137, 101930, 16530, 100939, 39762, 99518, 106888, 104215, 3837, 62244, 100374, 104823, 33126, 99835, 29258, 9370, 99252, 13072, 3837, 104074, 102410, 20412, 102031, 33126, 113879, 9370, 101304, 26850, 87256, 101118, 100158, 3837, 43288, 99730, 99520, 112507, 121813, 120201, 3837, 49828, 104857, 101042, 117405, 49185, 99696, 62926, 100347, 34794, 100743, 101893, 101120, 101368, 1773, 104823, 69249, 64355, 3837, 101893, 101107, 87267, 33126, 101137, 14009, 101782, 116066, 527, 57191, 14009, 64355, 120471, 527, 1773, 100001, 99252, 13072, 102119, 53481, 107200, 101120, 101304, 104813, 101776, 102581, 33108, 99877, 100250, 99561, 3407, 101997, 105419, 33108, 104661, 108372, 3837, 104797, 99518, 100102, 99518, 99259, 3837, 47815, 102494, 104892, 99873, 17254, 107200, 3837, 111309, 99899, 99932, 9370, 102778, 72448, 115781, 104036, 1773, 64355, 120471, 99487, 99252, 13072, 115371, 101228, 99322, 39762, 3837, 108884, 53481, 9370, 101304, 101120, 3837, 101776, 102581, 26939, 100347, 34794, 100743, 3407, 100632, 3837, 104857, 115104, 33447, 99879, 3837, 100626, 18947, 99252, 13072, 101994, 104652, 100968, 3837, 105792, 14009, 121617, 120657, 119740, 527, 3837, 43288, 99252, 18493, 104823, 69249, 95411, 63367, 65101, 100137, 101120, 101304, 62926, 115563, 99194, 32948, 34794, 100743, 102072, 1773, 99652, 74763, 114369, 32108, 115623, 33108, 102010, 49185, 99696, 100001, 101368, 3407, 104170, 3837, 75882, 99670, 20412, 106084, 100102, 99259, 3837, 100673, 100102, 99586, 110540, 3837, 104661, 108372, 53153, 101515, 3837, 41146, 106141, 99185, 12857, 101893, 101304, 11319, 99799, 101042, 33447, 104288, 14009, 121617, 120657, 119740, 527, 99487, 99252, 13072, 101228, 102084, 101137, 1773, 151645, 198, 151644, 77091, 198, 151667, 198, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 8997, 151668, 271, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 1773, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>user\n",
            "æ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ è¯·é€æ­¥æ¨ç†å¹¶ç»™å‡ºç­”æ¡ˆã€‚\n",
            "è¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\n",
            "\n",
            "ç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\n",
            "\n",
            "ä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\n",
            "\n",
            "ç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\n",
            "\n",
            "ä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\n",
            "\n",
            "å›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\n",
            "\n",
            "å†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\n",
            "\n",
            "çœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\n",
            "\n",
            "ä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\n",
            "\n",
            "å“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<think>\n",
            "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
            "</think>\n",
            "\n",
            "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151667, 198, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 8997, 151668, 271, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 1773, 151645, 198]\n",
            "labels:\n",
            "<think>\n",
            "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
            "</think>\n",
            "\n",
            "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 16:35:37,717 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 16:35:37,718 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-12-05 16:35:37] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|2025-12-05 16:35:37] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|modeling_utils.py:1148] 2025-12-05 16:35:39,714 >> loading weights file /mnt/workspace/Qwen3-8B/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-12-05 16:35:39,715 >> Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-12-05 16:35:39,716 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:32<00:00,  6.45s/it]\n",
            "[INFO|modeling_utils.py:5131] 2025-12-05 16:36:12,130 >> All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-12-05 16:36:12,130 >> All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at /mnt/workspace/Qwen3-8B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1088] 2025-12-05 16:36:12,134 >> loading configuration file /mnt/workspace/Qwen3-8B/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-12-05 16:36:12,135 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.95\n",
            "}\n",
            "\n",
            "[INFO|2025-12-05 16:36:12] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-12-05 16:36:12] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-12-05 16:36:12] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-12-05 16:36:12] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-12-05 16:36:12] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,k_proj,v_proj,down_proj,gate_proj,o_proj,up_proj\n",
            "[INFO|2025-12-05 16:36:12] llamafactory.model.loader:143 >> trainable params: 21,823,488 || all params: 8,212,558,848 || trainable%: 0.2657\n",
            "[INFO|trainer.py:756] 2025-12-05 16:36:12,842 >> Using auto half precision backend\n",
            "[INFO|2025-12-05 16:36:13] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
            "[INFO|trainer.py:2409] 2025-12-05 16:36:13,109 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-12-05 16:36:13,109 >>   Num examples = 850\n",
            "[INFO|trainer.py:2411] 2025-12-05 16:36:13,109 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2412] 2025-12-05 16:36:13,109 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2415] 2025-12-05 16:36:13,110 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:2416] 2025-12-05 16:36:13,110 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2417] 2025-12-05 16:36:13,110 >>   Total optimization steps = 639\n",
            "[INFO|trainer.py:2418] 2025-12-05 16:36:13,116 >>   Number of trainable parameters = 21,823,488\n",
            "  1%|â–                                          | 5/639 [00:20<42:36,  4.03s/it][INFO|2025-12-05 16:36:33] llamafactory.train.callbacks:143 >> {'loss': 0.6963, 'learning_rate': 9.9990e-05, 'epoch': 0.02, 'throughput': 692.28}\n",
            "{'loss': 0.6963, 'grad_norm': 0.2321816235780716, 'learning_rate': 9.999033183566353e-05, 'epoch': 0.02, 'num_input_tokens_seen': 14376, 'train_runtime': 20.7719, 'train_tokens_per_second': 692.09}\n",
            "  2%|â–‹                                         | 10/639 [00:40<41:57,  4.00s/it][INFO|2025-12-05 16:36:53] llamafactory.train.callbacks:143 >> {'loss': 0.5433, 'learning_rate': 9.9951e-05, 'epoch': 0.05, 'throughput': 698.16}\n",
            "{'loss': 0.5433, 'grad_norm': 0.3457997143268585, 'learning_rate': 9.995106132599869e-05, 'epoch': 0.05, 'num_input_tokens_seen': 28296, 'train_runtime': 40.5348, 'train_tokens_per_second': 698.067}\n",
            "  2%|â–‰                                         | 15/639 [01:04<48:13,  4.64s/it][INFO|2025-12-05 16:37:17] llamafactory.train.callbacks:143 >> {'loss': 0.5772, 'learning_rate': 9.9882e-05, 'epoch': 0.07, 'throughput': 701.34}\n",
            "{'loss': 0.5772, 'grad_norm': 0.26326724886894226, 'learning_rate': 9.988160792165562e-05, 'epoch': 0.07, 'num_input_tokens_seen': 44992, 'train_runtime': 64.1569, 'train_tokens_per_second': 701.281}\n",
            "  3%|â–ˆâ–                                        | 20/639 [01:25<43:38,  4.23s/it][INFO|2025-12-05 16:37:38] llamafactory.train.callbacks:143 >> {'loss': 0.5999, 'learning_rate': 9.9782e-05, 'epoch': 0.09, 'throughput': 700.42}\n",
            "{'loss': 0.5999, 'grad_norm': 0.39365607500076294, 'learning_rate': 9.978201358980645e-05, 'epoch': 0.09, 'num_input_tokens_seen': 59576, 'train_runtime': 85.0631, 'train_tokens_per_second': 700.374}\n",
            "  4%|â–ˆâ–‹                                        | 25/639 [01:46<42:46,  4.18s/it][INFO|2025-12-05 16:37:59] llamafactory.train.callbacks:143 >> {'loss': 0.5982, 'learning_rate': 9.9652e-05, 'epoch': 0.12, 'throughput': 699.56}\n",
            "{'loss': 0.5982, 'grad_norm': 0.3449796736240387, 'learning_rate': 9.965233851025814e-05, 'epoch': 0.12, 'num_input_tokens_seen': 74240, 'train_runtime': 106.1298, 'train_tokens_per_second': 699.521}\n",
            "  5%|â–ˆâ–‰                                        | 30/639 [02:08<44:10,  4.35s/it][INFO|2025-12-05 16:38:21] llamafactory.train.callbacks:143 >> {'loss': 0.6129, 'learning_rate': 9.9493e-05, 'epoch': 0.14, 'throughput': 700.61}\n",
            "{'loss': 0.6129, 'grad_norm': 0.24829399585723877, 'learning_rate': 9.949266103908895e-05, 'epoch': 0.14, 'num_input_tokens_seen': 89944, 'train_runtime': 128.3858, 'train_tokens_per_second': 700.576}\n",
            "  5%|â–ˆâ–ˆâ–                                       | 35/639 [02:29<42:53,  4.26s/it][INFO|2025-12-05 16:38:43] llamafactory.train.callbacks:143 >> {'loss': 0.6044, 'learning_rate': 9.9303e-05, 'epoch': 0.16, 'throughput': 702.44}\n",
            "{'loss': 0.6044, 'grad_norm': 0.25331586599349976, 'learning_rate': 9.930307766130169e-05, 'epoch': 0.16, 'num_input_tokens_seen': 105288, 'train_runtime': 149.8946, 'train_tokens_per_second': 702.414}\n",
            "  6%|â–ˆâ–ˆâ–‹                                       | 40/639 [02:50<40:32,  4.06s/it][INFO|2025-12-05 16:39:03] llamafactory.train.callbacks:143 >> {'loss': 0.5386, 'learning_rate': 9.9084e-05, 'epoch': 0.19, 'throughput': 702.40}\n",
            "{'loss': 0.5386, 'grad_norm': 0.3147159218788147, 'learning_rate': 9.90837029325229e-05, 'epoch': 0.19, 'num_input_tokens_seen': 119432, 'train_runtime': 170.0396, 'train_tokens_per_second': 702.377}\n",
            "  7%|â–ˆâ–ˆâ–‰                                       | 45/639 [03:08<37:22,  3.78s/it][INFO|2025-12-05 16:39:21] llamafactory.train.callbacks:143 >> {'loss': 0.5771, 'learning_rate': 9.8835e-05, 'epoch': 0.21, 'throughput': 702.87}\n",
            "{'loss': 0.5771, 'grad_norm': 0.26449665427207947, 'learning_rate': 9.883466940978252e-05, 'epoch': 0.21, 'num_input_tokens_seen': 132576, 'train_runtime': 188.6257, 'train_tokens_per_second': 702.852}\n",
            "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 50/639 [03:27<38:06,  3.88s/it][INFO|2025-12-05 16:39:40] llamafactory.train.callbacks:143 >> {'loss': 0.5280, 'learning_rate': 9.8556e-05, 'epoch': 0.24, 'throughput': 703.69}\n",
            "{'loss': 0.528, 'grad_norm': 0.2869769334793091, 'learning_rate': 9.855612757141655e-05, 'epoch': 0.24, 'num_input_tokens_seen': 146112, 'train_runtime': 207.6438, 'train_tokens_per_second': 703.666}\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–Œ                                      | 55/639 [03:50<42:31,  4.37s/it][INFO|2025-12-05 16:40:03] llamafactory.train.callbacks:143 >> {'loss': 0.5661, 'learning_rate': 9.8248e-05, 'epoch': 0.26, 'throughput': 704.44}\n",
            "{'loss': 0.5661, 'grad_norm': 0.38860568404197693, 'learning_rate': 9.824824572614051e-05, 'epoch': 0.26, 'num_input_tokens_seen': 162048, 'train_runtime': 230.0436, 'train_tokens_per_second': 704.423}\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 60/639 [04:10<39:44,  4.12s/it][INFO|2025-12-05 16:40:23] llamafactory.train.callbacks:143 >> {'loss': 0.6074, 'learning_rate': 9.7911e-05, 'epoch': 0.28, 'throughput': 703.07}\n",
            "{'loss': 0.6074, 'grad_norm': 0.2487288862466812, 'learning_rate': 9.791120991134904e-05, 'epoch': 0.28, 'num_input_tokens_seen': 176064, 'train_runtime': 250.4283, 'train_tokens_per_second': 703.052}\n",
            " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 65/639 [04:30<38:22,  4.01s/it][INFO|2025-12-05 16:40:43] llamafactory.train.callbacks:143 >> {'loss': 0.5677, 'learning_rate': 9.7545e-05, 'epoch': 0.31, 'throughput': 702.68}\n",
            "{'loss': 0.5677, 'grad_norm': 0.3434200882911682, 'learning_rate': 9.754522378070297e-05, 'epoch': 0.31, 'num_input_tokens_seen': 190008, 'train_runtime': 270.4088, 'train_tokens_per_second': 702.669}\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 70/639 [04:49<35:44,  3.77s/it][INFO|2025-12-05 16:41:02] llamafactory.train.callbacks:143 >> {'loss': 0.5055, 'learning_rate': 9.7151e-05, 'epoch': 0.33, 'throughput': 702.15}\n",
            "{'loss': 0.5055, 'grad_norm': 0.36478644609451294, 'learning_rate': 9.715050848107168e-05, 'epoch': 0.33, 'num_input_tokens_seen': 203000, 'train_runtime': 289.1177, 'train_tokens_per_second': 702.136}\n",
            " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 75/639 [05:10<38:38,  4.11s/it][INFO|2025-12-05 16:41:23] llamafactory.train.callbacks:143 >> {'loss': 0.5521, 'learning_rate': 9.6727e-05, 'epoch': 0.35, 'throughput': 702.10}\n",
            "{'loss': 0.5521, 'grad_norm': 0.9521490931510925, 'learning_rate': 9.67273025189053e-05, 'epoch': 0.35, 'num_input_tokens_seen': 217824, 'train_runtime': 310.2515, 'train_tokens_per_second': 702.089}\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 80/639 [05:30<38:51,  4.17s/it][INFO|2025-12-05 16:41:43] llamafactory.train.callbacks:143 >> {'loss': 0.5730, 'learning_rate': 9.6276e-05, 'epoch': 0.38, 'throughput': 701.87}\n",
            "{'loss': 0.573, 'grad_norm': 0.5956977009773254, 'learning_rate': 9.627586161611732e-05, 'epoch': 0.38, 'num_input_tokens_seen': 231792, 'train_runtime': 330.2557, 'train_tokens_per_second': 701.856}\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 85/639 [05:50<38:53,  4.21s/it][INFO|2025-12-05 16:42:03] llamafactory.train.callbacks:143 >> {'loss': 0.5542, 'learning_rate': 9.5796e-05, 'epoch': 0.40, 'throughput': 701.34}\n",
            "{'loss': 0.5542, 'grad_norm': 0.3111181855201721, 'learning_rate': 9.57964585555648e-05, 'epoch': 0.4, 'num_input_tokens_seen': 245992, 'train_runtime': 350.7533, 'train_tokens_per_second': 701.325}\n",
            " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 90/639 [06:11<36:50,  4.03s/it][INFO|2025-12-05 16:42:24] llamafactory.train.callbacks:143 >> {'loss': 0.5717, 'learning_rate': 9.5289e-05, 'epoch': 0.42, 'throughput': 700.85}\n",
            "{'loss': 0.5717, 'grad_norm': 0.28236424922943115, 'learning_rate': 9.528938301621956e-05, 'epoch': 0.42, 'num_input_tokens_seen': 260376, 'train_runtime': 371.5205, 'train_tokens_per_second': 700.839}\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 95/639 [06:32<38:09,  4.21s/it][INFO|2025-12-05 16:42:45] llamafactory.train.callbacks:143 >> {'loss': 0.5811, 'learning_rate': 9.4755e-05, 'epoch': 0.45, 'throughput': 700.51}\n",
            "{'loss': 0.5811, 'grad_norm': 0.28853729367256165, 'learning_rate': 9.475494139812979e-05, 'epoch': 0.45, 'num_input_tokens_seen': 274824, 'train_runtime': 392.3248, 'train_tokens_per_second': 700.501}\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 100/639 [06:52<37:18,  4.15s/it][INFO|2025-12-05 16:43:05] llamafactory.train.callbacks:143 >> {'loss': 0.6045, 'learning_rate': 9.4193e-05, 'epoch': 0.47, 'throughput': 701.10}\n",
            "{'loss': 0.6045, 'grad_norm': 0.5802721977233887, 'learning_rate': 9.419345663727805e-05, 'epoch': 0.47, 'num_input_tokens_seen': 289392, 'train_runtime': 412.7717, 'train_tokens_per_second': 701.095}\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 100/639 [06:52<37:18,  4.15s/it][INFO|trainer.py:4327] 2025-12-05 16:43:05,892 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4329] 2025-12-05 16:43:05,892 >>   Num examples = 150\n",
            "[INFO|trainer.py:4332] 2025-12-05 16:43:05,892 >>   Batch size = 1\n",
            "\n",
            "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|â–Œ                                          | 2/150 [00:00<00:18,  7.88it/s]\u001b[A\n",
            "  2%|â–Š                                          | 3/150 [00:00<00:26,  5.47it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 4/150 [00:00<00:35,  4.06it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 5/150 [00:01<00:37,  3.84it/s]\u001b[A\n",
            "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:38,  3.72it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:39,  3.62it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:40,  3.53it/s]\u001b[A\n",
            "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:38,  3.65it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:39,  3.58it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:39,  3.55it/s]\u001b[A\n",
            "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:39,  3.52it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:41,  3.31it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:42,  3.20it/s]\u001b[A\n",
            " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:41,  3.27it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:43,  3.05it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:41,  3.17it/s]\u001b[A\n",
            " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:40,  3.23it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:39,  3.28it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:40,  3.18it/s]\u001b[A\n",
            " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:05<00:36,  3.51it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:33,  3.81it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:36,  3.50it/s]\u001b[A\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:36,  3.48it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:36,  3.46it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:34,  3.56it/s]\u001b[A\n",
            " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:34,  3.57it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:07<00:36,  3.37it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:37,  3.21it/s]\u001b[A\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:36,  3.31it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:08<00:34,  3.46it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:30,  3.81it/s]\u001b[A\n",
            " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:34,  3.41it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:32,  3.54it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:09<00:34,  3.33it/s]\u001b[A\n",
            " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:31,  3.65it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:34,  3.25it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:10<00:33,  3.31it/s]\u001b[A\n",
            " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:34,  3.20it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:33,  3.27it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:11<00:32,  3.37it/s]\u001b[A\n",
            " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:31,  3.43it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.44it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:12<00:35,  3.03it/s]\u001b[A\n",
            " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:32,  3.18it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:34,  2.99it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:30,  3.39it/s]\u001b[A\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:13<00:30,  3.37it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.48it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:28,  3.50it/s]\u001b[A\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:14<00:27,  3.55it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:31,  3.12it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.33it/s]\u001b[A\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:15<00:28,  3.42it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:15<00:26,  3.53it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:29,  3.20it/s]\u001b[A\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:16<00:29,  3.13it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:16<00:28,  3.24it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:26,  3.37it/s]\u001b[A\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:26,  3.40it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:17<00:26,  3.39it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:25,  3.40it/s]\u001b[A\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:25,  3.39it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:18<00:23,  3.67it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:18<00:23,  3.61it/s]\u001b[A\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.34it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:19<00:27,  2.98it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:19<00:26,  3.10it/s]\u001b[A\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:23,  3.45it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:22,  3.55it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:20<00:20,  3.83it/s]\u001b[A\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:20<00:18,  4.12it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:19,  3.89it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:18,  4.09it/s]\u001b[A\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:21<00:19,  3.86it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:21,  3.38it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:19,  3.72it/s]\u001b[A\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:22<00:19,  3.69it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:22<00:19,  3.61it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.67it/s]\u001b[A\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:17,  3.93it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:23<00:18,  3.58it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:23<00:18,  3.59it/s]\u001b[A\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:19,  3.37it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:24<00:18,  3.45it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:24<00:20,  3.09it/s]\u001b[A\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:20,  3.01it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:25<00:19,  3.13it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:25<00:21,  2.85it/s]\u001b[A\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.09it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:26<00:19,  3.01it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:26<00:19,  2.98it/s]\u001b[A\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.33it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:27<00:16,  3.43it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:27<00:16,  3.42it/s]\u001b[A\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:16,  3.23it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:28<00:16,  3.30it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:28<00:15,  3.32it/s]\u001b[A\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:28<00:15,  3.20it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:29<00:16,  2.96it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:29<00:15,  3.14it/s]\u001b[A\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:29<00:13,  3.49it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:30<00:14,  3.17it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:30<00:14,  3.24it/s]\u001b[A\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:30<00:13,  3.41it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:30<00:11,  3.87it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:31<00:11,  3.69it/s]\u001b[A\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:31<00:11,  3.67it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:31<00:12,  3.28it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:32<00:12,  3.32it/s]\u001b[A\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:32<00:11,  3.36it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:32<00:11,  3.40it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:33<00:11,  3.26it/s]\u001b[A\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:33<00:10,  3.42it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:33<00:09,  3.74it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:33<00:09,  3.45it/s]\u001b[A\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:34<00:10,  3.14it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:34<00:09,  3.27it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:34<00:09,  3.33it/s]\u001b[A\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:35<00:09,  3.18it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:35<00:08,  3.29it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:35<00:08,  3.14it/s]\u001b[A\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:36<00:08,  3.32it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:36<00:07,  3.46it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:36<00:07,  3.56it/s]\u001b[A\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:36<00:06,  3.52it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:37<00:07,  3.29it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:37<00:06,  3.43it/s]\u001b[A\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:37<00:05,  3.73it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:38<00:05,  3.64it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:38<00:05,  3.40it/s]\u001b[A\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:38<00:05,  3.25it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:39<00:05,  3.39it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:39<00:04,  3.45it/s]\u001b[A\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:39<00:04,  3.51it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:39<00:04,  3.50it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:40<00:04,  3.06it/s]\u001b[A\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:40<00:03,  3.28it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:40<00:03,  3.14it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:41<00:03,  3.26it/s]\u001b[A\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:41<00:02,  3.03it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:41<00:02,  3.14it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:42<00:02,  3.24it/s]\u001b[A\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:42<00:01,  3.30it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:42<00:01,  3.37it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:42<00:01,  3.71it/s]\u001b[A\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:43<00:00,  3.77it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:43<00:00,  4.21it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:43<00:00,  4.33it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 0.5517488718032837, 'eval_runtime': 44.0942, 'eval_samples_per_second': 3.402, 'eval_steps_per_second': 3.402, 'epoch': 0.47, 'num_input_tokens_seen': 289392}\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 100/639 [07:36<37:18,  4.15s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:43<00:00,  4.09it/s]\u001b[A\n",
            "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-05 16:43:49,983 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-100\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 16:43:50,033 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 16:43:50,033 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-12-05 16:43:50,155 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-100/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-12-05 16:43:50,155 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-12-05 16:43:50,155 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-100/special_tokens_map.json\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 105/639 [07:57<1:04:05,  7.20s/it][INFO|2025-12-05 16:44:10] llamafactory.train.callbacks:143 >> {'loss': 0.5623, 'learning_rate': 9.3605e-05, 'epoch': 0.49, 'throughput': 635.91}\n",
            "{'loss': 0.5623, 'grad_norm': 0.3822658360004425, 'learning_rate': 9.360526801044752e-05, 'epoch': 0.49, 'num_input_tokens_seen': 303552, 'train_runtime': 477.3564, 'train_tokens_per_second': 635.902}\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 110/639 [08:15<38:32,  4.37s/it][INFO|2025-12-05 16:44:29] llamafactory.train.callbacks:143 >> {'loss': 0.5430, 'learning_rate': 9.2991e-05, 'epoch': 0.52, 'throughput': 637.52}\n",
            "{'loss': 0.543, 'grad_norm': 0.5196649432182312, 'learning_rate': 9.299073093021405e-05, 'epoch': 0.52, 'num_input_tokens_seen': 316176, 'train_runtime': 495.9551, 'train_tokens_per_second': 637.509}\n",
            " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 115/639 [08:36<36:14,  4.15s/it][INFO|2025-12-05 16:44:49] llamafactory.train.callbacks:143 >> {'loss': 0.5576, 'learning_rate': 9.2350e-05, 'epoch': 0.54, 'throughput': 639.87}\n",
            "{'loss': 0.5576, 'grad_norm': 0.321527361869812, 'learning_rate': 9.235021673018849e-05, 'epoch': 0.54, 'num_input_tokens_seen': 330344, 'train_runtime': 516.2693, 'train_tokens_per_second': 639.868}\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 120/639 [08:57<38:28,  4.45s/it][INFO|2025-12-05 16:45:10] llamafactory.train.callbacks:143 >> {'loss': 0.5372, 'learning_rate': 9.1684e-05, 'epoch': 0.56, 'throughput': 642.83}\n",
            "{'loss': 0.5372, 'grad_norm': 0.2928081154823303, 'learning_rate': 9.168411244063863e-05, 'epoch': 0.56, 'num_input_tokens_seen': 345608, 'train_runtime': 537.6415, 'train_tokens_per_second': 642.822}\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 125/639 [09:17<34:39,  4.05s/it][INFO|2025-12-05 16:45:30] llamafactory.train.callbacks:143 >> {'loss': 0.5997, 'learning_rate': 9.0993e-05, 'epoch': 0.59, 'throughput': 644.48}\n",
            "{'loss': 0.5997, 'grad_norm': 0.468326598405838, 'learning_rate': 9.09928205546263e-05, 'epoch': 0.59, 'num_input_tokens_seen': 359168, 'train_runtime': 557.3021, 'train_tokens_per_second': 644.476}\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 130/639 [09:40<40:05,  4.73s/it][INFO|2025-12-05 16:45:53] llamafactory.train.callbacks:143 >> {'loss': 0.6049, 'learning_rate': 9.0277e-05, 'epoch': 0.61, 'throughput': 647.08}\n",
            "{'loss': 0.6049, 'grad_norm': 0.35992908477783203, 'learning_rate': 9.027675878480131e-05, 'epoch': 0.61, 'num_input_tokens_seen': 375824, 'train_runtime': 580.8035, 'train_tokens_per_second': 647.076}\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 135/639 [10:00<34:32,  4.11s/it][INFO|2025-12-05 16:46:13] llamafactory.train.callbacks:143 >> {'loss': 0.5706, 'learning_rate': 8.9536e-05, 'epoch': 0.64, 'throughput': 648.79}\n",
            "{'loss': 0.5706, 'grad_norm': 0.46006250381469727, 'learning_rate': 8.953635981099887e-05, 'epoch': 0.64, 'num_input_tokens_seen': 389832, 'train_runtime': 600.8661, 'train_tokens_per_second': 648.783}\n",
            " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 140/639 [10:21<34:34,  4.16s/it][INFO|2025-12-05 16:46:34] llamafactory.train.callbacks:143 >> {'loss': 0.5997, 'learning_rate': 8.8772e-05, 'epoch': 0.66, 'throughput': 650.43}\n",
            "{'loss': 0.5997, 'grad_norm': 0.416066974401474, 'learning_rate': 8.877207101879302e-05, 'epoch': 0.66, 'num_input_tokens_seen': 404320, 'train_runtime': 621.6279, 'train_tokens_per_second': 650.421}\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 145/639 [10:42<35:06,  4.26s/it][INFO|2025-12-05 16:46:55] llamafactory.train.callbacks:143 >> {'loss': 0.5647, 'learning_rate': 8.7984e-05, 'epoch': 0.68, 'throughput': 652.35}\n",
            "{'loss': 0.5647, 'grad_norm': 0.28357335925102234, 'learning_rate': 8.798435422916425e-05, 'epoch': 0.68, 'num_input_tokens_seen': 419128, 'train_runtime': 642.5, 'train_tokens_per_second': 652.339}\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 150/639 [11:04<34:27,  4.23s/it][INFO|2025-12-05 16:47:17] llamafactory.train.callbacks:143 >> {'loss': 0.5706, 'learning_rate': 8.7174e-05, 'epoch': 0.71, 'throughput': 653.85}\n",
            "{'loss': 0.5706, 'grad_norm': 0.3501749336719513, 'learning_rate': 8.717368541944452e-05, 'epoch': 0.71, 'num_input_tokens_seen': 434248, 'train_runtime': 664.1431, 'train_tokens_per_second': 653.847}\n",
            " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 155/639 [11:25<33:52,  4.20s/it][INFO|2025-12-05 16:47:38] llamafactory.train.callbacks:143 >> {'loss': 0.5784, 'learning_rate': 8.6341e-05, 'epoch': 0.73, 'throughput': 655.52}\n",
            "{'loss': 0.5784, 'grad_norm': 0.3737603724002838, 'learning_rate': 8.634055443570826e-05, 'epoch': 0.73, 'num_input_tokens_seen': 449104, 'train_runtime': 685.1114, 'train_tokens_per_second': 655.52}\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 160/639 [11:45<32:30,  4.07s/it][INFO|2025-12-05 16:47:58] llamafactory.train.callbacks:143 >> {'loss': 0.5315, 'learning_rate': 8.5485e-05, 'epoch': 0.75, 'throughput': 656.52}\n",
            "{'loss': 0.5315, 'grad_norm': 0.4209366738796234, 'learning_rate': 8.548546469678311e-05, 'epoch': 0.75, 'num_input_tokens_seen': 463328, 'train_runtime': 705.7336, 'train_tokens_per_second': 656.52}\n",
            " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 165/639 [12:04<30:32,  3.87s/it][INFO|2025-12-05 16:48:18] llamafactory.train.callbacks:143 >> {'loss': 0.5623, 'learning_rate': 8.4609e-05, 'epoch': 0.78, 'throughput': 657.63}\n",
            "{'loss': 0.5623, 'grad_norm': 0.4078158140182495, 'learning_rate': 8.460893289005965e-05, 'epoch': 0.78, 'num_input_tokens_seen': 476752, 'train_runtime': 724.9607, 'train_tokens_per_second': 657.625}\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 170/639 [12:25<30:50,  3.94s/it][INFO|2025-12-05 16:48:38] llamafactory.train.callbacks:143 >> {'loss': 0.6175, 'learning_rate': 8.3711e-05, 'epoch': 0.80, 'throughput': 658.60}\n",
            "{'loss': 0.6175, 'grad_norm': 0.7568594813346863, 'learning_rate': 8.371148865928319e-05, 'epoch': 0.8, 'num_input_tokens_seen': 490888, 'train_runtime': 745.3591, 'train_tokens_per_second': 658.593}\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 175/639 [12:45<30:11,  3.90s/it][INFO|2025-12-05 16:48:58] llamafactory.train.callbacks:143 >> {'loss': 0.5636, 'learning_rate': 8.2794e-05, 'epoch': 0.82, 'throughput': 659.66}\n",
            "{'loss': 0.5636, 'grad_norm': 0.4207252860069275, 'learning_rate': 8.279367428451702e-05, 'epoch': 0.82, 'num_input_tokens_seen': 504944, 'train_runtime': 765.472, 'train_tokens_per_second': 659.651}\n",
            " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 180/639 [13:06<32:49,  4.29s/it][INFO|2025-12-05 16:49:19] llamafactory.train.callbacks:143 >> {'loss': 0.5857, 'learning_rate': 8.1856e-05, 'epoch': 0.85, 'throughput': 660.55}\n",
            "{'loss': 0.5857, 'grad_norm': 0.3625451624393463, 'learning_rate': 8.185604435447002e-05, 'epoch': 0.85, 'num_input_tokens_seen': 519432, 'train_runtime': 786.3678, 'train_tokens_per_second': 660.546}\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 185/639 [13:26<30:46,  4.07s/it][INFO|2025-12-05 16:49:39] llamafactory.train.callbacks:143 >> {'loss': 0.5548, 'learning_rate': 8.0899e-05, 'epoch': 0.87, 'throughput': 661.28}\n",
            "{'loss': 0.5548, 'grad_norm': 0.44318634271621704, 'learning_rate': 8.089916543138681e-05, 'epoch': 0.87, 'num_input_tokens_seen': 533328, 'train_runtime': 806.5133, 'train_tokens_per_second': 661.276}\n",
            " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 190/639 [13:46<30:40,  4.10s/it][INFO|2025-12-05 16:49:59] llamafactory.train.callbacks:143 >> {'loss': 0.5771, 'learning_rate': 7.9924e-05, 'epoch': 0.89, 'throughput': 662.44}\n",
            "{'loss': 0.5771, 'grad_norm': 0.4291533827781677, 'learning_rate': 7.992361570870288e-05, 'epoch': 0.89, 'num_input_tokens_seen': 547712, 'train_runtime': 826.8127, 'train_tokens_per_second': 662.438}\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 195/639 [14:07<30:22,  4.11s/it][INFO|2025-12-05 16:50:20] llamafactory.train.callbacks:143 >> {'loss': 0.5774, 'learning_rate': 7.8930e-05, 'epoch': 0.92, 'throughput': 663.14}\n",
            "{'loss': 0.5774, 'grad_norm': 0.3444051146507263, 'learning_rate': 7.892998466167165e-05, 'epoch': 0.92, 'num_input_tokens_seen': 562024, 'train_runtime': 847.5225, 'train_tokens_per_second': 663.138}\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 200/639 [14:27<29:52,  4.08s/it][INFO|2025-12-05 16:50:41] llamafactory.train.callbacks:143 >> {'loss': 0.6603, 'learning_rate': 7.7919e-05, 'epoch': 0.94, 'throughput': 663.87}\n",
            "{'loss': 0.6603, 'grad_norm': 0.4485686719417572, 'learning_rate': 7.791887269117442e-05, 'epoch': 0.94, 'num_input_tokens_seen': 576232, 'train_runtime': 867.9964, 'train_tokens_per_second': 663.865}\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 200/639 [14:27<29:52,  4.08s/it][INFO|trainer.py:4327] 2025-12-05 16:50:41,117 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4329] 2025-12-05 16:50:41,117 >>   Num examples = 150\n",
            "[INFO|trainer.py:4332] 2025-12-05 16:50:41,117 >>   Batch size = 1\n",
            "\n",
            "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|â–Œ                                          | 2/150 [00:00<00:18,  7.80it/s]\u001b[A\n",
            "  2%|â–Š                                          | 3/150 [00:00<00:26,  5.46it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 4/150 [00:00<00:36,  4.05it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 5/150 [00:01<00:37,  3.84it/s]\u001b[A\n",
            "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:38,  3.72it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:39,  3.61it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:40,  3.53it/s]\u001b[A\n",
            "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:38,  3.65it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:38,  3.59it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:39,  3.55it/s]\u001b[A\n",
            "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:39,  3.52it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:41,  3.31it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:42,  3.19it/s]\u001b[A\n",
            " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:41,  3.26it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:43,  3.06it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:42,  3.16it/s]\u001b[A\n",
            " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:40,  3.23it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:39,  3.29it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:40,  3.18it/s]\u001b[A\n",
            " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:05<00:36,  3.51it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:33,  3.82it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:36,  3.50it/s]\u001b[A\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:36,  3.48it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:36,  3.45it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:34,  3.57it/s]\u001b[A\n",
            " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:34,  3.58it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:07<00:36,  3.38it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:37,  3.22it/s]\u001b[A\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:36,  3.32it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:08<00:34,  3.46it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:30,  3.81it/s]\u001b[A\n",
            " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:34,  3.41it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:32,  3.55it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:09<00:34,  3.34it/s]\u001b[A\n",
            " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:31,  3.65it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:34,  3.26it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:10<00:33,  3.31it/s]\u001b[A\n",
            " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:34,  3.20it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:33,  3.27it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:11<00:32,  3.37it/s]\u001b[A\n",
            " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:31,  3.44it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:30,  3.45it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:12<00:34,  3.03it/s]\u001b[A\n",
            " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:32,  3.19it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:34,  2.99it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:30,  3.38it/s]\u001b[A\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:13<00:30,  3.37it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.48it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:28,  3.50it/s]\u001b[A\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:14<00:27,  3.55it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:31,  3.12it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.33it/s]\u001b[A\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:15<00:27,  3.43it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:15<00:26,  3.54it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:29,  3.20it/s]\u001b[A\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:16<00:29,  3.13it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:16<00:28,  3.23it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:26,  3.37it/s]\u001b[A\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:26,  3.40it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:17<00:26,  3.39it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:25,  3.39it/s]\u001b[A\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:25,  3.39it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:18<00:23,  3.67it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:18<00:23,  3.61it/s]\u001b[A\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.34it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:19<00:27,  2.98it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:19<00:26,  3.11it/s]\u001b[A\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:23,  3.45it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:22,  3.55it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:20<00:20,  3.82it/s]\u001b[A\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:20<00:18,  4.11it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:19,  3.90it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:18,  4.08it/s]\u001b[A\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:21<00:19,  3.84it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:21,  3.37it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:19,  3.72it/s]\u001b[A\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:22<00:19,  3.68it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:22<00:19,  3.61it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.67it/s]\u001b[A\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:17,  3.92it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:23<00:19,  3.57it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:23<00:18,  3.59it/s]\u001b[A\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:19,  3.36it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:24<00:18,  3.44it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:24<00:20,  3.09it/s]\u001b[A\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:20,  3.01it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:25<00:19,  3.13it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:25<00:21,  2.85it/s]\u001b[A\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.08it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:26<00:19,  3.02it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:26<00:19,  2.98it/s]\u001b[A\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.34it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:27<00:16,  3.43it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:27<00:16,  3.42it/s]\u001b[A\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:16,  3.23it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:28<00:16,  3.30it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:28<00:15,  3.33it/s]\u001b[A\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:28<00:15,  3.21it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:29<00:16,  2.97it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:29<00:15,  3.15it/s]\u001b[A\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:29<00:13,  3.49it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:30<00:14,  3.17it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:30<00:14,  3.25it/s]\u001b[A\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:30<00:13,  3.42it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:30<00:11,  3.88it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:31<00:11,  3.70it/s]\u001b[A\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:31<00:11,  3.67it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:31<00:12,  3.28it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:32<00:12,  3.32it/s]\u001b[A\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:32<00:11,  3.36it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:32<00:11,  3.40it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:33<00:11,  3.25it/s]\u001b[A\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:33<00:10,  3.42it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:33<00:09,  3.73it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:33<00:09,  3.45it/s]\u001b[A\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:34<00:10,  3.15it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:34<00:09,  3.27it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:34<00:09,  3.33it/s]\u001b[A\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:35<00:09,  3.18it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:35<00:08,  3.30it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:35<00:08,  3.15it/s]\u001b[A\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:36<00:08,  3.32it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:36<00:07,  3.46it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:36<00:07,  3.56it/s]\u001b[A\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:36<00:06,  3.52it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:37<00:06,  3.29it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:37<00:06,  3.44it/s]\u001b[A\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:37<00:05,  3.74it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:38<00:05,  3.65it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:38<00:05,  3.40it/s]\u001b[A\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:38<00:05,  3.26it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:39<00:04,  3.40it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:39<00:04,  3.46it/s]\u001b[A\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:39<00:04,  3.52it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:39<00:03,  3.51it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:40<00:04,  3.07it/s]\u001b[A\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:40<00:03,  3.28it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:40<00:03,  3.14it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:41<00:03,  3.26it/s]\u001b[A\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:41<00:02,  3.04it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:41<00:02,  3.15it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:42<00:02,  3.24it/s]\u001b[A\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:42<00:01,  3.30it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:42<00:01,  3.37it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:42<00:01,  3.71it/s]\u001b[A\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:43<00:00,  3.77it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:43<00:00,  4.21it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:43<00:00,  4.34it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 0.5446243286132812, 'eval_runtime': 44.0695, 'eval_samples_per_second': 3.404, 'eval_steps_per_second': 3.404, 'epoch': 0.94, 'num_input_tokens_seen': 576232}\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 200/639 [15:12<29:52,  4.08s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:43<00:00,  4.10it/s]\u001b[A\n",
            "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-05 16:51:25,183 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-200\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 16:51:25,232 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 16:51:25,233 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-12-05 16:51:25,350 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-200/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-12-05 16:51:25,350 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-12-05 16:51:25,350 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-200/special_tokens_map.json\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 205/639 [15:33<51:59,  7.19s/it][INFO|2025-12-05 16:51:46] llamafactory.train.callbacks:143 >> {'loss': 0.5863, 'learning_rate': 7.6891e-05, 'epoch': 0.96, 'throughput': 633.12}\n",
            "{'loss': 0.5863, 'grad_norm': 0.5526119470596313, 'learning_rate': 7.68908907609285e-05, 'epoch': 0.96, 'num_input_tokens_seen': 590944, 'train_runtime': 933.3953, 'train_tokens_per_second': 633.112}\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 210/639 [15:54<33:15,  4.65s/it][INFO|2025-12-05 16:52:07] llamafactory.train.callbacks:143 >> {'loss': 0.5886, 'learning_rate': 7.5847e-05, 'epoch': 0.99, 'throughput': 634.58}\n",
            "{'loss': 0.5886, 'grad_norm': 0.4210559129714966, 'learning_rate': 7.584666002831296e-05, 'epoch': 0.99, 'num_input_tokens_seen': 605408, 'train_runtime': 954.0392, 'train_tokens_per_second': 634.573}\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 215/639 [16:11<27:17,  3.86s/it][INFO|2025-12-05 16:52:24] llamafactory.train.callbacks:143 >> {'loss': 0.4728, 'learning_rate': 7.4787e-05, 'epoch': 1.01, 'throughput': 635.44}\n",
            "{'loss': 0.4728, 'grad_norm': 0.5060725212097168, 'learning_rate': 7.478681146903448e-05, 'epoch': 1.01, 'num_input_tokens_seen': 617520, 'train_runtime': 971.8081, 'train_tokens_per_second': 635.434}\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 220/639 [16:33<29:29,  4.22s/it][INFO|2025-12-05 16:52:46] llamafactory.train.callbacks:143 >> {'loss': 0.4469, 'learning_rate': 7.3712e-05, 'epoch': 1.03, 'throughput': 637.04}\n",
            "{'loss': 0.4469, 'grad_norm': 0.3605037331581116, 'learning_rate': 7.371198549586091e-05, 'epoch': 1.03, 'num_input_tokens_seen': 633016, 'train_runtime': 993.6893, 'train_tokens_per_second': 637.036}\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 225/639 [16:55<29:30,  4.28s/it][INFO|2025-12-05 16:53:08] llamafactory.train.callbacks:143 >> {'loss': 0.3989, 'learning_rate': 7.2623e-05, 'epoch': 1.06, 'throughput': 638.52}\n",
            "{'loss': 0.3989, 'grad_norm': 0.4293307662010193, 'learning_rate': 7.262283157165219e-05, 'epoch': 1.06, 'num_input_tokens_seen': 648120, 'train_runtime': 1015.038, 'train_tokens_per_second': 638.518}\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 230/639 [17:15<27:28,  4.03s/it][INFO|2025-12-05 16:53:28] llamafactory.train.callbacks:143 >> {'loss': 0.4687, 'learning_rate': 7.1520e-05, 'epoch': 1.08, 'throughput': 639.68}\n",
            "{'loss': 0.4687, 'grad_norm': 0.4180750250816345, 'learning_rate': 7.152000781692286e-05, 'epoch': 1.08, 'num_input_tokens_seen': 662304, 'train_runtime': 1035.3802, 'train_tokens_per_second': 639.672}\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 235/639 [17:37<29:33,  4.39s/it][INFO|2025-12-05 16:53:50] llamafactory.train.callbacks:143 >> {'loss': 0.4310, 'learning_rate': 7.0404e-05, 'epoch': 1.10, 'throughput': 641.18}\n",
            "{'loss': 0.431, 'grad_norm': 0.44979315996170044, 'learning_rate': 7.040418061217325e-05, 'epoch': 1.1, 'num_input_tokens_seen': 677896, 'train_runtime': 1057.2647, 'train_tokens_per_second': 641.179}\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 240/639 [17:58<28:16,  4.25s/it][INFO|2025-12-05 16:54:11] llamafactory.train.callbacks:143 >> {'loss': 0.4294, 'learning_rate': 6.9276e-05, 'epoch': 1.13, 'throughput': 642.35}\n",
            "{'loss': 0.4294, 'grad_norm': 0.4801521897315979, 'learning_rate': 6.927602419522947e-05, 'epoch': 1.13, 'num_input_tokens_seen': 692648, 'train_runtime': 1078.3123, 'train_tokens_per_second': 642.345}\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 245/639 [18:18<25:58,  3.95s/it][INFO|2025-12-05 16:54:31] llamafactory.train.callbacks:143 >> {'loss': 0.4224, 'learning_rate': 6.8136e-05, 'epoch': 1.15, 'throughput': 643.33}\n",
            "{'loss': 0.4224, 'grad_norm': 0.4224030673503876, 'learning_rate': 6.813622025383565e-05, 'epoch': 1.15, 'num_input_tokens_seen': 706472, 'train_runtime': 1098.1544, 'train_tokens_per_second': 643.327}\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 250/639 [18:37<24:57,  3.85s/it][INFO|2025-12-05 16:54:50] llamafactory.train.callbacks:143 >> {'loss': 0.3871, 'learning_rate': 6.6985e-05, 'epoch': 1.17, 'throughput': 644.17}\n",
            "{'loss': 0.3871, 'grad_norm': 0.4873518645763397, 'learning_rate': 6.698545751374465e-05, 'epoch': 1.17, 'num_input_tokens_seen': 719920, 'train_runtime': 1117.5965, 'train_tokens_per_second': 644.168}\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 255/639 [18:56<24:05,  3.76s/it][INFO|2025-12-05 16:55:09] llamafactory.train.callbacks:143 >> {'loss': 0.3701, 'learning_rate': 6.5824e-05, 'epoch': 1.20, 'throughput': 644.76}\n",
            "{'loss': 0.3701, 'grad_norm': 0.7170208692550659, 'learning_rate': 6.582443132255592e-05, 'epoch': 1.2, 'num_input_tokens_seen': 732888, 'train_runtime': 1136.6889, 'train_tokens_per_second': 644.757}\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 260/639 [19:16<24:17,  3.84s/it][INFO|2025-12-05 16:55:29] llamafactory.train.callbacks:143 >> {'loss': 0.3777, 'learning_rate': 6.4654e-05, 'epoch': 1.22, 'throughput': 645.72}\n",
            "{'loss': 0.3777, 'grad_norm': 0.6889306902885437, 'learning_rate': 6.465384322955224e-05, 'epoch': 1.22, 'num_input_tokens_seen': 746904, 'train_runtime': 1156.7065, 'train_tokens_per_second': 645.716}\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 265/639 [19:37<26:07,  4.19s/it][INFO|2025-12-05 16:55:51] llamafactory.train.callbacks:143 >> {'loss': 0.3584, 'learning_rate': 6.3474e-05, 'epoch': 1.24, 'throughput': 646.96}\n",
            "{'loss': 0.3584, 'grad_norm': 0.40200626850128174, 'learning_rate': 6.347440056178904e-05, 'epoch': 1.24, 'num_input_tokens_seen': 762040, 'train_runtime': 1177.8852, 'train_tokens_per_second': 646.956}\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 270/639 [19:58<26:20,  4.28s/it][INFO|2025-12-05 16:56:12] llamafactory.train.callbacks:143 >> {'loss': 0.4179, 'learning_rate': 6.2287e-05, 'epoch': 1.27, 'throughput': 647.79}\n",
            "{'loss': 0.4179, 'grad_norm': 0.38500693440437317, 'learning_rate': 6.228681599669248e-05, 'epoch': 1.27, 'num_input_tokens_seen': 776704, 'train_runtime': 1199.0051, 'train_tokens_per_second': 647.79}\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 275/639 [20:18<22:57,  3.79s/it][INFO|2025-12-05 16:56:31] llamafactory.train.callbacks:143 >> {'loss': 0.3796, 'learning_rate': 6.1092e-05, 'epoch': 1.29, 'throughput': 648.44}\n",
            "{'loss': 0.3796, 'grad_norm': 0.5141023397445679, 'learning_rate': 6.109180713142465e-05, 'epoch': 1.29, 'num_input_tokens_seen': 789840, 'train_runtime': 1218.0713, 'train_tokens_per_second': 648.435}\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 280/639 [20:38<24:19,  4.06s/it][INFO|2025-12-05 16:56:51] llamafactory.train.callbacks:143 >> {'loss': 0.3855, 'learning_rate': 5.9890e-05, 'epoch': 1.32, 'throughput': 649.56}\n",
            "{'loss': 0.3855, 'grad_norm': 0.4194045066833496, 'learning_rate': 5.989009604927587e-05, 'epoch': 1.32, 'num_input_tokens_seen': 804600, 'train_runtime': 1238.6854, 'train_tokens_per_second': 649.56}\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 285/639 [20:59<24:08,  4.09s/it][INFO|2025-12-05 16:57:12] llamafactory.train.callbacks:143 >> {'loss': 0.3766, 'learning_rate': 5.8682e-05, 'epoch': 1.34, 'throughput': 650.36}\n",
            "{'loss': 0.3766, 'grad_norm': 0.5102554559707642, 'learning_rate': 5.868240888334653e-05, 'epoch': 1.34, 'num_input_tokens_seen': 818976, 'train_runtime': 1259.2728, 'train_tokens_per_second': 650.356}\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 290/639 [21:21<25:59,  4.47s/it][INFO|2025-12-05 16:57:34] llamafactory.train.callbacks:143 >> {'loss': 0.4189, 'learning_rate': 5.7469e-05, 'epoch': 1.36, 'throughput': 651.21}\n",
            "{'loss': 0.4189, 'grad_norm': 0.4565547704696655, 'learning_rate': 5.74694753777815e-05, 'epoch': 1.36, 'num_input_tokens_seen': 834520, 'train_runtime': 1281.4914, 'train_tokens_per_second': 651.21}\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 295/639 [21:42<24:24,  4.26s/it][INFO|2025-12-05 16:57:55] llamafactory.train.callbacks:143 >> {'loss': 0.4033, 'learning_rate': 5.6252e-05, 'epoch': 1.39, 'throughput': 652.17}\n",
            "{'loss': 0.4033, 'grad_norm': 0.47303926944732666, 'learning_rate': 5.62520284468228e-05, 'epoch': 1.39, 'num_input_tokens_seen': 849544, 'train_runtime': 1302.6494, 'train_tokens_per_second': 652.166}\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 300/639 [22:04<24:40,  4.37s/it][INFO|2025-12-05 16:58:17] llamafactory.train.callbacks:143 >> {'loss': 0.4065, 'learning_rate': 5.5031e-05, 'epoch': 1.41, 'throughput': 652.92}\n",
            "{'loss': 0.4065, 'grad_norm': 0.4113789498806, 'learning_rate': 5.5030803731946665e-05, 'epoch': 1.41, 'num_input_tokens_seen': 864576, 'train_runtime': 1324.1718, 'train_tokens_per_second': 652.918}\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 300/639 [22:04<24:40,  4.37s/it][INFO|trainer.py:4327] 2025-12-05 16:58:17,292 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4329] 2025-12-05 16:58:17,293 >>   Num examples = 150\n",
            "[INFO|trainer.py:4332] 2025-12-05 16:58:17,293 >>   Batch size = 1\n",
            "\n",
            "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|â–Œ                                          | 2/150 [00:00<00:18,  7.79it/s]\u001b[A\n",
            "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.44it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 4/150 [00:00<00:35,  4.06it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 5/150 [00:01<00:37,  3.83it/s]\u001b[A\n",
            "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:38,  3.71it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:39,  3.61it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:40,  3.53it/s]\u001b[A\n",
            "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:38,  3.65it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:39,  3.59it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:39,  3.55it/s]\u001b[A\n",
            "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:39,  3.52it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:41,  3.31it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:42,  3.20it/s]\u001b[A\n",
            " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:41,  3.27it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:43,  3.06it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:42,  3.16it/s]\u001b[A\n",
            " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:40,  3.23it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:39,  3.29it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:40,  3.18it/s]\u001b[A\n",
            " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:05<00:36,  3.52it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:33,  3.82it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:36,  3.50it/s]\u001b[A\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:36,  3.48it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:36,  3.46it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:34,  3.57it/s]\u001b[A\n",
            " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:34,  3.58it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:07<00:36,  3.37it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:37,  3.22it/s]\u001b[A\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:36,  3.32it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:08<00:34,  3.46it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.81it/s]\u001b[A\n",
            " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:34,  3.41it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:32,  3.54it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:09<00:34,  3.33it/s]\u001b[A\n",
            " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:31,  3.65it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:34,  3.26it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:10<00:33,  3.31it/s]\u001b[A\n",
            " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:34,  3.20it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:33,  3.28it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:11<00:32,  3.37it/s]\u001b[A\n",
            " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:31,  3.44it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:30,  3.46it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:12<00:34,  3.04it/s]\u001b[A\n",
            " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:32,  3.19it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:34,  2.99it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:30,  3.38it/s]\u001b[A\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:13<00:30,  3.37it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.47it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:28,  3.50it/s]\u001b[A\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:14<00:27,  3.55it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:31,  3.12it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.33it/s]\u001b[A\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:15<00:27,  3.43it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:15<00:26,  3.54it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:29,  3.20it/s]\u001b[A\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:16<00:29,  3.12it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:16<00:28,  3.23it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:26,  3.37it/s]\u001b[A\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:26,  3.39it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:17<00:26,  3.39it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:25,  3.39it/s]\u001b[A\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:25,  3.39it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:18<00:23,  3.67it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:18<00:23,  3.61it/s]\u001b[A\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.34it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:19<00:27,  2.98it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:19<00:26,  3.10it/s]\u001b[A\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:23,  3.45it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:22,  3.55it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:20<00:20,  3.82it/s]\u001b[A\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:20<00:19,  4.10it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:19,  3.89it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:18,  4.08it/s]\u001b[A\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:21<00:19,  3.86it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:21,  3.37it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:19,  3.71it/s]\u001b[A\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:22<00:19,  3.69it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:22<00:19,  3.61it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.67it/s]\u001b[A\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:17,  3.94it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:23<00:18,  3.58it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:23<00:18,  3.59it/s]\u001b[A\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:19,  3.36it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:24<00:18,  3.44it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:24<00:20,  3.09it/s]\u001b[A\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:20,  3.01it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:25<00:19,  3.13it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:25<00:21,  2.85it/s]\u001b[A\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.08it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:26<00:19,  3.01it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:26<00:19,  2.99it/s]\u001b[A\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.34it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:27<00:16,  3.43it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:27<00:16,  3.42it/s]\u001b[A\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:16,  3.23it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:28<00:16,  3.30it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:28<00:15,  3.33it/s]\u001b[A\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:28<00:15,  3.21it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:29<00:16,  2.97it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:29<00:15,  3.15it/s]\u001b[A\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:29<00:13,  3.49it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:30<00:14,  3.17it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:30<00:14,  3.24it/s]\u001b[A\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:30<00:13,  3.41it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:30<00:11,  3.88it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:31<00:11,  3.70it/s]\u001b[A\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:31<00:11,  3.67it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:31<00:12,  3.28it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:32<00:12,  3.32it/s]\u001b[A\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:32<00:11,  3.35it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:32<00:11,  3.40it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:33<00:11,  3.26it/s]\u001b[A\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:33<00:10,  3.43it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:33<00:09,  3.74it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:33<00:09,  3.46it/s]\u001b[A\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:34<00:10,  3.15it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:34<00:09,  3.28it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:34<00:09,  3.34it/s]\u001b[A\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:35<00:09,  3.18it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:35<00:08,  3.30it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:35<00:08,  3.15it/s]\u001b[A\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:36<00:08,  3.32it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:36<00:07,  3.46it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:36<00:07,  3.56it/s]\u001b[A\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:36<00:06,  3.52it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:37<00:06,  3.29it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:37<00:06,  3.44it/s]\u001b[A\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:37<00:05,  3.74it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:38<00:05,  3.64it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:38<00:05,  3.41it/s]\u001b[A\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:38<00:05,  3.26it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:39<00:04,  3.41it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:39<00:04,  3.47it/s]\u001b[A\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:39<00:04,  3.52it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:39<00:04,  3.50it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:40<00:04,  3.07it/s]\u001b[A\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:40<00:03,  3.28it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:40<00:03,  3.15it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:41<00:03,  3.27it/s]\u001b[A\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:41<00:02,  3.05it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:41<00:02,  3.16it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:42<00:02,  3.25it/s]\u001b[A\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:42<00:01,  3.31it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:42<00:01,  3.37it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:42<00:01,  3.71it/s]\u001b[A\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:43<00:00,  3.77it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:43<00:00,  4.20it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:43<00:00,  4.32it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 0.5772320628166199, 'eval_runtime': 44.0692, 'eval_samples_per_second': 3.404, 'eval_steps_per_second': 3.404, 'epoch': 1.41, 'num_input_tokens_seen': 864576}\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 300/639 [22:48<24:40,  4.37s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:43<00:00,  4.09it/s]\u001b[A\n",
            "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-05 16:59:01,358 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-300\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 16:59:01,407 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 16:59:01,407 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-12-05 16:59:01,525 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-300/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-12-05 16:59:01,525 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-300/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-12-05 16:59:01,525 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-300/special_tokens_map.json\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 305/639 [23:09<41:14,  7.41s/it][INFO|2025-12-05 16:59:22] llamafactory.train.callbacks:143 >> {'loss': 0.3736, 'learning_rate': 5.3807e-05, 'epoch': 1.43, 'throughput': 632.75}\n",
            "{'loss': 0.3736, 'grad_norm': 0.5207501649856567, 'learning_rate': 5.380653915735272e-05, 'epoch': 1.43, 'num_input_tokens_seen': 879120, 'train_runtime': 1389.3757, 'train_tokens_per_second': 632.745}\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 310/639 [23:29<25:48,  4.71s/it][INFO|2025-12-05 16:59:42] llamafactory.train.callbacks:143 >> {'loss': 0.3616, 'learning_rate': 5.2580e-05, 'epoch': 1.46, 'throughput': 633.61}\n",
            "{'loss': 0.3616, 'grad_norm': 0.5005728602409363, 'learning_rate': 5.2579974484073655e-05, 'epoch': 1.46, 'num_input_tokens_seen': 892880, 'train_runtime': 1409.1944, 'train_tokens_per_second': 633.61}\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 315/639 [23:49<22:50,  4.23s/it][INFO|2025-12-05 17:00:02] llamafactory.train.callbacks:143 >> {'loss': 0.3661, 'learning_rate': 5.1352e-05, 'epoch': 1.48, 'throughput': 634.46}\n",
            "{'loss': 0.3661, 'grad_norm': 0.5166866779327393, 'learning_rate': 5.1351850862975315e-05, 'epoch': 1.48, 'num_input_tokens_seen': 906960, 'train_runtime': 1429.5057, 'train_tokens_per_second': 634.457}\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 320/639 [24:08<20:11,  3.80s/it][INFO|2025-12-05 17:00:21] llamafactory.train.callbacks:143 >> {'loss': 0.3775, 'learning_rate': 5.0123e-05, 'epoch': 1.50, 'throughput': 635.24}\n",
            "{'loss': 0.3775, 'grad_norm': 0.5939179062843323, 'learning_rate': 5.0122910386916656e-05, 'epoch': 1.5, 'num_input_tokens_seen': 920144, 'train_runtime': 1448.4969, 'train_tokens_per_second': 635.241}\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 325/639 [24:28<21:20,  4.08s/it][INFO|2025-12-05 17:00:42] llamafactory.train.callbacks:143 >> {'loss': 0.3888, 'learning_rate': 4.8894e-05, 'epoch': 1.53, 'throughput': 636.20}\n",
            "{'loss': 0.3888, 'grad_norm': 0.616510808467865, 'learning_rate': 4.889389564234066e-05, 'epoch': 1.53, 'num_input_tokens_seen': 934536, 'train_runtime': 1468.9288, 'train_tokens_per_second': 636.202}\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 330/639 [24:49<21:35,  4.19s/it][INFO|2025-12-05 17:01:02] llamafactory.train.callbacks:143 >> {'loss': 0.4049, 'learning_rate': 4.7666e-05, 'epoch': 1.55, 'throughput': 637.14}\n",
            "{'loss': 0.4049, 'grad_norm': 0.43675681948661804, 'learning_rate': 4.766554926056707e-05, 'epoch': 1.55, 'num_input_tokens_seen': 948784, 'train_runtime': 1489.1337, 'train_tokens_per_second': 637.138}\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 335/639 [25:09<20:03,  3.96s/it][INFO|2025-12-05 17:01:22] llamafactory.train.callbacks:143 >> {'loss': 0.3847, 'learning_rate': 4.6439e-05, 'epoch': 1.57, 'throughput': 637.81}\n",
            "{'loss': 0.3847, 'grad_norm': 0.45452815294265747, 'learning_rate': 4.643861346905781e-05, 'epoch': 1.57, 'num_input_tokens_seen': 962552, 'train_runtime': 1509.1616, 'train_tokens_per_second': 637.806}\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 340/639 [25:29<20:20,  4.08s/it][INFO|2025-12-05 17:01:42] llamafactory.train.callbacks:143 >> {'loss': 0.4056, 'learning_rate': 4.5214e-05, 'epoch': 1.60, 'throughput': 638.56}\n",
            "{'loss': 0.4056, 'grad_norm': 0.5582865476608276, 'learning_rate': 4.521382964292663e-05, 'epoch': 1.6, 'num_input_tokens_seen': 976848, 'train_runtime': 1529.7781, 'train_tokens_per_second': 638.555}\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 345/639 [25:52<21:29,  4.39s/it][INFO|2025-12-05 17:02:05] llamafactory.train.callbacks:143 >> {'loss': 0.4295, 'learning_rate': 4.3992e-05, 'epoch': 1.62, 'throughput': 639.48}\n",
            "{'loss': 0.4295, 'grad_norm': 0.415873646736145, 'learning_rate': 4.399193785696366e-05, 'epoch': 1.62, 'num_input_tokens_seen': 992824, 'train_runtime': 1552.5469, 'train_tokens_per_second': 639.481}\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 350/639 [26:11<18:43,  3.89s/it][INFO|2025-12-05 17:02:24] llamafactory.train.callbacks:143 >> {'loss': 0.3927, 'learning_rate': 4.2774e-05, 'epoch': 1.64, 'throughput': 639.96}\n",
            "{'loss': 0.3927, 'grad_norm': 0.6131463050842285, 'learning_rate': 4.277367643844574e-05, 'epoch': 1.64, 'num_input_tokens_seen': 1005672, 'train_runtime': 1571.4714, 'train_tokens_per_second': 639.956}\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 355/639 [26:32<19:34,  4.13s/it][INFO|2025-12-05 17:02:45] llamafactory.train.callbacks:143 >> {'loss': 0.4634, 'learning_rate': 4.1560e-05, 'epoch': 1.67, 'throughput': 640.70}\n",
            "{'loss': 0.4634, 'grad_norm': 0.4992915987968445, 'learning_rate': 4.1559781521002664e-05, 'epoch': 1.67, 'num_input_tokens_seen': 1020032, 'train_runtime': 1592.0691, 'train_tokens_per_second': 640.696}\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 360/639 [26:51<18:14,  3.92s/it][INFO|2025-12-05 17:03:04] llamafactory.train.callbacks:143 >> {'loss': 0.3603, 'learning_rate': 4.0351e-05, 'epoch': 1.69, 'throughput': 641.39}\n",
            "{'loss': 0.3603, 'grad_norm': 0.4606201648712158, 'learning_rate': 4.035098659980891e-05, 'epoch': 1.69, 'num_input_tokens_seen': 1033576, 'train_runtime': 1611.4802, 'train_tokens_per_second': 641.383}\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 365/639 [27:12<18:36,  4.08s/it][INFO|2025-12-05 17:03:26] llamafactory.train.callbacks:143 >> {'loss': 0.4353, 'learning_rate': 3.9148e-05, 'epoch': 1.72, 'throughput': 642.35}\n",
            "{'loss': 0.4353, 'grad_norm': 0.520012617111206, 'learning_rate': 3.914802208836973e-05, 'epoch': 1.72, 'num_input_tokens_seen': 1048952, 'train_runtime': 1632.9883, 'train_tokens_per_second': 642.351}\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 370/639 [27:33<19:00,  4.24s/it][INFO|2025-12-05 17:03:47] llamafactory.train.callbacks:143 >> {'loss': 0.3856, 'learning_rate': 3.7952e-05, 'epoch': 1.74, 'throughput': 643.29}\n",
            "{'loss': 0.3856, 'grad_norm': 0.5544098615646362, 'learning_rate': 3.7951614877169284e-05, 'epoch': 1.74, 'num_input_tokens_seen': 1063992, 'train_runtime': 1653.9963, 'train_tokens_per_second': 643.286}\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 375/639 [27:56<19:47,  4.50s/it][INFO|2025-12-05 17:04:09] llamafactory.train.callbacks:143 >> {'loss': 0.3871, 'learning_rate': 3.6762e-05, 'epoch': 1.76, 'throughput': 644.20}\n",
            "{'loss': 0.3871, 'grad_norm': 0.36547377705574036, 'learning_rate': 3.67624878944475e-05, 'epoch': 1.76, 'num_input_tokens_seen': 1080056, 'train_runtime': 1676.5905, 'train_tokens_per_second': 644.198}\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 380/639 [28:17<18:37,  4.32s/it][INFO|2025-12-05 17:04:31] llamafactory.train.callbacks:143 >> {'loss': 0.4021, 'learning_rate': 3.5581e-05, 'epoch': 1.79, 'throughput': 644.92}\n",
            "{'loss': 0.4021, 'grad_norm': 0.5680540204048157, 'learning_rate': 3.558135966937123e-05, 'epoch': 1.79, 'num_input_tokens_seen': 1095056, 'train_runtime': 1697.9888, 'train_tokens_per_second': 644.914}\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 385/639 [28:38<17:42,  4.18s/it][INFO|2025-12-05 17:04:51] llamafactory.train.callbacks:143 >> {'loss': 0.3618, 'learning_rate': 3.4409e-05, 'epoch': 1.81, 'throughput': 645.58}\n",
            "{'loss': 0.3618, 'grad_norm': 0.45075225830078125, 'learning_rate': 3.440894389786352e-05, 'epoch': 1.81, 'num_input_tokens_seen': 1109552, 'train_runtime': 1718.6963, 'train_tokens_per_second': 645.578}\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 390/639 [28:58<16:28,  3.97s/it][INFO|2025-12-05 17:05:11] llamafactory.train.callbacks:143 >> {'loss': 0.3676, 'learning_rate': 3.3246e-05, 'epoch': 1.83, 'throughput': 646.20}\n",
            "{'loss': 0.3676, 'grad_norm': 0.6395626068115234, 'learning_rate': 3.3245949011353264e-05, 'epoch': 1.83, 'num_input_tokens_seen': 1123480, 'train_runtime': 1738.6009, 'train_tokens_per_second': 646.198}\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 395/639 [29:18<16:01,  3.94s/it][INFO|2025-12-05 17:05:31] llamafactory.train.callbacks:143 >> {'loss': 0.4139, 'learning_rate': 3.2093e-05, 'epoch': 1.86, 'throughput': 646.95}\n",
            "{'loss': 0.4139, 'grad_norm': 0.5202656388282776, 'learning_rate': 3.209307774870603e-05, 'epoch': 1.86, 'num_input_tokens_seen': 1137664, 'train_runtime': 1758.5037, 'train_tokens_per_second': 646.95}\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 400/639 [29:38<16:12,  4.07s/it][INFO|2025-12-05 17:05:51] llamafactory.train.callbacks:143 >> {'loss': 0.3432, 'learning_rate': 3.0951e-05, 'epoch': 1.88, 'throughput': 647.49}\n",
            "{'loss': 0.3432, 'grad_norm': 0.5455382466316223, 'learning_rate': 3.0951026731594635e-05, 'epoch': 1.88, 'num_input_tokens_seen': 1151792, 'train_runtime': 1778.8499, 'train_tokens_per_second': 647.493}\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 400/639 [29:38<16:12,  4.07s/it][INFO|trainer.py:4327] 2025-12-05 17:05:51,971 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4329] 2025-12-05 17:05:51,971 >>   Num examples = 150\n",
            "[INFO|trainer.py:4332] 2025-12-05 17:05:51,971 >>   Batch size = 1\n",
            "\n",
            "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|â–Œ                                          | 2/150 [00:00<00:18,  7.82it/s]\u001b[A\n",
            "  2%|â–Š                                          | 3/150 [00:00<00:26,  5.47it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 4/150 [00:00<00:35,  4.08it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 5/150 [00:01<00:37,  3.84it/s]\u001b[A\n",
            "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:38,  3.71it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:39,  3.61it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:40,  3.53it/s]\u001b[A\n",
            "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:38,  3.64it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:39,  3.58it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:39,  3.54it/s]\u001b[A\n",
            "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:39,  3.51it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:41,  3.30it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:42,  3.19it/s]\u001b[A\n",
            " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:41,  3.27it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:43,  3.05it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:42,  3.16it/s]\u001b[A\n",
            " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:40,  3.23it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:39,  3.29it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:40,  3.18it/s]\u001b[A\n",
            " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:05<00:36,  3.51it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:33,  3.82it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:36,  3.49it/s]\u001b[A\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:36,  3.48it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:36,  3.46it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:34,  3.57it/s]\u001b[A\n",
            " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:34,  3.58it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:07<00:36,  3.37it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:37,  3.22it/s]\u001b[A\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:36,  3.32it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:08<00:34,  3.46it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:30,  3.81it/s]\u001b[A\n",
            " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:34,  3.40it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:32,  3.54it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:09<00:34,  3.33it/s]\u001b[A\n",
            " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:31,  3.64it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:34,  3.25it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:10<00:33,  3.31it/s]\u001b[A\n",
            " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:34,  3.19it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:33,  3.26it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:11<00:32,  3.37it/s]\u001b[A\n",
            " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:31,  3.43it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.45it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:12<00:34,  3.03it/s]\u001b[A\n",
            " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:32,  3.19it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:34,  3.00it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:30,  3.39it/s]\u001b[A\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:13<00:30,  3.37it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:28,  3.48it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:28,  3.50it/s]\u001b[A\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:14<00:27,  3.54it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:31,  3.12it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.32it/s]\u001b[A\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:15<00:28,  3.42it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:15<00:26,  3.53it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:29,  3.20it/s]\u001b[A\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:16<00:29,  3.13it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:16<00:28,  3.24it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:26,  3.38it/s]\u001b[A\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:26,  3.40it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:17<00:26,  3.40it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:25,  3.40it/s]\u001b[A\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:25,  3.39it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:18<00:23,  3.67it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:18<00:23,  3.61it/s]\u001b[A\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.34it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:19<00:27,  2.98it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:19<00:26,  3.11it/s]\u001b[A\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:23,  3.45it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:22,  3.55it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:20<00:20,  3.82it/s]\u001b[A\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:20<00:18,  4.11it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:19,  3.89it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:18,  4.09it/s]\u001b[A\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:21<00:19,  3.85it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:21,  3.38it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:19,  3.71it/s]\u001b[A\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:22<00:19,  3.68it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:22<00:19,  3.61it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.67it/s]\u001b[A\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:17,  3.93it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:23<00:18,  3.58it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:23<00:18,  3.59it/s]\u001b[A\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:19,  3.36it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:24<00:18,  3.44it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:24<00:20,  3.09it/s]\u001b[A\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:20,  3.01it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:25<00:19,  3.13it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:25<00:21,  2.85it/s]\u001b[A\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.09it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:26<00:19,  3.01it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:26<00:19,  2.99it/s]\u001b[A\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.34it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:27<00:16,  3.43it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:27<00:16,  3.42it/s]\u001b[A\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:16,  3.23it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:28<00:16,  3.30it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:28<00:15,  3.33it/s]\u001b[A\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:28<00:15,  3.21it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:29<00:16,  2.96it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:29<00:15,  3.14it/s]\u001b[A\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:29<00:13,  3.49it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:30<00:14,  3.16it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:30<00:14,  3.24it/s]\u001b[A\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:30<00:13,  3.41it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:30<00:11,  3.86it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:31<00:11,  3.68it/s]\u001b[A\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:31<00:11,  3.67it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:31<00:12,  3.28it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:32<00:12,  3.32it/s]\u001b[A\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:32<00:11,  3.36it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:32<00:11,  3.41it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:33<00:11,  3.25it/s]\u001b[A\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:33<00:10,  3.42it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:33<00:09,  3.74it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:33<00:09,  3.46it/s]\u001b[A\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:34<00:10,  3.14it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:34<00:09,  3.27it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:34<00:09,  3.33it/s]\u001b[A\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:35<00:09,  3.18it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:35<00:08,  3.29it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:35<00:08,  3.14it/s]\u001b[A\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:36<00:08,  3.32it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:36<00:07,  3.46it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:36<00:07,  3.56it/s]\u001b[A\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:36<00:06,  3.53it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:37<00:06,  3.29it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:37<00:06,  3.44it/s]\u001b[A\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:37<00:05,  3.74it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:38<00:05,  3.65it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:38<00:05,  3.41it/s]\u001b[A\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:38<00:05,  3.27it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:39<00:04,  3.41it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:39<00:04,  3.47it/s]\u001b[A\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:39<00:04,  3.52it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:39<00:03,  3.51it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:40<00:04,  3.07it/s]\u001b[A\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:40<00:03,  3.29it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:40<00:03,  3.14it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:41<00:03,  3.26it/s]\u001b[A\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:41<00:02,  3.03it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:41<00:02,  3.14it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:42<00:02,  3.22it/s]\u001b[A\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:42<00:01,  3.29it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:42<00:01,  3.37it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:42<00:01,  3.69it/s]\u001b[A\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:43<00:00,  3.77it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:43<00:00,  4.19it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:43<00:00,  4.30it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 0.5823313593864441, 'eval_runtime': 44.092, 'eval_samples_per_second': 3.402, 'eval_steps_per_second': 3.402, 'epoch': 1.88, 'num_input_tokens_seen': 1151792}\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 400/639 [30:22<16:12,  4.07s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:43<00:00,  4.08it/s]\u001b[A\n",
            "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-05 17:06:36,059 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-400\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 17:06:36,107 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 17:06:36,108 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-12-05 17:06:36,226 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-400/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-12-05 17:06:36,226 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-12-05 17:06:36,226 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-400/special_tokens_map.json\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 405/639 [30:42<27:48,  7.13s/it][INFO|2025-12-05 17:06:55] llamafactory.train.callbacks:143 >> {'loss': 0.3587, 'learning_rate': 2.9820e-05, 'epoch': 1.90, 'throughput': 632.23}\n",
            "{'loss': 0.3587, 'grad_norm': 0.7978941202163696, 'learning_rate': 2.9820486043565854e-05, 'epoch': 1.9, 'num_input_tokens_seen': 1165048, 'train_runtime': 1842.7629, 'train_tokens_per_second': 632.229}\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 410/639 [31:03<17:27,  4.58s/it][INFO|2025-12-05 17:07:16] llamafactory.train.callbacks:143 >> {'loss': 0.3727, 'learning_rate': 2.8702e-05, 'epoch': 1.93, 'throughput': 632.96}\n",
            "{'loss': 0.3727, 'grad_norm': 0.5025384426116943, 'learning_rate': 2.870213881305802e-05, 'epoch': 1.93, 'num_input_tokens_seen': 1179328, 'train_runtime': 1863.1883, 'train_tokens_per_second': 632.962}\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 415/639 [31:23<16:11,  4.34s/it][INFO|2025-12-05 17:07:36] llamafactory.train.callbacks:143 >> {'loss': 0.4493, 'learning_rate': 2.7597e-05, 'epoch': 1.95, 'throughput': 633.57}\n",
            "{'loss': 0.4493, 'grad_norm': 0.4756803810596466, 'learning_rate': 2.7596660800621078e-05, 'epoch': 1.95, 'num_input_tokens_seen': 1193472, 'train_runtime': 1883.721, 'train_tokens_per_second': 633.572}\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 420/639 [31:43<14:41,  4.03s/it][INFO|2025-12-05 17:07:56] llamafactory.train.callbacks:143 >> {'loss': 0.3802, 'learning_rate': 2.6505e-05, 'epoch': 1.97, 'throughput': 634.12}\n",
            "{'loss': 0.3802, 'grad_norm': 0.6353713870048523, 'learning_rate': 2.650471999058875e-05, 'epoch': 1.97, 'num_input_tokens_seen': 1206920, 'train_runtime': 1903.3175, 'train_tokens_per_second': 634.114}\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 425/639 [32:04<14:12,  3.98s/it][INFO|2025-12-05 17:08:17] llamafactory.train.callbacks:143 >> {'loss': 0.3989, 'learning_rate': 2.5427e-05, 'epoch': 2.00, 'throughput': 635.06}\n",
            "{'loss': 0.3989, 'grad_norm': 0.512599766254425, 'learning_rate': 2.542697618744945e-05, 'epoch': 2.0, 'num_input_tokens_seen': 1222080, 'train_runtime': 1924.3662, 'train_tokens_per_second': 635.056}\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 430/639 [32:21<12:41,  3.64s/it][INFO|2025-12-05 17:08:34] llamafactory.train.callbacks:143 >> {'loss': 0.2457, 'learning_rate': 2.4364e-05, 'epoch': 2.02, 'throughput': 635.74}\n",
            "{'loss': 0.2457, 'grad_norm': 0.913605809211731, 'learning_rate': 2.4364080617159886e-05, 'epoch': 2.02, 'num_input_tokens_seen': 1234488, 'train_runtime': 1941.8172, 'train_tokens_per_second': 635.739}\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 435/639 [32:41<13:35,  4.00s/it][INFO|2025-12-05 17:08:54] llamafactory.train.callbacks:143 >> {'loss': 0.2235, 'learning_rate': 2.3317e-05, 'epoch': 2.04, 'throughput': 636.31}\n",
            "{'loss': 0.2235, 'grad_norm': 0.4349004328250885, 'learning_rate': 2.3316675533642214e-05, 'epoch': 2.04, 'num_input_tokens_seen': 1248328, 'train_runtime': 1961.823, 'train_tokens_per_second': 636.31}\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 440/639 [33:03<14:06,  4.25s/it][INFO|2025-12-05 17:09:16] llamafactory.train.callbacks:143 >> {'loss': 0.2387, 'learning_rate': 2.2285e-05, 'epoch': 2.07, 'throughput': 636.91}\n",
            "{'loss': 0.2387, 'grad_norm': 0.5271391868591309, 'learning_rate': 2.22853938307025e-05, 'epoch': 2.07, 'num_input_tokens_seen': 1263224, 'train_runtime': 1983.3768, 'train_tokens_per_second': 636.906}\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 445/639 [33:21<12:19,  3.81s/it][INFO|2025-12-05 17:09:34] llamafactory.train.callbacks:143 >> {'loss': 0.2714, 'learning_rate': 2.1271e-05, 'epoch': 2.09, 'throughput': 637.34}\n",
            "{'loss': 0.2714, 'grad_norm': 1.0308226346969604, 'learning_rate': 2.1270858659605158e-05, 'epoch': 2.09, 'num_input_tokens_seen': 1275792, 'train_runtime': 2001.7452, 'train_tokens_per_second': 637.34}\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 450/639 [33:40<12:10,  3.87s/it][INFO|2025-12-05 17:09:53] llamafactory.train.callbacks:143 >> {'loss': 0.2108, 'learning_rate': 2.0274e-05, 'epoch': 2.11, 'throughput': 637.73}\n",
            "{'loss': 0.2108, 'grad_norm': 0.5175082087516785, 'learning_rate': 2.0273683052534175e-05, 'epoch': 2.11, 'num_input_tokens_seen': 1288592, 'train_runtime': 2020.5824, 'train_tokens_per_second': 637.733}\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 455/639 [34:01<12:31,  4.08s/it][INFO|2025-12-05 17:10:14] llamafactory.train.callbacks:143 >> {'loss': 0.2362, 'learning_rate': 1.9294e-05, 'epoch': 2.14, 'throughput': 638.47}\n",
            "{'loss': 0.2362, 'grad_norm': 0.6105539202690125, 'learning_rate': 1.9294469552168813e-05, 'epoch': 2.14, 'num_input_tokens_seen': 1303376, 'train_runtime': 2041.3979, 'train_tokens_per_second': 638.472}\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 460/639 [34:22<11:57,  4.01s/it][INFO|2025-12-05 17:10:35] llamafactory.train.callbacks:143 >> {'loss': 0.2592, 'learning_rate': 1.8334e-05, 'epoch': 2.16, 'throughput': 639.08}\n",
            "{'loss': 0.2592, 'grad_norm': 0.5930584073066711, 'learning_rate': 1.8333809847597642e-05, 'epoch': 2.16, 'num_input_tokens_seen': 1317856, 'train_runtime': 2062.1049, 'train_tokens_per_second': 639.083}\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 465/639 [34:42<11:50,  4.08s/it][INFO|2025-12-05 17:10:55] llamafactory.train.callbacks:143 >> {'loss': 0.2211, 'learning_rate': 1.7392e-05, 'epoch': 2.18, 'throughput': 639.57}\n",
            "{'loss': 0.2211, 'grad_norm': 0.8096970319747925, 'learning_rate': 1.739228441679081e-05, 'epoch': 2.18, 'num_input_tokens_seen': 1331792, 'train_runtime': 2082.3185, 'train_tokens_per_second': 639.572}\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 470/639 [35:03<11:54,  4.23s/it][INFO|2025-12-05 17:11:16] llamafactory.train.callbacks:143 >> {'loss': 0.2206, 'learning_rate': 1.6470e-05, 'epoch': 2.21, 'throughput': 640.29}\n",
            "{'loss': 0.2206, 'grad_norm': 0.45921462774276733, 'learning_rate': 1.647046217584661e-05, 'epoch': 2.21, 'num_input_tokens_seen': 1346640, 'train_runtime': 2103.1679, 'train_tokens_per_second': 640.291}\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 475/639 [35:23<10:51,  3.97s/it][INFO|2025-12-05 17:11:36] llamafactory.train.callbacks:143 >> {'loss': 0.2263, 'learning_rate': 1.5569e-05, 'epoch': 2.23, 'throughput': 640.91}\n",
            "{'loss': 0.2263, 'grad_norm': 0.6970124840736389, 'learning_rate': 1.556890013522428e-05, 'epoch': 2.23, 'num_input_tokens_seen': 1360720, 'train_runtime': 2123.1037, 'train_tokens_per_second': 640.911}\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 480/639 [35:44<11:07,  4.20s/it][INFO|2025-12-05 17:11:57] llamafactory.train.callbacks:143 >> {'loss': 0.2307, 'learning_rate': 1.4688e-05, 'epoch': 2.25, 'throughput': 641.60}\n",
            "{'loss': 0.2307, 'grad_norm': 0.6777554154396057, 'learning_rate': 1.4688143063170923e-05, 'epoch': 2.25, 'num_input_tokens_seen': 1375984, 'train_runtime': 2144.6321, 'train_tokens_per_second': 641.594}\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 485/639 [36:05<10:20,  4.03s/it][INFO|2025-12-05 17:12:18] llamafactory.train.callbacks:143 >> {'loss': 0.2023, 'learning_rate': 1.3829e-05, 'epoch': 2.28, 'throughput': 642.25}\n",
            "{'loss': 0.2023, 'grad_norm': 0.5179945230484009, 'learning_rate': 1.3828723156545553e-05, 'epoch': 2.28, 'num_input_tokens_seen': 1390624, 'train_runtime': 2165.2472, 'train_tokens_per_second': 642.247}\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 490/639 [36:28<11:23,  4.59s/it][INFO|2025-12-05 17:12:41] llamafactory.train.callbacks:143 >> {'loss': 0.2329, 'learning_rate': 1.2991e-05, 'epoch': 2.30, 'throughput': 642.80}\n",
            "{'loss': 0.2329, 'grad_norm': 0.6251744031906128, 'learning_rate': 1.2991159719239582e-05, 'epoch': 2.3, 'num_input_tokens_seen': 1406672, 'train_runtime': 2188.3655, 'train_tokens_per_second': 642.796}\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 495/639 [36:49<10:16,  4.28s/it][INFO|2025-12-05 17:13:02] llamafactory.train.callbacks:143 >> {'loss': 0.2432, 'learning_rate': 1.2176e-05, 'epoch': 2.32, 'throughput': 643.52}\n",
            "{'loss': 0.2432, 'grad_norm': 0.7177473902702332, 'learning_rate': 1.2175958848387765e-05, 'epoch': 2.32, 'num_input_tokens_seen': 1421880, 'train_runtime': 2209.5422, 'train_tokens_per_second': 643.518}\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 500/639 [37:10<09:46,  4.22s/it][INFO|2025-12-05 17:13:23] llamafactory.train.callbacks:143 >> {'loss': 0.2355, 'learning_rate': 1.1384e-05, 'epoch': 2.35, 'throughput': 644.19}\n",
            "{'loss': 0.2355, 'grad_norm': 0.7056769728660583, 'learning_rate': 1.1383613128559306e-05, 'epoch': 2.35, 'num_input_tokens_seen': 1437056, 'train_runtime': 2230.8089, 'train_tokens_per_second': 644.186}\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 500/639 [37:10<09:46,  4.22s/it][INFO|trainer.py:4327] 2025-12-05 17:13:23,929 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4329] 2025-12-05 17:13:23,930 >>   Num examples = 150\n",
            "[INFO|trainer.py:4332] 2025-12-05 17:13:23,930 >>   Batch size = 1\n",
            "\n",
            "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|â–Œ                                          | 2/150 [00:00<00:18,  7.86it/s]\u001b[A\n",
            "  2%|â–Š                                          | 3/150 [00:00<00:26,  5.47it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 4/150 [00:00<00:35,  4.07it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 5/150 [00:01<00:37,  3.84it/s]\u001b[A\n",
            "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:38,  3.72it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:39,  3.62it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:40,  3.53it/s]\u001b[A\n",
            "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:38,  3.65it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:39,  3.58it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:39,  3.55it/s]\u001b[A\n",
            "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:39,  3.52it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:41,  3.31it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:42,  3.20it/s]\u001b[A\n",
            " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:41,  3.27it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:43,  3.05it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:42,  3.16it/s]\u001b[A\n",
            " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:40,  3.23it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:39,  3.28it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:40,  3.18it/s]\u001b[A\n",
            " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:05<00:36,  3.51it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:33,  3.82it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:36,  3.50it/s]\u001b[A\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:36,  3.49it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:36,  3.46it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:34,  3.57it/s]\u001b[A\n",
            " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:34,  3.57it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:07<00:36,  3.37it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:37,  3.21it/s]\u001b[A\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:36,  3.32it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:08<00:34,  3.46it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.80it/s]\u001b[A\n",
            " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:34,  3.40it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:32,  3.54it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:09<00:34,  3.33it/s]\u001b[A\n",
            " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:31,  3.64it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:34,  3.25it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:10<00:33,  3.30it/s]\u001b[A\n",
            " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:34,  3.19it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:33,  3.27it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:11<00:32,  3.37it/s]\u001b[A\n",
            " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:31,  3.43it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.44it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:12<00:34,  3.03it/s]\u001b[A\n",
            " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:32,  3.19it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:34,  2.99it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:30,  3.38it/s]\u001b[A\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:13<00:30,  3.37it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.48it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:28,  3.50it/s]\u001b[A\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:14<00:27,  3.54it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:31,  3.12it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.32it/s]\u001b[A\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:15<00:28,  3.42it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:15<00:26,  3.54it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:29,  3.20it/s]\u001b[A\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:16<00:29,  3.13it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:16<00:28,  3.24it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:26,  3.38it/s]\u001b[A\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:26,  3.39it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:17<00:26,  3.39it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:25,  3.40it/s]\u001b[A\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:25,  3.39it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:18<00:23,  3.66it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:18<00:23,  3.60it/s]\u001b[A\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.35it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:19<00:27,  2.98it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:19<00:26,  3.11it/s]\u001b[A\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:23,  3.45it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:22,  3.55it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:20<00:20,  3.82it/s]\u001b[A\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:20<00:18,  4.11it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:19,  3.90it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:18,  4.09it/s]\u001b[A\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:21<00:19,  3.86it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:21,  3.38it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:19,  3.71it/s]\u001b[A\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:22<00:19,  3.69it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:22<00:19,  3.61it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.67it/s]\u001b[A\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:17,  3.93it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:23<00:19,  3.57it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:23<00:18,  3.59it/s]\u001b[A\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:19,  3.37it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:24<00:18,  3.45it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:24<00:20,  3.09it/s]\u001b[A\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:20,  3.01it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:25<00:19,  3.13it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:25<00:21,  2.85it/s]\u001b[A\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.09it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:26<00:19,  3.02it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:26<00:19,  2.99it/s]\u001b[A\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.34it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:27<00:16,  3.44it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:27<00:16,  3.42it/s]\u001b[A\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:16,  3.23it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:28<00:16,  3.30it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:28<00:15,  3.32it/s]\u001b[A\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:28<00:15,  3.20it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:29<00:16,  2.96it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:29<00:15,  3.14it/s]\u001b[A\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:29<00:13,  3.48it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:30<00:14,  3.17it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:30<00:14,  3.23it/s]\u001b[A\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:30<00:13,  3.40it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:30<00:11,  3.86it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:31<00:11,  3.68it/s]\u001b[A\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:31<00:11,  3.67it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:31<00:12,  3.28it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:32<00:12,  3.32it/s]\u001b[A\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:32<00:11,  3.36it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:32<00:11,  3.40it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:33<00:11,  3.25it/s]\u001b[A\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:33<00:10,  3.42it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:33<00:09,  3.74it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:33<00:09,  3.46it/s]\u001b[A\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:34<00:10,  3.14it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:34<00:09,  3.27it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:34<00:09,  3.33it/s]\u001b[A\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:35<00:09,  3.18it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:35<00:08,  3.28it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:35<00:08,  3.14it/s]\u001b[A\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:36<00:08,  3.32it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:36<00:07,  3.46it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:36<00:07,  3.56it/s]\u001b[A\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:36<00:06,  3.53it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:37<00:06,  3.30it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:37<00:06,  3.44it/s]\u001b[A\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:37<00:05,  3.73it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:38<00:05,  3.65it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:38<00:05,  3.40it/s]\u001b[A\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:38<00:05,  3.25it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:39<00:05,  3.40it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:39<00:04,  3.45it/s]\u001b[A\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:39<00:04,  3.50it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:39<00:04,  3.49it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:40<00:04,  3.06it/s]\u001b[A\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:40<00:03,  3.28it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:40<00:03,  3.14it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:41<00:03,  3.26it/s]\u001b[A\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:41<00:02,  3.03it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:41<00:02,  3.14it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:42<00:02,  3.24it/s]\u001b[A\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:42<00:01,  3.30it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:42<00:01,  3.37it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:42<00:01,  3.71it/s]\u001b[A\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:43<00:00,  3.78it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:43<00:00,  4.22it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:43<00:00,  4.33it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 0.6823136210441589, 'eval_runtime': 44.0944, 'eval_samples_per_second': 3.402, 'eval_steps_per_second': 3.402, 'epoch': 2.35, 'num_input_tokens_seen': 1437056}\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 500/639 [37:54<09:46,  4.22s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:43<00:00,  4.09it/s]\u001b[A\n",
            "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-05 17:14:08,021 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-500\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 17:14:08,069 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 17:14:08,070 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-12-05 17:14:08,186 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-500/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-12-05 17:14:08,187 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-12-05 17:14:08,187 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-500/special_tokens_map.json\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 505/639 [38:16<16:31,  7.40s/it][INFO|2025-12-05 17:14:29] llamafactory.train.callbacks:143 >> {'loss': 0.2032, 'learning_rate': 1.0615e-05, 'epoch': 2.37, 'throughput': 632.19}\n",
            "{'loss': 0.2032, 'grad_norm': 0.6620310544967651, 'learning_rate': 1.0614601334114099e-05, 'epoch': 2.37, 'num_input_tokens_seen': 1451560, 'train_runtime': 2296.0789, 'train_tokens_per_second': 632.191}\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 510/639 [38:36<10:14,  4.76s/it][INFO|2025-12-05 17:14:50] llamafactory.train.callbacks:143 >> {'loss': 0.2162, 'learning_rate': 9.8694e-06, 'epoch': 2.40, 'throughput': 632.74}\n",
            "{'loss': 0.2162, 'grad_norm': 0.5055356025695801, 'learning_rate': 9.869388139903496e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1466016, 'train_runtime': 2316.934, 'train_tokens_per_second': 632.74}\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 515/639 [38:55<08:09,  3.95s/it][INFO|2025-12-05 17:15:08] llamafactory.train.callbacks:143 >> {'loss': 0.1858, 'learning_rate': 9.1484e-06, 'epoch': 2.42, 'throughput': 633.20}\n",
            "{'loss': 0.1858, 'grad_norm': 0.616602897644043, 'learning_rate': 9.148423840490954e-06, 'epoch': 2.42, 'num_input_tokens_seen': 1478944, 'train_runtime': 2335.6642, 'train_tokens_per_second': 633.201}\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 520/639 [39:17<08:23,  4.23s/it][INFO|2025-12-05 17:15:30] llamafactory.train.callbacks:143 >> {'loss': 0.2427, 'learning_rate': 8.4521e-06, 'epoch': 2.44, 'throughput': 633.89}\n",
            "{'loss': 0.2427, 'grad_norm': 0.5273712873458862, 'learning_rate': 8.452144078061818e-06, 'epoch': 2.44, 'num_input_tokens_seen': 1494488, 'train_runtime': 2357.6499, 'train_tokens_per_second': 633.889}\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 525/639 [39:38<08:02,  4.24s/it][INFO|2025-12-05 17:15:51] llamafactory.train.callbacks:143 >> {'loss': 0.2412, 'learning_rate': 7.7810e-06, 'epoch': 2.47, 'throughput': 634.47}\n",
            "{'loss': 0.2412, 'grad_norm': 0.5530897378921509, 'learning_rate': 7.780969579186814e-06, 'epoch': 2.47, 'num_input_tokens_seen': 1509128, 'train_runtime': 2378.5606, 'train_tokens_per_second': 634.471}\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 530/639 [40:00<08:14,  4.54s/it][INFO|2025-12-05 17:16:13] llamafactory.train.callbacks:143 >> {'loss': 0.2784, 'learning_rate': 7.1353e-06, 'epoch': 2.49, 'throughput': 635.12}\n",
            "{'loss': 0.2784, 'grad_norm': 0.3819183111190796, 'learning_rate': 7.135305900598321e-06, 'epoch': 2.49, 'num_input_tokens_seen': 1524464, 'train_runtime': 2400.2932, 'train_tokens_per_second': 635.116}\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 535/639 [40:22<07:43,  4.45s/it][INFO|2025-12-05 17:16:36] llamafactory.train.callbacks:143 >> {'loss': 0.2787, 'learning_rate': 6.5155e-06, 'epoch': 2.51, 'throughput': 635.88}\n",
            "{'loss': 0.2787, 'grad_norm': 0.7438096404075623, 'learning_rate': 6.515543184132999e-06, 'epoch': 2.51, 'num_input_tokens_seen': 1540688, 'train_runtime': 2422.9429, 'train_tokens_per_second': 635.875}\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 540/639 [40:42<06:30,  3.94s/it][INFO|2025-12-05 17:16:55] llamafactory.train.callbacks:143 >> {'loss': 0.2335, 'learning_rate': 5.9221e-06, 'epoch': 2.54, 'throughput': 636.38}\n",
            "{'loss': 0.2335, 'grad_norm': 0.5677853226661682, 'learning_rate': 5.922055920988817e-06, 'epoch': 2.54, 'num_input_tokens_seen': 1554264, 'train_runtime': 2442.3749, 'train_tokens_per_second': 636.374}\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 545/639 [41:02<06:26,  4.11s/it][INFO|2025-12-05 17:17:16] llamafactory.train.callbacks:143 >> {'loss': 0.2195, 'learning_rate': 5.3552e-06, 'epoch': 2.56, 'throughput': 636.87}\n",
            "{'loss': 0.2195, 'grad_norm': 0.743032693862915, 'learning_rate': 5.355202725439046e-06, 'epoch': 2.56, 'num_input_tokens_seen': 1568584, 'train_runtime': 2462.9467, 'train_tokens_per_second': 636.873}\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 550/639 [41:23<05:49,  3.93s/it][INFO|2025-12-05 17:17:36] llamafactory.train.callbacks:143 >> {'loss': 0.2079, 'learning_rate': 4.8153e-06, 'epoch': 2.58, 'throughput': 637.39}\n",
            "{'loss': 0.2079, 'grad_norm': 0.9537621736526489, 'learning_rate': 4.8153261181398125e-06, 'epoch': 2.58, 'num_input_tokens_seen': 1582712, 'train_runtime': 2483.1367, 'train_tokens_per_second': 637.384}\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 555/639 [41:42<05:20,  3.81s/it][INFO|2025-12-05 17:17:55] llamafactory.train.callbacks:143 >> {'loss': 0.2139, 'learning_rate': 4.3028e-06, 'epoch': 2.61, 'throughput': 637.81}\n",
            "{'loss': 0.2139, 'grad_norm': 0.6003711223602295, 'learning_rate': 4.302752319162212e-06, 'epoch': 2.61, 'num_input_tokens_seen': 1596192, 'train_runtime': 2502.6286, 'train_tokens_per_second': 637.806}\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 560/639 [42:03<05:40,  4.31s/it][INFO|2025-12-05 17:18:16] llamafactory.train.callbacks:143 >> {'loss': 0.2153, 'learning_rate': 3.8178e-06, 'epoch': 2.63, 'throughput': 638.35}\n",
            "{'loss': 0.2153, 'grad_norm': 0.44228821992874146, 'learning_rate': 3.81779105087407e-06, 'epoch': 2.63, 'num_input_tokens_seen': 1611088, 'train_runtime': 2523.8308, 'train_tokens_per_second': 638.35}\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 565/639 [42:24<05:03,  4.10s/it][INFO|2025-12-05 17:18:37] llamafactory.train.callbacks:143 >> {'loss': 0.2409, 'learning_rate': 3.3607e-06, 'epoch': 2.65, 'throughput': 638.86}\n",
            "{'loss': 0.2409, 'grad_norm': 0.5779442191123962, 'learning_rate': 3.3607353507904283e-06, 'epoch': 2.65, 'num_input_tokens_seen': 1625696, 'train_runtime': 2544.6793, 'train_tokens_per_second': 638.861}\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 570/639 [42:46<05:12,  4.53s/it][INFO|2025-12-05 17:18:59] llamafactory.train.callbacks:143 >> {'loss': 0.2367, 'learning_rate': 2.9319e-06, 'epoch': 2.68, 'throughput': 639.34}\n",
            "{'loss': 0.2367, 'grad_norm': 0.5928359031677246, 'learning_rate': 2.931861394505764e-06, 'epoch': 2.68, 'num_input_tokens_seen': 1641048, 'train_runtime': 2566.7837, 'train_tokens_per_second': 639.34}\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 575/639 [43:08<04:41,  4.40s/it][INFO|2025-12-05 17:19:21] llamafactory.train.callbacks:143 >> {'loss': 0.2079, 'learning_rate': 2.5314e-06, 'epoch': 2.70, 'throughput': 639.87}\n",
            "{'loss': 0.2079, 'grad_norm': 0.6690888404846191, 'learning_rate': 2.531428328815155e-06, 'epoch': 2.7, 'num_input_tokens_seen': 1656160, 'train_runtime': 2588.2838, 'train_tokens_per_second': 639.868}\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 580/639 [43:27<03:50,  3.91s/it][INFO|2025-12-05 17:19:40] llamafactory.train.callbacks:143 >> {'loss': 0.2116, 'learning_rate': 2.1597e-06, 'epoch': 2.72, 'throughput': 640.23}\n",
            "{'loss': 0.2116, 'grad_norm': 0.855820894241333, 'learning_rate': 2.1596781151249524e-06, 'epoch': 2.72, 'num_input_tokens_seen': 1669320, 'train_runtime': 2607.3795, 'train_tokens_per_second': 640.229}\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 585/639 [43:47<03:32,  3.94s/it][INFO|2025-12-05 17:20:00] llamafactory.train.callbacks:143 >> {'loss': 0.2399, 'learning_rate': 1.8168e-06, 'epoch': 2.75, 'throughput': 640.69}\n",
            "{'loss': 0.2399, 'grad_norm': 0.7848758697509766, 'learning_rate': 1.8168353832477947e-06, 'epoch': 2.75, 'num_input_tokens_seen': 1683616, 'train_runtime': 2627.828, 'train_tokens_per_second': 640.687}\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 590/639 [44:09<03:26,  4.20s/it][INFO|2025-12-05 17:20:22] llamafactory.train.callbacks:143 >> {'loss': 0.2378, 'learning_rate': 1.5031e-06, 'epoch': 2.77, 'throughput': 641.17}\n",
            "{'loss': 0.2378, 'grad_norm': 0.5970350503921509, 'learning_rate': 1.5031072956701697e-06, 'epoch': 2.77, 'num_input_tokens_seen': 1698728, 'train_runtime': 2649.4153, 'train_tokens_per_second': 641.171}\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 595/639 [44:27<02:47,  3.80s/it][INFO|2025-12-05 17:20:40] llamafactory.train.callbacks:143 >> {'loss': 0.2284, 'learning_rate': 1.2187e-06, 'epoch': 2.80, 'throughput': 641.49}\n",
            "{'loss': 0.2284, 'grad_norm': 0.5509009957313538, 'learning_rate': 1.2186834223746612e-06, 'epoch': 2.8, 'num_input_tokens_seen': 1711320, 'train_runtime': 2667.7342, 'train_tokens_per_second': 641.488}\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 600/639 [44:49<02:46,  4.27s/it][INFO|2025-12-05 17:21:02] llamafactory.train.callbacks:143 >> {'loss': 0.2681, 'learning_rate': 9.6374e-07, 'epoch': 2.82, 'throughput': 641.98}\n",
            "{'loss': 0.2681, 'grad_norm': 0.5576382279396057, 'learning_rate': 9.637356262923725e-07, 'epoch': 2.82, 'num_input_tokens_seen': 1726344, 'train_runtime': 2689.0878, 'train_tokens_per_second': 641.981}\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 600/639 [44:49<02:46,  4.27s/it][INFO|trainer.py:4327] 2025-12-05 17:21:02,208 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4329] 2025-12-05 17:21:02,208 >>   Num examples = 150\n",
            "[INFO|trainer.py:4332] 2025-12-05 17:21:02,209 >>   Batch size = 1\n",
            "\n",
            "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|â–Œ                                          | 2/150 [00:00<00:18,  7.86it/s]\u001b[A\n",
            "  2%|â–Š                                          | 3/150 [00:00<00:26,  5.46it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 4/150 [00:00<00:35,  4.07it/s]\u001b[A\n",
            "  3%|â–ˆâ–                                         | 5/150 [00:01<00:37,  3.84it/s]\u001b[A\n",
            "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:38,  3.71it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:39,  3.61it/s]\u001b[A\n",
            "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:40,  3.53it/s]\u001b[A\n",
            "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:38,  3.64it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:39,  3.58it/s]\u001b[A\n",
            "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:39,  3.55it/s]\u001b[A\n",
            "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:39,  3.52it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:41,  3.30it/s]\u001b[A\n",
            "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:42,  3.19it/s]\u001b[A\n",
            " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:41,  3.27it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:43,  3.05it/s]\u001b[A\n",
            " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:42,  3.16it/s]\u001b[A\n",
            " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:40,  3.23it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:39,  3.29it/s]\u001b[A\n",
            " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:40,  3.18it/s]\u001b[A\n",
            " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:05<00:36,  3.51it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:33,  3.82it/s]\u001b[A\n",
            " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:36,  3.50it/s]\u001b[A\n",
            " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:36,  3.48it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:36,  3.46it/s]\u001b[A\n",
            " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:34,  3.57it/s]\u001b[A\n",
            " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:34,  3.57it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:07<00:36,  3.36it/s]\u001b[A\n",
            " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:37,  3.21it/s]\u001b[A\n",
            " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:36,  3.31it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:08<00:34,  3.46it/s]\u001b[A\n",
            " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:30,  3.81it/s]\u001b[A\n",
            " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:34,  3.41it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:32,  3.54it/s]\u001b[A\n",
            " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:09<00:34,  3.33it/s]\u001b[A\n",
            " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:31,  3.65it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:34,  3.25it/s]\u001b[A\n",
            " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:10<00:33,  3.30it/s]\u001b[A\n",
            " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:34,  3.19it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:33,  3.26it/s]\u001b[A\n",
            " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:11<00:32,  3.37it/s]\u001b[A\n",
            " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:31,  3.43it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.45it/s]\u001b[A\n",
            " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:12<00:34,  3.03it/s]\u001b[A\n",
            " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:32,  3.18it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:34,  2.99it/s]\u001b[A\n",
            " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:30,  3.38it/s]\u001b[A\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:13<00:30,  3.37it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.47it/s]\u001b[A\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:28,  3.49it/s]\u001b[A\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:14<00:27,  3.54it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:31,  3.11it/s]\u001b[A\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.32it/s]\u001b[A\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:15<00:28,  3.42it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:15<00:26,  3.53it/s]\u001b[A\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:29,  3.20it/s]\u001b[A\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:16<00:29,  3.13it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:16<00:28,  3.24it/s]\u001b[A\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:26,  3.37it/s]\u001b[A\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:26,  3.40it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:17<00:26,  3.39it/s]\u001b[A\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:25,  3.40it/s]\u001b[A\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:25,  3.39it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:18<00:23,  3.68it/s]\u001b[A\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:18<00:23,  3.61it/s]\u001b[A\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.35it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:19<00:27,  2.98it/s]\u001b[A\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:19<00:26,  3.11it/s]\u001b[A\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:23,  3.45it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:22,  3.55it/s]\u001b[A\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:20<00:20,  3.83it/s]\u001b[A\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:20<00:18,  4.11it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:19,  3.89it/s]\u001b[A\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:18,  4.09it/s]\u001b[A\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:21<00:19,  3.86it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:21,  3.38it/s]\u001b[A\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:19,  3.71it/s]\u001b[A\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:22<00:19,  3.69it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:22<00:19,  3.61it/s]\u001b[A\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.67it/s]\u001b[A\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:17,  3.93it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:23<00:19,  3.58it/s]\u001b[A\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:23<00:18,  3.59it/s]\u001b[A\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:19,  3.36it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:24<00:18,  3.45it/s]\u001b[A\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:24<00:20,  3.09it/s]\u001b[A\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:20,  3.01it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:25<00:19,  3.13it/s]\u001b[A\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:25<00:21,  2.85it/s]\u001b[A\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.08it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:26<00:19,  3.01it/s]\u001b[A\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:26<00:19,  2.98it/s]\u001b[A\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.33it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:27<00:16,  3.42it/s]\u001b[A\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:27<00:16,  3.41it/s]\u001b[A\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:16,  3.23it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:28<00:16,  3.30it/s]\u001b[A\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:28<00:15,  3.33it/s]\u001b[A\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:15,  3.20it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:29<00:16,  2.96it/s]\u001b[A\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:29<00:15,  3.14it/s]\u001b[A\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:29<00:13,  3.48it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:30<00:14,  3.17it/s]\u001b[A\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:30<00:14,  3.24it/s]\u001b[A\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:30<00:13,  3.41it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:30<00:11,  3.86it/s]\u001b[A\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:31<00:11,  3.68it/s]\u001b[A\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:31<00:11,  3.65it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:31<00:12,  3.27it/s]\u001b[A\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:32<00:12,  3.31it/s]\u001b[A\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:32<00:11,  3.35it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:32<00:11,  3.39it/s]\u001b[A\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:33<00:11,  3.25it/s]\u001b[A\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:33<00:10,  3.42it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:33<00:09,  3.73it/s]\u001b[A\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:33<00:09,  3.45it/s]\u001b[A\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:34<00:10,  3.15it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:34<00:09,  3.28it/s]\u001b[A\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:34<00:09,  3.34it/s]\u001b[A\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:35<00:09,  3.18it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:35<00:08,  3.30it/s]\u001b[A\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:35<00:08,  3.14it/s]\u001b[A\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:36<00:08,  3.31it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:36<00:07,  3.46it/s]\u001b[A\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:36<00:07,  3.56it/s]\u001b[A\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:36<00:06,  3.52it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:37<00:07,  3.28it/s]\u001b[A\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:37<00:06,  3.44it/s]\u001b[A\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:37<00:05,  3.74it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:38<00:05,  3.64it/s]\u001b[A\n",
            " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:38<00:05,  3.40it/s]\u001b[A\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:38<00:05,  3.26it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:39<00:04,  3.40it/s]\u001b[A\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:39<00:04,  3.46it/s]\u001b[A\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:39<00:04,  3.52it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:39<00:03,  3.50it/s]\u001b[A\n",
            " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:40<00:04,  3.07it/s]\u001b[A\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:40<00:03,  3.28it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:40<00:03,  3.14it/s]\u001b[A\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:41<00:03,  3.26it/s]\u001b[A\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:41<00:02,  3.02it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:41<00:02,  3.13it/s]\u001b[A\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:42<00:02,  3.23it/s]\u001b[A\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:42<00:01,  3.29it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:42<00:01,  3.37it/s]\u001b[A\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:42<00:01,  3.70it/s]\u001b[A\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:43<00:00,  3.77it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:43<00:00,  4.20it/s]\u001b[A\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:43<00:00,  4.32it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 0.689818799495697, 'eval_runtime': 44.1058, 'eval_samples_per_second': 3.401, 'eval_steps_per_second': 3.401, 'epoch': 2.82, 'num_input_tokens_seen': 1726344}\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 600/639 [45:33<02:46,  4.27s/it]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:43<00:00,  4.10it/s]\u001b[A\n",
            "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-05 17:21:46,311 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-600\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 17:21:46,358 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 17:21:46,359 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-12-05 17:21:46,475 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-600/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-12-05 17:21:46,475 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-12-05 17:21:46,475 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-600/special_tokens_map.json\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 605/639 [45:53<03:59,  7.05s/it][INFO|2025-12-05 17:22:06] llamafactory.train.callbacks:143 >> {'loss': 0.2445, 'learning_rate': 7.3842e-07, 'epoch': 2.84, 'throughput': 632.09}\n",
            "{'loss': 0.2445, 'grad_norm': 0.8062381744384766, 'learning_rate': 7.384179594548957e-07, 'epoch': 2.84, 'num_input_tokens_seen': 1740472, 'train_runtime': 2753.5367, 'train_tokens_per_second': 632.086}\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 610/639 [46:13<02:08,  4.44s/it][INFO|2025-12-05 17:22:26] llamafactory.train.callbacks:143 >> {'loss': 0.2161, 'learning_rate': 5.4287e-07, 'epoch': 2.87, 'throughput': 632.46}\n",
            "{'loss': 0.2161, 'grad_norm': 0.5711224675178528, 'learning_rate': 5.428665699084789e-07, 'epoch': 2.87, 'num_input_tokens_seen': 1754136, 'train_runtime': 2773.4999, 'train_tokens_per_second': 632.463}\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 615/639 [46:34<01:43,  4.33s/it][INFO|2025-12-05 17:22:47] llamafactory.train.callbacks:143 >> {'loss': 0.2422, 'learning_rate': 3.7720e-07, 'epoch': 2.89, 'throughput': 632.99}\n",
            "{'loss': 0.2422, 'grad_norm': 0.5394325256347656, 'learning_rate': 3.7719961944664985e-07, 'epoch': 2.89, 'num_input_tokens_seen': 1768960, 'train_runtime': 2794.6182, 'train_tokens_per_second': 632.988}\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 620/639 [46:54<01:14,  3.95s/it][INFO|2025-12-05 17:23:07] llamafactory.train.callbacks:143 >> {'loss': 0.2027, 'learning_rate': 2.4152e-07, 'epoch': 2.91, 'throughput': 633.38}\n",
            "{'loss': 0.2027, 'grad_norm': 0.6823627948760986, 'learning_rate': 2.415172122110343e-07, 'epoch': 2.91, 'num_input_tokens_seen': 1782728, 'train_runtime': 2814.616, 'train_tokens_per_second': 633.382}\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 625/639 [47:15<00:57,  4.13s/it][INFO|2025-12-05 17:23:28] llamafactory.train.callbacks:143 >> {'loss': 0.2515, 'learning_rate': 1.3590e-07, 'epoch': 2.94, 'throughput': 633.90}\n",
            "{'loss': 0.2515, 'grad_norm': 0.6167343854904175, 'learning_rate': 1.3590133420350315e-07, 'epoch': 2.94, 'num_input_tokens_seen': 1797592, 'train_runtime': 2835.7703, 'train_tokens_per_second': 633.899}\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 630/639 [47:36<00:37,  4.15s/it][INFO|2025-12-05 17:23:49] llamafactory.train.callbacks:143 >> {'loss': 0.2111, 'learning_rate': 6.0416e-08, 'epoch': 2.96, 'throughput': 634.42}\n",
            "{'loss': 0.2111, 'grad_norm': 0.62283855676651, 'learning_rate': 6.041580374618328e-08, 'epoch': 2.96, 'num_input_tokens_seen': 1812104, 'train_runtime': 2856.3274, 'train_tokens_per_second': 634.417}\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 635/639 [47:55<00:15,  3.83s/it][INFO|2025-12-05 17:24:08] llamafactory.train.callbacks:143 >> {'loss': 0.2196, 'learning_rate': 1.5106e-08, 'epoch': 2.98, 'throughput': 634.81}\n",
            "{'loss': 0.2196, 'grad_norm': 0.8071909546852112, 'learning_rate': 1.5106232919276375e-08, 'epoch': 2.98, 'num_input_tokens_seen': 1825320, 'train_runtime': 2875.3787, 'train_tokens_per_second': 634.81}\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 639/639 [48:09<00:00,  3.39s/it][INFO|trainer.py:3993] 2025-12-05 17:24:22,628 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-639\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 17:24:22,675 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 17:24:22,675 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-12-05 17:24:22,796 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-639/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-12-05 17:24:22,796 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-639/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-12-05 17:24:22,797 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-639/special_tokens_map.json\n",
            "[INFO|trainer.py:2676] 2025-12-05 17:24:23,147 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2890.0317, 'train_samples_per_second': 0.882, 'train_steps_per_second': 0.221, 'train_loss': 0.40096471054080135, 'epoch': 3.0, 'num_input_tokens_seen': 1835184}\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 639/639 [48:10<00:00,  4.52s/it]\n",
            "[INFO|trainer.py:3993] 2025-12-05 17:24:23,149 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_qwen3_8b\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 17:24:23,197 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 17:24:23,198 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-12-05 17:24:23,336 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-12-05 17:24:23,337 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-12-05 17:24:23,337 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  num_input_tokens_seen    =    1835184\n",
            "  total_flos               = 77836961GF\n",
            "  train_loss               =      0.401\n",
            "  train_runtime            = 0:48:10.03\n",
            "  train_samples_per_second =      0.882\n",
            "  train_steps_per_second   =      0.221\n",
            "Figure saved at: saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/training_loss.png\n",
            "Figure saved at: saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/training_eval_loss.png\n",
            "[WARNING|2025-12-05 17:24:23] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:4327] 2025-12-05 17:24:23,705 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4329] 2025-12-05 17:24:23,705 >>   Num examples = 150\n",
            "[INFO|trainer.py:4332] 2025-12-05 17:24:23,705 >>   Batch size = 1\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:43<00:00,  3.42it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_loss               =     0.6883\n",
            "  eval_runtime            = 0:00:44.09\n",
            "  eval_samples_per_second =      3.402\n",
            "  eval_steps_per_second   =      3.402\n",
            "  num_input_tokens_seen   =    1835184\n",
            "[INFO|modelcard.py:450] 2025-12-05 17:25:07,792 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "[WARNING|2025-12-05 17:36:03] llamafactory.webui.common:148 >> Found complex path, some features may be not available.\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,325 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,325 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,325 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,325 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,325 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,325 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,325 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-12-05 17:36:03,648 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 17:36:03,648 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 17:36:03,652 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,652 >> loading file vocab.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,653 >> loading file merges.txt\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,653 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,653 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,653 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,653 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2021] 2025-12-05 17:36:03,653 >> loading file chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2299] 2025-12-05 17:36:03,983 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:696] 2025-12-05 17:36:04,007 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-12-05 17:36:04,008 >> Model config Qwen3Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 12288,\n",
            "  \"max_position_embeddings\": 40960,\n",
            "  \"max_window_layers\": 36,\n",
            "  \"model_type\": \"qwen3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 36,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000,\n",
            "  \"sliding_window\": null,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-12-05 17:36:04] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
            "[INFO|2025-12-05 17:36:04] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1148] 2025-12-05 17:36:05,681 >> loading weights file /mnt/workspace/Qwen3-8B/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2241] 2025-12-05 17:36:05,681 >> Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-12-05 17:36:05,683 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:30<00:00,  6.04s/it]\n",
            "[INFO|modeling_utils.py:5131] 2025-12-05 17:36:36,067 >> All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-12-05 17:36:36,067 >> All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at /mnt/workspace/Qwen3-8B.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1088] 2025-12-05 17:36:36,071 >> loading configuration file /mnt/workspace/Qwen3-8B/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-12-05 17:36:36,071 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.95\n",
            "}\n",
            "\n",
            "[INFO|2025-12-05 17:36:36] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-12-05 17:36:36] llamafactory.model.adapter:143 >> Loaded adapter(s): /mnt/workspace/LLaMA-Factory/saves/Qwen3-8B-Instruct/lora/train_qwen3_8b/checkpoint-200\n",
            "[INFO|2025-12-05 17:36:36] llamafactory.model.loader:143 >> all params: 8,212,558,848\n",
            "[WARNING|2025-12-05 17:36:36] llamafactory.chat.hf_engine:154 >> There is no current event loop, creating a new one.\n",
            "[WARNING|logging.py:328] 2025-12-05 17:37:32,264 >> `generation_config` default values have been modified to match model-specific defaults: {'top_k': 20, 'bos_token_id': 151643}. If this is not desired, please set these values explicitly.\n"
          ]
        }
      ],
      "source": [
        "!export USE_MODELSCOPE_HUB=1 && \\\n",
        "llamafactory-cli webui"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7ff4f1-3221-4478-984b-f37f4fa57ff5",
      "metadata": {
        "tags": [],
        "id": "fd7ff4f1-3221-4478-984b-f37f4fa57ff5"
      },
      "source": [
        "### 3.2 é…ç½®å‚æ•°\n",
        "ç»“åˆ **A10 (24G)** æ˜¾å¡ä¸ **Qwen3-8B** çš„ç¯å¢ƒï¼Œä»¥ä¸‹æ˜¯é’ˆå¯¹é…ç½®ç•Œé¢çš„è¯¦ç»†å‚æ•°è§£è¯»ã€‚è¿™äº›è®¾ç½®å…±åŒæ„æˆäº† **QLoRA** å¾®è°ƒæ–¹æ¡ˆï¼Œæ˜¯åœ¨å•å¡ç¯å¢ƒä¸‹å¤„ç†åŒ»ç–—æ¨ç†ï¼ˆReasoningï¼‰é•¿æ–‡æœ¬æ•°æ®çš„æœ€ä½³å®è·µã€‚\n",
        "\n",
        "![image-20251204185719305](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512041857405.png)\n",
        "\n",
        "#### 3.2.1 æ¨¡å‹è·¯å¾„ä¸åç§°é…ç½® (Model Configuration)\n",
        "* **å‚æ•°é¡¹**ï¼š`æ¨¡å‹åç§°` & `æ¨¡å‹è·¯å¾„`\n",
        "* **å½“å‰å€¼**ï¼š\n",
        "    * åç§°ï¼š`Qwen3-8B-Instruct`\n",
        "    * è·¯å¾„ï¼š`/mnt/workspace/Qwen3-8B`\n",
        "* **é…ç½®ä½œç”¨**ï¼š\n",
        "    * **æ¨¡å‹è·¯å¾„**ï¼šè¿™æ˜¯â€œåœ°åŸºâ€ã€‚ç¨‹åºä¼šä» `/mnt/workspace/Qwen3-8B` è¯»å–æ¨¡å‹æƒé‡çš„ç‰©ç†æ–‡ä»¶ã€‚\n",
        "    * **æ¨¡å‹åç§°**ï¼šè¿™æ˜¯â€œæ ‡è¯†â€ã€‚LLaMA-Factory ä¼šæ ¹æ®è¿™ä¸ªåç§°è‡ªåŠ¨åŒ¹é…é»˜è®¤çš„è¶…å‚æ•°å’Œæ¨¡æ¿ã€‚\n",
        "* **ç¡¬ä»¶å½±å“**ï¼šA10 çš„ 24G æ˜¾å­˜åŠ è½½ 8B æ¨¡å‹æ¯«æ— å‹åŠ›ï¼Œè¿™ä¸€æ­¥æ˜¯åŸºç¡€ã€‚\n",
        "\n",
        "æ¨¡å‹è·¯å¾„ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼Œå¦‚ï¼š`/mnt/workspace/Qwen3-8B`ã€‚ï¼ˆè€ƒè™‘å…¬ç½‘æœ‰æ—¶å€™ä¸‹è½½æ¨¡å‹æ…¢ï¼Œæä¾›ç»™å¤§å®¶å‡†å¤‡å¥½äº†ç¦»çº¿æ¨¡å‹å‹ç¼©åŒ…ï¼Œå¯ä»¥è§£å‹åˆ°`/mnt/workspace/Qwen3-8B`ï¼‰\n",
        "\n",
        "å…¶ä¸­`/mnt/workspace/Qwen3-8B`å†…å®¹å¦‚ä¸‹ï¼Œé‡åˆ°å‡ºé”™ï¼Œè¯·æ ¸å¯¹æ–‡ä»¶å¤¹æ˜¯ä¸æ˜¯å°‘å†…å®¹\n",
        "\n",
        "```bash\n",
        "root@dsw-523480-6d6bb75cf6-kz46d:/mnt/workspace/Qwen3-8B# ll\n",
        "total 16013364\n",
        "drwxr-xr-x  2 root root       4096 Dec  4 17:47 ./\n",
        "drwxr-xr-x 10 root root       4096 Dec  4 18:38 ../\n",
        "-rw-r--r--  1 root root        757 Dec  4 17:02 config.json\n",
        "-rw-r--r--  1 root root         73 Dec  4 17:02 configuration.json\n",
        "-rw-r--r--  1 root root        251 Dec  4 17:02 generation_config.json\n",
        "-rw-r--r--  1 root root       2175 Dec  4 17:02 .gitattributes\n",
        "-rw-r--r--  1 root root      11544 Dec  4 17:02 LICENSE\n",
        "-rw-r--r--  1 root root    1823241 Dec  4 17:02 merges.txt\n",
        "-rw-r--r--  1 root root 3996250744 Dec  4 17:41 model-00001-of-00005.safetensors\n",
        "-rw-r--r--  1 root root 3993160032 Dec  4 17:45 model-00002-of-00005.safetensors\n",
        "-rw-r--r--  1 root root 3959604768 Dec  4 17:47 model-00003-of-00005.safetensors\n",
        "-rw-r--r--  1 root root 3187841392 Dec  4 17:43 model-00004-of-00005.safetensors\n",
        "-rw-r--r--  1 root root 1244659840 Dec  4 17:14 model-00005-of-00005.safetensors\n",
        "-rw-r--r--  1 root root      33284 Dec  4 17:02 model.safetensors.index.json\n",
        "-rw-r--r--  1 root root      17012 Dec  4 17:02 README.md\n",
        "-rw-r--r--  1 root root       9971 Dec  4 17:02 tokenizer_config.json\n",
        "-rw-r--r--  1 root root   11422654 Dec  4 17:02 tokenizer.json\n",
        "-rw-r--r--  1 root root    2776833 Dec  4 17:02 vocab.json\n",
        "```\n",
        "\n",
        "è¿™æ˜¯å…¸å‹çš„ Hugging Face `transformers` æ ¼å¼çš„æ¨¡å‹ç›®å½•ç»“æ„ï¼ˆQwen ç³»åˆ—ï¼‰ã€‚\n",
        "\n",
        "è¿™ä¸ªç›®å½•åŒ…å«äº† **æ¨¡å‹æ¶æ„é…ç½®**ã€**åˆ†è¯å™¨ï¼ˆTokenizerï¼‰** å’Œ **æ¨¡å‹æƒé‡ï¼ˆWeightsï¼‰** ä¸‰å¤§æ ¸å¿ƒéƒ¨åˆ†ã€‚\n",
        "\n",
        "ä»¥ä¸‹æ˜¯é€è¡Œæ³¨é‡Šè§£é‡Šï¼š\n",
        "\n",
        "```bash\n",
        "# ----------------- æ ¸å¿ƒé…ç½®åŒº -----------------\n",
        "-rw-r--r--  1 root root        757 Dec  4 17:02 config.json             # ã€æœ€å…³é”®ã€‘æ¨¡å‹æ¶æ„é…ç½®æ–‡ä»¶ã€‚å®šä¹‰äº†æ¨¡å‹æœ‰å¤šå°‘å±‚ã€éšè—å±‚å¤§å°ã€æ³¨æ„åŠ›å¤´æ•°ç­‰ã€‚ä»£ç é€šè¿‡å®ƒæ¥æ„å»ºæ¨¡å‹éª¨æ¶ã€‚\n",
        "-rw-r--r--  1 root root         73 Dec  4 17:02 configuration.json      # è¾…åŠ©é…ç½®æ–‡ä»¶ï¼ˆé€šå¸¸è¾ƒå°‘è§ï¼Œå¯èƒ½æ˜¯ç‰¹å®šæ¡†æ¶ç”Ÿæˆçš„å…ƒæ•°æ®ï¼Œé€šå¸¸ config.json æ˜¯ä¸»æ–‡ä»¶ï¼‰ã€‚\n",
        "-rw-r--r--  1 root root        251 Dec  4 17:02 generation_config.json  # æ¨ç†å‚æ•°é…ç½®ã€‚å®šä¹‰äº†æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶çš„é»˜è®¤è¡Œä¸ºï¼ˆå¦‚ max_length, temperature, top_p, eos_token_id ç­‰ï¼‰ã€‚\n",
        "\n",
        "# ----------------- ç‰ˆæœ¬æ§åˆ¶ä¸åè®® -----------------\n",
        "-rw-r--r--  1 root root       2175 Dec  4 17:02 .gitattributes          # Git LFS é…ç½®æ–‡ä»¶ã€‚å‘Šè¯‰ git å“ªäº›æ–‡ä»¶æ˜¯å¤§æ–‡ä»¶ï¼ˆå¦‚ .safetensorsï¼‰ï¼Œéœ€è¦ç”¨ LFS æ–¹å¼ä¸‹è½½ã€‚\n",
        "-rw-r--r--  1 root root      11544 Dec  4 17:02 LICENSE                 # æ¨¡å‹çš„å¼€æºè®¸å¯è¯æ–‡ä»¶ï¼ˆå¦‚ Apache 2.0 æˆ– Qwen Research Licenseï¼‰ï¼Œè§„å®šäº†ä½ èƒ½å¦å•†ç”¨ã€‚\n",
        "\n",
        "# ----------------- åˆ†è¯å™¨ (Tokenizer) è¯è¡¨åŒº -----------------\n",
        "# ä½œç”¨ï¼šå°†äººç±»æ–‡å­—è½¬æ¢æˆæ•°å­— IDï¼Œåä¹‹äº¦ç„¶\n",
        "-rw-r--r--  1 root root    1823241 Dec  4 17:02 merges.txt              # BPE åˆå¹¶è§„åˆ™æ–‡ä»¶ã€‚è®°å½•äº†å“ªäº›å­—ç¬¦ç»„åˆå¯ä»¥åˆå¹¶æˆä¸€ä¸ª Tokenï¼ˆå¸¸è§äº GPT ç±»æ¨¡å‹ï¼‰ã€‚\n",
        "-rw-r--r--  1 root root    2776833 Dec  4 17:02 vocab.json              # è¯è¡¨æ–‡ä»¶ã€‚ä¸€ä¸ªå·¨å¤§çš„å­—å…¸ï¼Œè®°å½•äº†æ‰€æœ‰ Token å­—ç¬¦ä¸²åˆ°æ•°å­— ID çš„æ˜ å°„å…³ç³»ã€‚\n",
        "-rw-r--r--  1 root root   11422654 Dec  4 17:02 tokenizer.json          # ã€é‡è¦ã€‘Fast Tokenizer çš„å®Œæ•´å®šä¹‰æ–‡ä»¶ã€‚åŒ…å«äº†åˆ†è¯çš„æ‰€æœ‰é€»è¾‘ï¼ŒåŠ è½½é€Ÿåº¦æ¯” Python ä»£ç å¿«ã€‚\n",
        "-rw-r--r--  1 root root       9971 Dec  4 17:02 tokenizer_config.json   # åˆ†è¯å™¨è®¾ç½®æ–‡ä»¶ã€‚å®šä¹‰äº†ç‰¹æ®Š Tokenï¼ˆå¦‚ <|endoftext|>ï¼‰ï¼Œä»¥åŠä½¿ç”¨å“ªä¸ª Tokenizer ç±»ã€‚\n",
        "\n",
        "# ----------------- æ¨¡å‹æƒé‡åŒº (Weights) -----------------\n",
        "# ä½œç”¨ï¼šæ¨¡å‹çš„â€œå¤§è„‘â€ï¼Œå­˜å‚¨äº†è®­ç»ƒå¥½çš„å‚æ•°ï¼ˆçŸ©é˜µæ•°å€¼ï¼‰\n",
        "# ç”±äº 8B æ¨¡å‹å¾ˆå¤§ï¼ˆçº¦ 15GB+ï¼‰ï¼Œæ‰€ä»¥è¢«åˆ‡åˆ†æˆäº† 5 ä¸ªæ–‡ä»¶ï¼ˆShardingï¼‰\n",
        "-rw-r--r--  1 root root 3996250744 Dec  4 17:41 model-00001-of-00005.safetensors # æƒé‡åˆ†ç‰‡ 1/5\n",
        "-rw-r--r--  1 root root 3993160032 Dec  4 17:45 model-00002-of-00005.safetensors # æƒé‡åˆ†ç‰‡ 2/5\n",
        "-rw-r--r--  1 root root 3959604768 Dec  4 17:47 model-00003-of-00005.safetensors # æƒé‡åˆ†ç‰‡ 3/5\n",
        "-rw-r--r--  1 root root 3187841392 Dec  4 17:43 model-00004-of-00005.safetensors # æƒé‡åˆ†ç‰‡ 4/5\n",
        "-rw-r--r--  1 root root 1244659840 Dec  4 17:14 model-00005-of-00005.safetensors # æƒé‡åˆ†ç‰‡ 5/5\n",
        "\n",
        "# ----------------- æƒé‡ç´¢å¼•ä¸è¯´æ˜ -----------------\n",
        "-rw-r--r--  1 root root      33284 Dec  4 17:02 model.safetensors.index.json     # æƒé‡ç´¢å¼•æ˜ å°„è¡¨ã€‚å› ä¸ºæƒé‡è¢«åˆ‡åˆ†äº†ï¼Œè¿™ä¸ªæ–‡ä»¶å‘Šè¯‰åŠ è½½å™¨ï¼šâ€œç¬¬ X å±‚çš„å‚æ•°åœ¨ç¬¬ Y ä¸ª safetensors æ–‡ä»¶é‡Œâ€ã€‚\n",
        "-rw-r--r--  1 root root      17012 Dec  4 17:02 README.md               # æ¨¡å‹è¯´æ˜ä¹¦ï¼ˆMarkdown æ ¼å¼ï¼‰ã€‚é€šå¸¸åŒ…å«æ¨¡å‹ä»‹ç»ã€å¼•ç”¨æ–¹æ³•ã€ä½¿ç”¨ç¤ºä¾‹ä»£ç ç­‰ã€‚\n",
        "```\n",
        "\n",
        "å½“ä½ è¿è¡Œ `AutoModelForCausalLM.from_pretrained(\"è¿™ä¸ªç›®å½•\")` æ—¶ï¼š\n",
        "\n",
        "1. **è¯»å– `config.json`**ï¼šæ­å»ºç©ºçš„ç¥ç»ç½‘ç»œéª¨æ¶ã€‚\n",
        "2. **è¯»å– `model.safetensors.index.json`**ï¼šæŸ¥æ‰¾å‚æ•°éƒ½åœ¨å“ªã€‚\n",
        "3. **åŠ è½½ `model-0000\\*.safetensors`**ï¼šæŠŠå…·ä½“çš„æ•°å­—å¡«å…¥éª¨æ¶ä¸­ã€‚\n",
        "\n",
        "å½“ä½ è¿è¡Œ `AutoTokenizer.from_pretrained(\"è¿™ä¸ªç›®å½•\")` æ—¶ï¼š\n",
        "\n",
        "1. **è¯»å– `tokenizer_config.json`**ï¼šç¡®è®¤é…ç½®ã€‚\n",
        "2. **ä¼˜å…ˆåŠ è½½ `tokenizer.json`**ï¼šå¦‚æœå¤±è´¥ï¼Œæ‰å»å°è¯•ç»„åˆ `vocab.json` å’Œ `merges.txt`ã€‚\n",
        "\n",
        "#### 3.2.2 å¾®è°ƒæ ¸å¿ƒæ–¹æ³• (Finetuning Method)\n",
        "* **å‚æ•°é¡¹**ï¼š`å¾®è°ƒæ–¹æ³•`\n",
        "* **å½“å‰å€¼**ï¼š`lora`\n",
        "* **é…ç½®ä½œç”¨**ï¼š\n",
        "    * é€‰æ‹© **LoRA** (Low-Rank Adaptation) æŠ€æœ¯ã€‚å®ƒä¸ä¼šä¿®æ”¹æ¨¡å‹åŸæœ¬çš„å‡ åäº¿ä¸ªå‚æ•°ï¼Œè€Œæ˜¯åœ¨æ¨¡å‹æ—æŒ‚è½½ä¸€äº›å°çš„â€œé€‚é…å™¨â€çŸ©é˜µæ¥è®­ç»ƒã€‚\n",
        "* **ä¸ºä»€ä¹ˆä¸é€‰ fullï¼ˆå…¨é‡ï¼‰**ï¼š\n",
        "    * å…¨é‡å¾®è°ƒ 8B æ¨¡å‹éœ€è¦ä¿å­˜åºå¤§çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œè‡³å°‘éœ€è¦ 100GB+ çš„æ˜¾å­˜ã€‚\n",
        "    * **A10 (24G) çš„å”¯ä¸€å‡ºè·¯**ï¼šå¯¹äºå•å¡ A10ï¼Œå¿…é¡»ä½¿ç”¨ LoRA æˆ– QLoRA æ‰èƒ½è·‘å¾—èµ·æ¥ã€‚\n",
        "\n",
        "#### 3.2.3 é‡åŒ–è®¾ç½® (Quantization) â€”â€” å…³é”®ä¼˜åŒ–\n",
        "è¿™ä¸€æ­¥æ˜¯å°† **LoRA** å‡çº§ä¸º **QLoRA** çš„å…³é”®ï¼Œç›´æ¥å†³å®šäº†æ‚¨èƒ½è®­ç»ƒå¤šé•¿çš„åŒ»ç–—ç—…ä¾‹ã€‚\n",
        "\n",
        "* **å‚æ•°é¡¹**ï¼š`é‡åŒ–ç­‰çº§`\n",
        "* **å½“å‰å€¼**ï¼š`4`\n",
        "* **é…ç½®ä½œç”¨**ï¼š\n",
        "    * å°†æ¨¡å‹æƒé‡çš„ç²¾åº¦ä» 16-bit (åŠç²¾åº¦) å‹ç¼©åˆ° 4-bitã€‚\n",
        "* **å¯¹ A10 (24G) çš„å·¨å¤§å½±å“**ï¼š\n",
        "    * **ä¸é‡åŒ– (16-bit)**ï¼š8B æ¨¡å‹ä»…æƒé‡å°±å ç”¨çº¦ **16GB** æ˜¾å­˜ã€‚æ­¤æ—¶ A10 åªå‰© 8GB ç»™è®­ç»ƒæ•°æ®ï¼Œåªè¦åŒ»ç–—æ–‡æœ¬ç¨å¾®é•¿ä¸€ç‚¹ï¼ˆä¾‹å¦‚åŒ…å«å¤æ‚çš„ç—…æƒ…æè¿°ï¼‰ï¼Œæ˜¾å­˜å°±ä¼šç¬é—´æº¢å‡º (OOM)ã€‚\n",
        "    * **4-bit é‡åŒ–**ï¼šæ¨¡å‹æƒé‡ä»…å ç”¨çº¦ **5.5GB** æ˜¾å­˜ã€‚\n",
        "    * **æ”¶ç›Š**ï¼šæ‚¨ç©ºå‡ºäº†è¿‘ **18GB** çš„æ˜¾å­˜ï¼è¿™æ„å‘³ç€æ‚¨å¯ä»¥å°† `Context Length`ï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰æ‹‰å¾—å¾ˆå¤§ï¼ˆä¾‹å¦‚ 4096 æˆ– 8192ï¼‰ï¼Œè¿™å¯¹äº **medical-o1-reasoning** è¿™ç§éœ€è¦é•¿é€»è¾‘é“¾æ¨ç†çš„æ•°æ®é›†è‡³å…³é‡è¦ã€‚\n",
        "\n",
        "* **å‚æ•°é¡¹**ï¼š`é‡åŒ–æ–¹æ³•`\n",
        "* **å½“å‰å€¼**ï¼š`bnb` (bitsandbytes)\n",
        "* **é…ç½®ä½œç”¨**ï¼š\n",
        "    * è¿™æ˜¯å®ç° 4-bit é‡åŒ–çš„åº•å±‚åŠ é€Ÿåº“ã€‚æ‚¨çš„ç¯å¢ƒæ˜¯ Ubuntu 22.04 + CUDA 12.4ï¼Œ`bnb` èƒ½å¤Ÿå®Œç¾è°ƒç”¨ A10 çš„ç¡¬ä»¶åŠ é€Ÿèƒ½åŠ›ã€‚\n",
        "\n",
        "#### 3.2.4 æ•°æ®æ¨¡æ¿ä¸æ ¼å¼ (Template)\n",
        "* **å‚æ•°é¡¹**ï¼š`å¯¹è¯æ¨¡æ¿`\n",
        "* **å½“å‰å€¼**ï¼š`qwen3` (æ³¨æ„ï¼šæˆªå›¾å¯èƒ½éœ€ç¡®è®¤ä¸º qwen æˆ– qwen2.5/3 å¯¹åº”æ¨¡æ¿ï¼ŒQwen3 é€šå¸¸å…¼å®¹ Qwen2.5 æ¨¡æ¿)\n",
        "* **é…ç½®ä½œç”¨**ï¼š\n",
        "    * å¦‚åŒâ€œç¿»è¯‘å™¨â€ï¼Œå®ƒå°†æ‚¨çš„åŒ»ç–—æ•°æ®é›†æ ¼å¼åŒ–ä¸º Qwen æ¨¡å‹èƒ½å¬æ‡‚çš„ç‰¹æ®Šå­—ç¬¦ç»“æ„ï¼ˆä¾‹å¦‚ `<|im_start|>`ï¼‰ã€‚\n",
        "* **ä¸¥é‡åæœ**ï¼šå¦‚æœé€‰é”™ï¼ˆä¾‹å¦‚é€‰æˆ llama3ï¼‰ï¼Œæ¨¡å‹ä¼šå› ä¸ºçœ‹ä¸æ‡‚æ•°æ®ç»“æ„è€Œè¾“å‡ºä¹±ç ï¼Œæˆ–è€…åœ¨è®­ç»ƒæ—¶ loss æ— æ³•ä¸‹é™ã€‚åŠ¡å¿…ç¡®ä¿ä¸æ¨¡å‹ç³»åˆ—ä¸€è‡´ã€‚\n",
        "\n",
        "#### 3.2.5 ç¡¬ä»¶åŠ é€Ÿç­–ç•¥ (Booster)\n",
        "* **å‚æ•°é¡¹**ï¼š`åŠ é€Ÿæ–¹å¼`\n",
        "* **å½“å‰å€¼**ï¼š`auto`\n",
        "* **é…ç½®ä½œç”¨**ï¼š\n",
        "    * è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„æ³¨æ„åŠ›æœºåˆ¶åŠ é€Ÿç®—æ³•ï¼Œé€šå¸¸ä¼šå¯ç”¨ **FlashAttention-2**ã€‚\n",
        "* **ç¯å¢ƒåŒ¹é…**ï¼š\n",
        "    * A10 æ˜¾å¡å±äº Ampere æ¶æ„ï¼Œå®Œç¾æ”¯æŒ FlashAttention-2ã€‚\n",
        "    * **æ•ˆæœ**ï¼šç›¸æ¯”ä¸å¼€å¯åŠ é€Ÿï¼Œè®­ç»ƒé€Ÿåº¦é€šå¸¸èƒ½æå‡ 2-3 å€ï¼Œä¸”èƒ½è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜ã€‚ä¿æŒ `auto` æ˜¯æœ€çœå¿ƒçš„é€‰æ‹©ã€‚\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74d9f0fc-4705-4808-8da8-1282e85c5b39",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-04T10:59:53.666507Z",
          "iopub.status.busy": "2025-12-04T10:59:53.666356Z",
          "iopub.status.idle": "2025-12-04T10:59:53.670561Z",
          "shell.execute_reply": "2025-12-04T10:59:53.669904Z",
          "shell.execute_reply.started": "2025-12-04T10:59:53.666492Z"
        },
        "tags": [],
        "id": "74d9f0fc-4705-4808-8da8-1282e85c5b39"
      },
      "source": [
        "#### 3.2.6 è®­ç»ƒé˜¶æ®µ (Training Stage)\n",
        "* **é…ç½®é¡¹**ï¼š`è®­ç»ƒé˜¶æ®µ`\n",
        "* **å½“å‰è®¾ç½®**ï¼š`Supervised Fine-Tuning` (SFT)\n",
        "* **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "    * **å®šä¹‰**ï¼šå…¨ç§°æ˜¯â€œæœ‰ç›‘ç£å¾®è°ƒâ€ã€‚è¿™æ˜¯ç›®å‰è®©å¤§æ¨¡å‹é€‚é…ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ï¼‰æœ€ç›´æ¥ã€æœ€é«˜æ•ˆçš„æ–¹æ³•ã€‚\n",
        "    * **åŸç†**ï¼šå°±å¥½æ¯”ç»™å­¦ç”Ÿï¼ˆæ¨¡å‹ï¼‰çœ‹å¤§é‡çš„â€œé—®é¢˜+æ ‡å‡†ç­”æ¡ˆâ€çš„è¯•å·ï¼Œè®©å®ƒå­¦ä¼šç…§è‘«èŠ¦ç”»ç“¢ã€‚ä¸ä¹‹ç›¸å¯¹çš„è¿˜æœ‰â€œé¢„è®­ç»ƒï¼ˆPre-Trainingï¼‰â€ï¼ˆè¯»è¯¾æœ¬ï¼Œå­¦é€šè¯†ï¼‰å’Œâ€œDPO/RLHFâ€ï¼ˆå­¦ä»·å€¼è§‚ï¼Œåˆ†è¾¨å¥½åï¼‰ã€‚\n",
        "    * **é€‚ç”¨åœºæ™¯**ï¼šæ‚¨ç°åœ¨çš„ç›®æ ‡æ˜¯è®© Qwen3 æ‹¥æœ‰åŒ»ç–—æ¨ç†èƒ½åŠ›ï¼ŒSFT æ˜¯æ ‡å‡†é€‰æ‹©ã€‚\n",
        "    * **ç¯å¢ƒå½±å“**ï¼šSFT ç›¸æ¯”é¢„è®­ç»ƒæå…¶èŠ‚çœèµ„æºã€‚åœ¨æ‚¨çš„ A10 (24G) ä¸Šï¼Œé…åˆ LoRAï¼ŒSFT è¿è¡Œèµ·æ¥éå¸¸è½»é‡ã€‚\n",
        "\n",
        "#### 3.2.7 æ•°æ®è·¯å¾„ä¸æ•°æ®é›†é€‰æ‹© (Data Configuration)\n",
        "* **é…ç½®é¡¹**ï¼š`æ•°æ®è·¯å¾„ (Data Path)` & `æ•°æ®é›† (Dataset)`\n",
        "* **å½“å‰è®¾ç½®**ï¼š\n",
        "    * è·¯å¾„ï¼š`data`\n",
        "    * æ•°æ®é›†ï¼š`medical_sft` (æˆªå›¾æ˜¾ç¤ºæ‚¨å·²é€‰ä¸­æ­¤é¡¹)\n",
        "* **è¯¦ç»†è§£é‡Š (æ–°æ‰‹å¿…è¯»)**ï¼š\n",
        "    * **æ•°æ®è·¯å¾„**ï¼šæŒ‡å‘ LLaMA-Factory æ ¹ç›®å½•ä¸‹çš„ `data` æ–‡ä»¶å¤¹ã€‚è¿™é‡Œé¢å­˜æ”¾ç€è‡³å…³é‡è¦çš„ `dataset_info.json` é…ç½®æ–‡ä»¶ã€‚\n",
        "    * **æ•°æ®é›†é€‰æ‹©**ï¼š\n",
        "        * è¿™é‡Œçš„ `medical_sft` å¹¶ä¸æ˜¯æ–‡ä»¶åï¼Œè€Œæ˜¯åœ¨ `dataset_info.json` ä¸­æ³¨å†Œçš„ä¸€ä¸ª**æ ‡ç­¾ï¼ˆKeyï¼‰**ã€‚\n",
        "        * **å…³é”®æ“ä½œ**ï¼šæ‚¨ä¹‹å‰æåˆ°äº† `medical-o1-reasoning-SFT` æ•°æ®é›†ã€‚æ‚¨éœ€è¦ç¡®ä¿å·²ç»åœ¨ `data/dataset_info.json` æ–‡ä»¶ä¸­æ·»åŠ äº†å¯¹åº”çš„é…ç½®ï¼Œå°† `medical_sft` è¿™ä¸ªæ ‡ç­¾æŒ‡å‘æ‚¨å®é™…ä¸‹è½½çš„ JSON æˆ– Parquet æ–‡ä»¶ã€‚\n",
        "        * *é”™è¯¯æ’æŸ¥*ï¼šå¦‚æœæ‚¨åœ¨ä¸‹æ‹‰èœå•é‡Œæ‰¾ä¸åˆ°æ‚¨çš„æ•°æ®é›†ï¼Œé€šå¸¸æ˜¯å› ä¸ºæ²¡æœ‰åœ¨ `dataset_info.json` é‡Œæ³¨å†Œï¼Œæˆ–è€…æ–‡ä»¶æ²¡æ”¾å¯¹ä½ç½®ã€‚\n",
        "* **ä½œç”¨**ï¼šå‘Šè¯‰ç¨‹åºä½¿ç”¨å“ªä¸€ä»½å…·ä½“çš„åŒ»ç–—ç—…å†æ•°æ®æ¥å–‚ç»™æ¨¡å‹ã€‚\n",
        "\n",
        "#### 3.2.8 æ•°æ®é¢„è§ˆåŠŸèƒ½ (Dataset Preview)\n",
        "* **é…ç½®é¡¹**ï¼š`é¢„è§ˆæ•°æ®é›†` (æŒ‰é’®)\n",
        "* **å»ºè®®æ“ä½œ**ï¼š**å¼ºçƒˆå»ºè®®ç‚¹å‡»**ã€‚\n",
        "* **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "    * **ä½œç”¨**ï¼šå®ƒä¼šè¯»å–å‰å‡ è¡Œæ•°æ®å¹¶å±•ç¤ºå‡ºæ¥ã€‚\n",
        "    * **æ–°æ‰‹æ£€æŸ¥ç‚¹**ï¼š\n",
        "        1.  **åˆ—ååŒ¹é…**ï¼šæ£€æŸ¥å±•ç¤ºçš„æ•°æ®ä¸­ï¼Œæ˜¯å¦åŒ…å«â€œinstructionâ€ï¼ˆæŒ‡ä»¤/é—®é¢˜ï¼‰ã€â€œinputâ€ï¼ˆè¾“å…¥/ç—…å†è¯¦æƒ…ï¼Œå¯é€‰ï¼‰ã€â€œoutputâ€ï¼ˆè¾“å‡º/è¯Šæ–­ç»“æœï¼‰è¿™å‡ åˆ—ã€‚\n",
        "        2.  **æ ¼å¼æ­£å¸¸**ï¼šç¡®ä¿æ²¡æœ‰ä¹±ç ï¼Œä¸”å†…å®¹ç¡®å®æ˜¯æ‚¨é¢„æœŸçš„åŒ»ç–—æ¨ç†æ•°æ®ã€‚\n",
        "    * **ä¸ºä»€ä¹ˆé‡è¦**ï¼šå¦‚æœæ•°æ®æ ¼å¼ä¸å¯¹ï¼ˆæ¯”å¦‚åˆ—åå†™æˆäº† `question` è€Œä¸æ˜¯ `instruction`ï¼‰ï¼Œæ¨¡å‹è®­ç»ƒæ—¶ä¼šæ‰¾ä¸åˆ°è¾“å…¥ï¼Œå¯¼è‡´è®­ç»ƒè™½ç„¶åœ¨è·‘ï¼Œä½†ä»€ä¹ˆéƒ½æ²¡å­¦åˆ°ï¼ˆLoss ä¸ä¸‹é™ï¼‰ã€‚åœ¨å¼€å§‹æ¼«é•¿çš„è®­ç»ƒå‰ï¼ŒèŠ± 10 ç§’é’Ÿé¢„è§ˆæ˜¯æ€§ä»·æ¯”æœ€é«˜çš„æ“ä½œã€‚\n",
        "\n",
        "æ•°æ®é›†ä½¿ç”¨æˆ‘ä»¬çš„åŒ»ç–—æ•°æ®é›†`medical_sft`ã€‚\n",
        "![image-20251203165018105](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512031650258.png)\n",
        "å¯ä»¥ç‚¹å‡»ã€Œé¢„è§ˆæ•°æ®é›†ã€ã€‚ç‚¹å‡»å…³é—­è¿”å›è®­ç»ƒç•Œé¢ã€‚\n",
        "![image-20251203165125842](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512031651285.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0bb212f-24d8-46a1-b4d1-8345f4f5531f",
      "metadata": {
        "id": "b0bb212f-24d8-46a1-b4d1-8345f4f5531f"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "è®¾ç½®`å­¦ä¹ ç‡`ä¸º`1e-4`ï¼Œ`æ¢¯åº¦ç´¯ç§¯`ä¸º`2`ï¼Œæœ‰åˆ©äºæ¨¡å‹æ‹Ÿåˆã€‚å¦‚æœæ˜¾å¡æ˜¯`V100`ï¼Œè®¡ç®—ç±»å‹ä¿æŒä¸º`fp16`ï¼›å¦‚æœä½¿ç”¨äº†`A10`ï¼Œå¯ä»¥æ›´æ”¹è®¡ç®—ç±»å‹ä¸º`bf16`ã€‚\n",
        "![image-20251203165304488](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512031653595.png)\n",
        "\n",
        "ç‚¹å‡»`LoRAå‚æ•°è®¾ç½®`å±•å¼€å‚æ•°åˆ—è¡¨ï¼Œè®¾ç½®`LoRA+å­¦ä¹ ç‡æ¯”ä¾‹`ä¸º`16`ï¼Œ`LoRA+`è¢«è¯æ˜æ˜¯æ¯”LoRAå­¦ä¹ æ•ˆæœæ›´å¥½çš„ç®—æ³•ã€‚åœ¨`LoRAä½œç”¨æ¨¡å—`ä¸­å¡«å†™`all`ï¼Œå³å°†LoRAå±‚æŒ‚è½½åˆ°æ¨¡å‹çš„æ‰€æœ‰çº¿æ€§å±‚ä¸Šï¼Œæé«˜æ‹Ÿåˆæ•ˆæœã€‚\n",
        "![image-20251203165600495](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512031656725.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fe1f273-1915-49a6-9b58-f0c4889e8c6f",
      "metadata": {
        "id": "8fe1f273-1915-49a6-9b58-f0c4889e8c6f"
      },
      "source": [
        "#### 3.2.9 å­¦ä¹ ç‡ (Learning Rate)\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`å­¦ä¹ ç‡`\n",
        "- **å½“å‰å€¼**ï¼š`1e-4` (å³ 0.0001)\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **ä½œç”¨**ï¼šå†³å®šäº†æ¨¡å‹â€œè¿ˆæ­¥å­â€çš„å¤§å°ã€‚\n",
        "  - **æ–°æ‰‹æŒ‡å—**ï¼š\n",
        "    - å¯¹äº **LoRA** å¾®è°ƒï¼Œ`1e-4` æ˜¯ä¸€ä¸ªéå¸¸æ ‡å‡†ä¸”ç¨³å¥çš„â€œé»„é‡‘åˆå§‹å€¼â€ã€‚\n",
        "    - å¦‚æœæ˜¯å…¨é‡å¾®è°ƒï¼Œè¿™ä¸ªå€¼é€šå¸¸è¦å°å¾—å¤šï¼ˆå¦‚ `1e-5`ï¼‰ã€‚ä½†å¯¹äº LoRAï¼Œä¿æŒ `1e-4` é€šå¸¸èƒ½è·å¾—ä¸é”™çš„æ”¶æ•›é€Ÿåº¦ã€‚\n",
        "  - **è°ƒæ•´å»ºè®®**ï¼šåˆæ¬¡è¿è¡Œå»ºè®®ä¿æŒä¸å˜ã€‚å¦‚æœå‘ç° Lossï¼ˆæŸå¤±å€¼ï¼‰å®Œå…¨ä¸ä¸‹é™ï¼Œå¯ä»¥å°è¯•ç¨å¾®è°ƒå¤§ï¼›å¦‚æœ Loss å‰§çƒˆéœ‡è¡ï¼Œåˆ™è°ƒå°ã€‚\n",
        "\n",
        "#### 3.2.10 è®­ç»ƒè½®æ•° (Epochs)\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`è®­ç»ƒè½®æ•°`\n",
        "- **å½“å‰å€¼**ï¼š`3.0`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **ä½œç”¨**ï¼šæŒ‡çš„æ˜¯æ¨¡å‹è¦æŠŠä½ çš„æ•´ä¸ªåŒ»ç–—æ•°æ®é›†å®Œæ•´åœ°â€œçœ‹â€å‡ éã€‚\n",
        "  - **å½±å“**ï¼š\n",
        "    - **è¿‡å°‘ (æ¯”å¦‚ 1)**ï¼šæ¨¡å‹å¯èƒ½è¿˜æ²¡å­¦ä¼šï¼Œå¯¼è‡´â€œæ¬ æ‹Ÿåˆâ€ã€‚\n",
        "    - **è¿‡å¤š (æ¯”å¦‚ 10)**ï¼šæ¨¡å‹å¯èƒ½ä¼šæ­»è®°ç¡¬èƒŒè®­ç»ƒé¢˜ï¼Œå¯¼è‡´â€œè¿‡æ‹Ÿåˆâ€ï¼Œé‡åˆ°æ–°ç—…å†å°±ä¸ä¼šæ²»äº†ã€‚\n",
        "  - **æ¨è**ï¼š`3.0` æ˜¯ç»å¤§å¤šæ•° SFT ä»»åŠ¡çš„æ ‡å‡†èµ·ç‚¹ï¼Œéå¸¸åˆé€‚ã€‚\n",
        "\n",
        "#### 3.2.11 æœ€å¤§æ ·æœ¬æ•° (Max Samples) â€”â€” **é‡è¦è­¦ç¤º**\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`æœ€å¤§æ ·æœ¬æ•°`\n",
        "- **å½“å‰å€¼**ï¼š`1000`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **ä½œç”¨**ï¼šå¼ºè¡Œé™åˆ¶åªä½¿ç”¨æ•°æ®é›†é‡Œçš„å‰ 1000 æ¡æ•°æ®æ¥è®­ç»ƒã€‚\n",
        "  - **æ–°æ‰‹è¯¯åŒºè­¦æŠ¥**ï¼š\n",
        "    - **è°ƒè¯•æ¨¡å¼**ï¼šå¦‚æœä½ åªæ˜¯æƒ³èŠ±å‡ åˆ†é’Ÿæµ‹è¯•ä¸€ä¸‹ A10 æ˜¾å¡èƒ½ä¸èƒ½è·‘é€šä»£ç ï¼Œè®¾ç½® `1000` æ˜¯å¯¹çš„ã€‚\n",
        "    - **æ­£å¼è®­ç»ƒ**ï¼š**è¯·åŠ¡å¿…æ¸…ç©ºæ­¤é¡¹**ï¼ˆç•™ç©ºï¼‰ã€‚å¦‚æœä½ çš„åŒ»ç–—æ•°æ®é›†æœ‰ 10 ä¸‡æ¡æ•°æ®ï¼Œå¡«äº† `1000` å°±æ„å‘³ç€æ¨¡å‹ä¸¢å¼ƒäº† 99% çš„çŸ¥è¯†ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æå…¶å¼±æ™ºã€‚\n",
        "  - **æ“ä½œå»ºè®®**ï¼šè·‘é€šæµ‹è¯•åï¼Œæ­£å¼è·‘æ•°æ®æ—¶è®°å¾—åˆ æ‰è¿™ä¸ªæ•°å­—ã€‚\n",
        "\n",
        "#### 3.2.12 æˆªæ–­é•¿åº¦ (Cutoff Length)\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`æˆªæ–­é•¿åº¦`\n",
        "- **å½“å‰å€¼**ï¼š`2048`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **ä½œç”¨**ï¼šé™åˆ¶ä¸€æ¡æ•°æ®ï¼ˆè¾“å…¥é—®é¢˜ + æ¨¡å‹å›ç­”ï¼‰çš„æœ€å¤§æ€»å­—æ•°ï¼ˆToken æ•°ï¼‰ã€‚è¶…è¿‡çš„éƒ¨åˆ†ä¼šè¢«â€œä¸€åˆ€åˆ‡â€åˆ‡æ‰ã€‚\n",
        "  - **ç”Ÿäº§å»ºè®®**ï¼š **Medical O1 Reasoning (åŒ»ç–—æ¨ç†)**ã€‚è¿™ç±»æ•°æ®é€šå¸¸åŒ…å«å¾ˆé•¿çš„æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰ï¼Œè¯¦ç»†æè¿°ç—…æƒ…å’Œæ¨ç†è¿‡ç¨‹ã€‚`2048` å¯èƒ½åçŸ­ï¼Œå®¹æ˜“æŠŠå…³é”®çš„æ¨ç†ç»“è®ºåˆ‡æ‰ã€‚\n",
        "\n",
        "#### 3.2.13 æ˜¾å­˜æ§åˆ¶æ ¸å¿ƒï¼šæ‰¹å¤„ç†ä¸æ¢¯åº¦ç´¯ç§¯\n",
        "\n",
        "è¿™é‡Œæ˜¯å†³å®šä½ è®­ç»ƒé€Ÿåº¦å’Œæ˜¾å­˜å ç”¨çš„â€œæ²¹é—¨â€å’Œâ€œåˆ¹è½¦â€ã€‚\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`æ‰¹å¤„ç†å¤§å° (Batch Size)`\n",
        "- **å½“å‰å€¼**ï¼š`1`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **å«ä¹‰**ï¼šæ¯ä¸ª GPU ä¸€æ¬¡è¯»å…¥å¹¶è®¡ç®— 1 æ¡æ•°æ®ã€‚\n",
        "- **é…ç½®é¡¹**ï¼š`æ¢¯åº¦ç´¯ç§¯ (Gradient Accumulation)`\n",
        "- **å½“å‰å€¼**ï¼š`4`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **å«ä¹‰**ï¼šæ”’å¤Ÿ 4 æ¬¡ï¼ˆ1x4=4ï¼‰å†ä¿®æ”¹ä¸€æ¬¡æ¨¡å‹å‚æ•°ã€‚\n",
        "  - **çœŸå®æ‰¹å¤§å°**ï¼šå®é™…ç”Ÿæ•ˆçš„ Batch Size = `æ‰¹å¤„ç†å¤§å°` * `æ¢¯åº¦ç´¯ç§¯` = 1 * 4 = 4ã€‚\n",
        "\n",
        "#### 3.2.14 éªŒè¯é›†æ¯”ä¾‹ (Val Ratio)\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`éªŒè¯é›†æ¯”ä¾‹`\n",
        "- **å½“å‰å€¼**ï¼š`0.15` (15%)\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **ä½œç”¨**ï¼šä»è®­ç»ƒæ•°æ®é‡Œåˆ‡å‡º 15% çš„é¢˜ä¸ç»™æ¨¡å‹å­¦ï¼Œä¸“é—¨ç•™åˆ°è€ƒè¯•ç”¨ï¼Œçœ‹çœ‹æ¨¡å‹æ˜¯ä¸æ˜¯çœŸçš„æ‡‚äº†ï¼ˆè¿˜æ˜¯æ­»è®°ç¡¬èƒŒï¼‰ã€‚\n",
        "  - **å»ºè®®**ï¼š\n",
        "    - å¦‚æœæ•°æ®æ€»é‡å¾ˆå¤§ï¼ˆæ¯”å¦‚ >1ä¸‡æ¡ï¼‰ï¼Œ15% å¯èƒ½å¤ªå¤šäº†ï¼ˆæµªè´¹æ•°æ®ï¼‰ï¼Œå¯ä»¥æ”¹ä¸º `0.05`ã€‚\n",
        "    - å¦‚æœæ•°æ®é‡å°‘ï¼ˆ<1000æ¡ï¼‰ï¼Œ`0.15` æ˜¯åˆç†çš„ã€‚\n",
        "\n",
        "#### 3.2.15 è®¡ç®—ç±»å‹ (Compute Type)\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`è®¡ç®—ç±»å‹`\n",
        "- **å½“å‰å€¼**ï¼š`bf16`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **å«ä¹‰**ï¼šä½¿ç”¨ Brain Float 16 ç²¾åº¦è¿›è¡Œè®¡ç®—ã€‚\n",
        "  - **ç¯å¢ƒåŒ¹é…**ï¼š**å®Œç¾**ã€‚ä½ çš„ A10 æ˜¾å¡ï¼ˆAmpere æ¶æ„ï¼‰åŸç”Ÿæ”¯æŒ bf16ã€‚ç›¸æ¯” fp16ï¼Œbf16 æ›´åŠ ç¨³å®šï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¸å®¹æ˜“å‡ºç° Loss å˜æˆ NaNï¼ˆæ•°å€¼æº¢å‡ºï¼‰çš„æŠ¥é”™ã€‚è¯·åŠ¡å¿…ä¿æŒ `bf16`ã€‚\n",
        "\n",
        "![image-20251204190718700](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512041907846.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11679d68-de89-4d29-ba89-8704ba4027d9",
      "metadata": {
        "id": "11679d68-de89-4d29-ba89-8704ba4027d9"
      },
      "source": [
        "#### 3.2.16 æ—¥å¿—ä¸ä¿å­˜é…ç½® (Logging & Checkpointing)\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`æ—¥å¿—é—´éš”`\n",
        "- **å½“å‰å€¼**ï¼š`5`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **ä½œç”¨**ï¼šæ¯è®­ç»ƒ 5 æ­¥ï¼ˆStepsï¼‰ï¼Œåœ¨å±å¹•ä¸Šæ‰“å°ä¸€æ¬¡å½“å‰çš„ Lossï¼ˆæŸå¤±å€¼ï¼‰å’Œå­¦ä¹ ç‡ã€‚\n",
        "  - **å»ºè®®**ï¼šä¿æŒé»˜è®¤ã€‚è¿™ä¸ªé¢‘ç‡èƒ½è®©ä½ å®æ—¶çœ‹åˆ°æ¨¡å‹æ˜¯ä¸æ˜¯â€œå­¦åºŸäº†â€ï¼ˆLoss æš´æ¶¨ï¼‰æˆ–è€…â€œå­¦å¾—å¥½â€ï¼ˆLoss ç¨³æ­¥ä¸‹é™ï¼‰ï¼Œæ–¹ä¾¿åŠæ—¶ç»ˆæ­¢é”™è¯¯è®­ç»ƒã€‚\n",
        "- **é…ç½®é¡¹**ï¼š`ä¿å­˜é—´éš”`\n",
        "- **å½“å‰å€¼**ï¼š`100`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **ä½œç”¨**ï¼šæ¯è®­ç»ƒ 100 æ­¥ï¼Œè‡ªåŠ¨æŠŠå½“å‰è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡ä¿å­˜ä¸ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ˆCheckpointï¼‰ã€‚\n",
        "  - **ç¡¬ç›˜ç©ºé—´é¢„è­¦**ï¼šè™½ç„¶ LoRA æ–‡ä»¶å¾ˆå°ï¼ˆå‡ å MBï¼‰ï¼Œä½†å¦‚æœä½ çš„è®­ç»ƒæ€»æ­¥æ•°å¾ˆå¤šï¼ˆæ¯”å¦‚ 10000 æ­¥ï¼‰ï¼Œæ¯ 100 æ­¥å­˜ä¸€æ¬¡ä¼šäº§ç”Ÿ 100 ä¸ªæ–‡ä»¶å¤¹ï¼Œç®¡ç†èµ·æ¥å¾ˆéº»çƒ¦ã€‚\n",
        "  - **ä¼˜åŒ–å»ºè®®**ï¼šå¦‚æœæ˜¯æ­£å¼è®­ç»ƒï¼Œå»ºè®®æ”¹ä¸º `500` æˆ– `1000`ï¼Œé¿å…ç”Ÿæˆå¤ªå¤šä¸­é—´æ–‡ä»¶ã€‚\n",
        "\n",
        "#### 3.2.17 é¢„çƒ­æ­¥æ•° (Warmup Steps)\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`é¢„çƒ­æ­¥æ•°`\n",
        "- **å½“å‰å€¼**ï¼š`0`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **å«ä¹‰**ï¼šåœ¨è®­ç»ƒåˆšå¼€å§‹æ—¶ï¼Œå…ˆç”¨å¾ˆå°çš„å­¦ä¹ ç‡æ…¢æ…¢çƒ­èº«ï¼Œç„¶åå†å¢åŠ åˆ° `1e-4`ã€‚\n",
        "  - **ä½œç”¨**ï¼šé˜²æ­¢æ¨¡å‹ä¸€å¼€å§‹å› ä¸ºâ€œæ­¥å­è¿ˆå¤ªå¤§â€è€Œè·‘åã€‚\n",
        "  - **å»ºè®®**ï¼šå¯¹äºåŒ»ç–—æ¨ç†è¿™ç§å¤æ‚ä»»åŠ¡ï¼Œå»ºè®®è®¾ç½®ä¸ºæ€»æ­¥æ•°çš„ 10% å·¦å³ï¼ˆä¾‹å¦‚å¡« `10` æˆ– `20`ï¼‰ã€‚è¿™èƒ½è®©æ¨¡å‹æ›´å¹³æ»‘åœ°è¿›å…¥å­¦ä¹ çŠ¶æ€ã€‚å¡« `0` ä¹Ÿèƒ½è·‘ï¼Œä½†ä¸å¤Ÿç¨³å¥ã€‚\n",
        "\n",
        "#### 3.2.18 é¢å¤–å‚æ•°æ·±åº¦è§£æ (Extra Arguments) â€”â€” **æ ¸å¿ƒä¼˜åŒ–**\n",
        "\n",
        "è¾“å…¥æ¡†ä¸­å¡«å†™çš„è¿™æ®µä»£ç æ˜¯æ•´ä¸ªé…ç½®çš„â€œç‚¹ç›ä¹‹ç¬”â€ï¼š\n",
        "\n",
        "`{\"optim\": \"adamw_torch\", \"gradient_checkpointing\": true}`\n",
        "\n",
        "è¿™ä¸¤ä¸ªå‚æ•°å¯¹ A10 æ˜¾å¡çš„å½±å“æå¤§ï¼Œä¸‹é¢é€ä¸€è§£é‡Šï¼š\n",
        "\n",
        "**1. ä¼˜åŒ–å™¨è®¾ç½® (`\"optim\": \"adamw_torch\"`)**\n",
        "\n",
        "- **å«ä¹‰**ï¼šæŒ‡å®šä½¿ç”¨ PyTorch å®˜æ–¹å®ç°çš„ AdamW ä¼˜åŒ–å™¨ã€‚\n",
        "- **ä½œç”¨**ï¼š\n",
        "  - **ç¨³å®šæ€§**ï¼šè¿™æ˜¯æœ€æ ‡å‡†ã€å…¼å®¹æ€§æœ€å¥½çš„ä¼˜åŒ–å™¨å®ç°ã€‚ç›¸æ¯”äºä¸€äº›é­”æ”¹ç‰ˆï¼ˆå¦‚ 8bit-adamï¼‰ï¼Œå®ƒæ›´ä¸å®¹æ˜“æŠ¥é”™ã€‚\n",
        "  - **æ˜¾å­˜ä»£ä»·**ï¼šå®ƒéœ€è¦çš„æ˜¾å­˜ç¨å¤šä¸€ç‚¹ç‚¹ï¼Œä½†åœ¨ä½ çš„ 24G æ˜¾å¡ + 4-bit é‡åŒ–ç¯å¢ƒä¸‹ï¼Œè¿™ç‚¹å¼€é”€å®Œå…¨å¯ä»¥æ¥å—ã€‚\n",
        "\n",
        "**2. æ¢¯åº¦æ£€æŸ¥ç‚¹ (`\"gradient_checkpointing\": true`) â€”â€” çœæ˜¾å­˜ç¥å™¨**\n",
        "\n",
        "- **å«ä¹‰**ï¼šè¿™æ˜¯ä¸€ç§â€œä»¥æ—¶é—´æ¢ç©ºé—´â€çš„æŠ€æœ¯ã€‚\n",
        "  - **ä¸å¼€å¯æ—¶**ï¼šæ¨¡å‹åœ¨å‘å‰è®¡ç®—ï¼ˆForwardï¼‰æ—¶ï¼Œä¼šæŠŠæ¯ä¸€å±‚çš„ä¸­é—´ç»“æœï¼ˆActivationsï¼‰éƒ½å­˜æ»¡æ˜¾å­˜ï¼Œç•™ç€ç»™åå‘ä¼ æ’­ï¼ˆBackwardï¼‰ç”¨ã€‚è¿™éå¸¸åƒæ˜¾å­˜ï¼\n",
        "  - **å¼€å¯å**ï¼šæ¨¡å‹ä¸å­˜ä¸­é—´ç»“æœäº†ã€‚ç­‰éœ€è¦åå‘ä¼ æ’­æ—¶ï¼Œå†ä¸´æ—¶é‡æ–°ç®—ä¸€éã€‚\n",
        "- **å¯¹ A10 (24G) çš„å·¨å¤§ä»·å€¼**ï¼š\n",
        "  - **æ˜¾å­˜æš´é™**ï¼šå¼€å¯æ­¤é€‰é¡¹é€šå¸¸èƒ½èŠ‚çœ **20%** çš„æ˜¾å­˜å ç”¨ã€‚\n",
        "  - **ä»£ä»·**ï¼šè®­ç»ƒé€Ÿåº¦ä¼šå˜æ…¢çº¦ 20-30%ã€‚\n",
        "  - **ç»“è®º**ï¼š**å¿…é¡»å¼€å¯**ã€‚å¯¹äºå•å¡è®­ç»ƒï¼Œä¸ºäº†èƒ½è·‘é•¿æ–‡æœ¬ï¼ˆåŒ»ç–—ç—…å†ï¼‰ï¼Œç‰ºç‰²ä¸€ç‚¹é€Ÿåº¦æ¢å–æ›´å¤§çš„ä¸Šä¸‹æ–‡ç©ºé—´æ˜¯éå¸¸åˆ’ç®—çš„ã€‚\n",
        "\n",
        "#### 3.2.19 NEFTune å™ªå£°å‚æ•°\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`NEFTune å™ªå£°å‚æ•°`\n",
        "- **å½“å‰å€¼**ï¼š`0`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **ä½œç”¨**ï¼šä¸€ç§ç»™ Embedding å±‚åŠ å™ªå£°çš„æŠ€æœ¯ï¼Œèƒ½é˜²æ­¢è¿‡æ‹Ÿåˆã€‚\n",
        "  - **åˆå­¦è€…å»ºè®®**ï¼šä¿æŒ `0`ï¼ˆå…³é—­ï¼‰ã€‚ç›®å‰è¿™ä¸ªæŠ€æœ¯è¿˜åœ¨å®éªŒé˜¶æ®µï¼Œæœ‰æ—¶å€™ä¼šå¯¼è‡´æ¨¡å‹å˜å¾—â€œèƒ¡è¨€ä¹±è¯­â€ã€‚å¯¹äºåˆæ¬¡ä¸Šæ‰‹ï¼Œæˆ‘ä»¬å…ˆè¿½æ±‚â€œè·‘é€šâ€å’Œâ€œç¨³å®šâ€ã€‚\n",
        "\n",
        "![image-20251204191239259](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512041912330.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33260fe5-3f6b-4873-9e25-711a0c8523da",
      "metadata": {
        "id": "33260fe5-3f6b-4873-9e25-711a0c8523da"
      },
      "source": [
        "#### 3.2.20 LoRA ç§© (LoRA Rank)\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`LoRA ç§©`\n",
        "- **å½“å‰å€¼**ï¼š`8`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **å«ä¹‰**ï¼šå¯ä»¥ç†è§£ä¸º LoRA è¿™ä¸ªâ€œå¤–æŒ‚å¤§è„‘â€çš„å®¹é‡æˆ–å¸¦å®½ã€‚æ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹èƒ½å­¦åˆ°çš„å¤æ‚çŸ¥è¯†è¶Šå¤šï¼Œä½†ç”Ÿæˆçš„æƒé‡æ–‡ä»¶ï¼ˆAdapterï¼‰ä¹Ÿè¶Šå¤§ï¼Œæ˜¾å­˜å ç”¨ç¨é«˜ã€‚\n",
        "  - **A10 24G å»ºè®®**ï¼š\n",
        "    - `8` æ˜¯ LLaMA-Factory çš„é»˜è®¤å€¼ï¼Œéå¸¸èŠ‚çœèµ„æºï¼Œé€‚åˆå…¥é—¨è·‘é€šæµç¨‹ã€‚\n",
        "    - **è¿›é˜¶å»ºè®®**ï¼šé’ˆå¯¹ **Medical O1 Reasoning** è¿™ç§éœ€è¦å¼ºé€»è¾‘æ¨ç†çš„ä»»åŠ¡ï¼Œ`8` å¯èƒ½ç•¥æ˜¾å•è–„ã€‚å¦‚æœæ‚¨çš„æ˜¾å­˜è¿˜å¾ˆå¯Œè£•ï¼ˆç»“åˆ 4-bit é‡åŒ–é€šå¸¸æ˜¯å¯Œè£•çš„ï¼‰ï¼Œå»ºè®®å°è¯•å°†æ­¤å€¼è°ƒé«˜è‡³ **`16`** æˆ– **`32`**ï¼Œç”šè‡³ **`64`**ã€‚è¿™èƒ½æ˜¾è‘—æå‡æ¨¡å‹å¯¹å¤æ‚åŒ»ç–—é€»è¾‘çš„æ‹Ÿåˆèƒ½åŠ›ã€‚\n",
        "\n",
        "#### 3.2.21 LoRA ç¼©æ”¾ç³»æ•° (LoRA Alpha)\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`LoRA ç¼©æ”¾ç³»æ•°`\n",
        "- **å½“å‰å€¼**ï¼š`16`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **å«ä¹‰**ï¼šå†³å®šäº† LoRA å­¦åˆ°çš„æ–°æƒé‡å¯¹åŸæ¨¡å‹å½±å“çš„â€œå€ç‡â€ã€‚\n",
        "  - **é»„é‡‘æ³•åˆ™**ï¼šé€šå¸¸è®¾ç½®ä¸º `LoRA ç§©` çš„ 2 å€ã€‚\n",
        "  - **æ“ä½œå»ºè®®**ï¼šå¦‚æœæ‚¨æŠŠä¸Šé¢çš„ Rank æ”¹æˆäº† 16ï¼Œè®°å¾—æŠŠè¿™é‡Œçš„ Alpha æ”¹æˆ 32ï¼›å¦‚æœ Rank æ˜¯ 32ï¼Œè¿™é‡Œå°±å¡« 64ã€‚ä¿æŒ `Alpha = 2 * Rank` è¿™ä¸ªæ¯”ä¾‹æ˜¯æœ€ç¨³å¥çš„åšæ³•ã€‚\n",
        "\n",
        "#### 3.2.22 LoRA+ å­¦ä¹ ç‡æ¯”ä¾‹ (LoRA+ LR Ratio) â€”â€” **é«˜çº§æŠ€å·§**\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`LoRA+ å­¦ä¹ ç‡æ¯”ä¾‹`\n",
        "- **å½“å‰å€¼**ï¼š`16`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **å«ä¹‰**ï¼šè¿™æ˜¯ä¸€ä¸ªåä¸º **LoRA+** çš„ä¼˜åŒ–ç®—æ³•ã€‚å®ƒè®© LoRA çš„ä¸¤ä¸ªçŸ©é˜µï¼ˆçŸ©é˜µ A å’ŒçŸ©é˜µ Bï¼‰ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡è¿›è¡Œè®­ç»ƒã€‚\n",
        "  - **ä½œç”¨**ï¼šç ”ç©¶è¡¨æ˜ï¼Œè®©çŸ©é˜µ B å­¦å¾—æ¯”çŸ©é˜µ A å¿«ï¼ˆé€šå¸¸å¿« 16 å€å·¦å³ï¼‰ï¼Œèƒ½å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚\n",
        "  - **ç¯å¢ƒå½±å“**ï¼šå¼€å¯è¿™ä¸ªé€‰é¡¹ï¼ˆåªè¦æ•°å€¼ä¸ä¸º 0 å³å¼€å¯ï¼‰ä¸ä¼šå¢åŠ æ˜¾å­˜æ¶ˆè€—ï¼Œå±äºâ€œå…è´¹çš„åˆé¤â€ã€‚å¯¹äº Qwen ç³»åˆ—æ¨¡å‹ï¼Œä¿ç•™é»˜è®¤å€¼ `16` æ•ˆæœé€šå¸¸å¾ˆå¥½ã€‚\n",
        "\n",
        "#### 3.2.23 LoRA ä½œç”¨æ¨¡å— (Target Modules) â€”â€” **å…³é”®é…ç½®**\n",
        "\n",
        "- **é…ç½®é¡¹**ï¼š`LoRA ä½œç”¨æ¨¡å—`\n",
        "- **å½“å‰å€¼**ï¼š`all`\n",
        "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
        "  - **å«ä¹‰**ï¼šæŒ‡å®š LoRA å…·ä½“è¦æŒ‚è½½åœ¨æ¨¡å‹çš„å“ªäº›å±‚ä¸Šï¼ˆä¾‹å¦‚ Qã€Kã€V æ³¨æ„åŠ›å±‚ï¼Œæˆ–è€… MLP å±‚ï¼‰ã€‚\n",
        "  - **å½“å‰è®¾ç½® \"all\"**ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸æ¿€è¿›ä¸”ä¼˜ç§€çš„è®¾ç½®ã€‚å®ƒæ„å‘³ç€ LoRA ä¼šæŒ‚è½½åˆ°æ¨¡å‹æ‰€æœ‰çš„çº¿æ€§å±‚ä¸Šï¼ˆ`q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`ï¼‰ã€‚\n",
        "  - **å¯¹åŒ»ç–—æ¨ç†çš„å½±å“**ï¼š\n",
        "    - ä»…æŒ‚è½½ Q/Vï¼ˆä¼ ç»Ÿåšæ³•ï¼‰ï¼šæ¨¡å‹åªèƒ½å­¦åˆ°ä¸€ç‚¹çš®æ¯›ã€‚\n",
        "    - **æŒ‚è½½ \"all\"**ï¼šæ¨¡å‹èƒ½å…¨æ–¹ä½åœ°è°ƒæ•´è‡ªå·±çš„é€»è¾‘å¤„ç†èƒ½åŠ›ã€‚å¯¹äºæ¨ç†ç±»ï¼ˆReasoningï¼‰ä»»åŠ¡ï¼Œå¿…é¡»é€‰ `all` æ‰èƒ½è·å¾—æœ€ä½³æ•ˆæœã€‚\n",
        "  - **ç¡¬ä»¶å‹åŠ›**ï¼šç›¸æ¯”åªæŒ‚è½½ Q/Vï¼Œé€‰æ‹© `all` ä¼šå¢åŠ ä¸€äº›æ˜¾å­˜æ¶ˆè€—å’Œè®¡ç®—é‡ã€‚ä½†åœ¨ **A10 (24G)** ä¸Šï¼Œè¿™å®Œå…¨åœ¨æ‰¿å—èŒƒå›´å†…ã€‚**è¯·åŠ¡å¿…ä¿æŒ \"all\"**ã€‚\n",
        "\n",
        "![image-20251204191714495](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512041917591.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb0f6dac-54af-4f83-90a6-922c7054301d",
      "metadata": {
        "id": "eb0f6dac-54af-4f83-90a6-922c7054301d"
      },
      "source": [
        "### 3.3 å¯åŠ¨å¾®è°ƒ\n",
        "å°†è¾“å‡ºç›®å½•ä¿®æ”¹ä¸º`train_qwen3_8b`ï¼Œè®­ç»ƒåçš„LoRAæƒé‡å°†ä¼šä¿å­˜åœ¨æ­¤ç›®å½•ä¸­ã€‚ç‚¹å‡»ã€Œé¢„è§ˆå‘½ä»¤ã€å¯å±•ç¤ºæ‰€æœ‰å·²é…ç½®çš„å‚æ•°ï¼Œæ‚¨å¦‚æœæƒ³é€šè¿‡ä»£ç è¿è¡Œå¾®è°ƒï¼Œå¯ä»¥å¤åˆ¶è¿™æ®µå‘½ä»¤ï¼Œåœ¨å‘½ä»¤è¡Œè¿è¡Œã€‚\n",
        "\n",
        "ç‚¹å‡»ã€Œå¼€å§‹ã€å¯åŠ¨æ¨¡å‹å¾®è°ƒã€‚\n",
        "![image-20251203170030807](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512031700144.png)\n",
        "\n",
        "å¯åŠ¨å¾®è°ƒåéœ€è¦ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œå¾…æ¨¡å‹ä¸‹è½½å®Œæ¯•åå¯åœ¨ç•Œé¢è§‚å¯Ÿåˆ°è®­ç»ƒè¿›åº¦å’ŒæŸå¤±æ›²çº¿ã€‚æ¨¡å‹å¾®è°ƒå¤§çº¦éœ€è¦50åˆ†é’Ÿï¼Œæ˜¾ç¤ºâ€œè®­ç»ƒå®Œæ¯•â€ä»£è¡¨å¾®è°ƒæˆåŠŸã€‚\n",
        "\n",
        "![image-20251204200651649](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042006782.png)\n",
        "\n",
        "è®­ç»ƒå®Œæ•´æ—¥å¿—\n",
        "\n",
        "```bash\n",
        "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
        "* Running on local URL:  http://0.0.0.0:7860\n",
        "* To create a public link, set `share=True` in `launch()`.\n",
        "[WARNING|2025-12-04 19:05:54] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
        "[INFO|2025-12-04 19:05:54] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file vocab.json\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file merges.txt\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file tokenizer.json\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file added_tokens.json\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file special_tokens_map.json\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file tokenizer_config.json\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file chat_template.jinja\n",
        "[INFO|tokenization_utils_base.py:2299] 2025-12-04 19:05:54,685 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
        "[INFO|configuration_utils.py:696] 2025-12-04 19:05:54,686 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
        "[INFO|configuration_utils.py:770] 2025-12-04 19:05:54,689 >> Model config Qwen3Config {\n",
        "  \"architectures\": [\n",
        "    \"Qwen3ForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"head_dim\": 128,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 12288,\n",
        "  \"max_position_embeddings\": 40960,\n",
        "  \"max_window_layers\": 36,\n",
        "  \"model_type\": \"qwen3\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 36,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 1000000,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.52.4\",\n",
        "  \"use_cache\": true,\n",
        "  \"use_sliding_window\": false,\n",
        "  \"vocab_size\": 151936\n",
        "}\n",
        "\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file vocab.json\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file merges.txt\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file tokenizer.json\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file added_tokens.json\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file special_tokens_map.json\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file tokenizer_config.json\n",
        "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,690 >> loading file chat_template.jinja\n",
        "[INFO|tokenization_utils_base.py:2299] 2025-12-04 19:05:55,018 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
        "[INFO|2025-12-04 19:05:55] llamafactory.data.loader:143 >> Loading dataset /mnt/workspace/medical_o1_sft_Chinese_alpaca_cot.jsonl...\n",
        "training example:\n",
        "input_ids:\n",
        "[151644, 872, 198, 100345, 53481, 3837, 46944, 16, 104252, 99657, 18493, 106084, 117405, 100347, 42140, 44290, 30709, 36885, 55502, 3837, 101930, 16530, 100939, 39762, 3837, 100136, 99601, 116066, 26288, 29524, 100711, 3837, 101776, 99577, 88653, 115623, 3837, 39426, 16530, 118609, 3837, 117405, 16872, 18830, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 1773, 100137, 116067, 18493, 104823, 15946, 105262, 100678, 99252, 11319, 220, 14880, 104137, 113272, 62926, 107485, 102349, 8997, 99487, 109661, 18493, 104797, 117405, 17447, 45861, 112312, 30709, 36885, 55502, 3837, 99725, 105562, 52801, 3837, 103957, 105231, 115623, 67279, 3837, 88653, 117633, 115623, 1773, 105839, 104797, 100624, 99259, 3837, 87267, 33108, 100102, 99259, 101063, 1773, 99306, 14777, 92015, 104006, 99406, 3837, 112575, 102347, 102441, 99193, 3837, 104797, 9370, 100102, 99259, 70927, 99308, 80158, 99873, 100585, 34187, 101099, 3407, 11622, 104823, 106413, 100192, 3837, 100347, 30709, 36885, 55502, 5373, 105566, 101930, 16530, 100939, 39762, 3837, 100001, 101368, 104029, 114316, 64355, 116066, 1773, 109661, 111335, 49828, 100001, 116771, 3837, 99558, 99519, 100102, 99259, 18493, 31914, 20742, 100601, 36885, 3407, 77288, 87256, 101997, 3837, 117405, 16872, 100626, 34794, 100743, 3837, 43288, 87267, 106015, 20412, 105172, 64355, 116066, 1773, 104544, 106141, 101174, 105806, 3837, 110090, 115623, 101561, 70927, 117083, 1773, 101893, 99559, 104823, 15946, 104854, 105792, 53556, 123, 116066, 100631, 100102, 116066, 3837, 74763, 104560, 109851, 99559, 3407, 49567, 100158, 3837, 117405, 101913, 34794, 100743, 33108, 102010, 49185, 99696, 118329, 101160, 99461, 100403, 26939, 117405, 16872, 3837, 100346, 99520, 66394, 102410, 20412, 88653, 25074, 57191, 121813, 120201, 11319, 100001, 101419, 38953, 53481, 107200, 57191, 117591, 9370, 101120, 101304, 3837, 104050, 18830, 32108, 115623, 16530, 100939, 39762, 3837, 99518, 101894, 102257, 57191, 34794, 100743, 102072, 3407, 104857, 105839, 3837, 35946, 99494, 100681, 100001, 101368, 33126, 111892, 121813, 120201, 105437, 11319, 101150, 106350, 104661, 106021, 33108, 104797, 106806, 105419, 33071, 100741, 3837, 100102, 99259, 104560, 35568, 62112, 3837, 77288, 87267, 103943, 79599, 99586, 100631, 110776, 100102, 105998, 101742, 99337, 3407, 102112, 99797, 111492, 53481, 108734, 3837, 100137, 101930, 16530, 100939, 39762, 99518, 106888, 104215, 3837, 62244, 100374, 104823, 33126, 99835, 29258, 9370, 99252, 13072, 3837, 104074, 102410, 20412, 102031, 33126, 113879, 9370, 101304, 26850, 87256, 101118, 100158, 3837, 43288, 99730, 99520, 112507, 121813, 120201, 3837, 49828, 104857, 101042, 117405, 49185, 99696, 62926, 100347, 34794, 100743, 101893, 101120, 101368, 1773, 104823, 69249, 64355, 3837, 101893, 101107, 87267, 33126, 101137, 14009, 101782, 116066, 527, 57191, 14009, 64355, 120471, 527, 1773, 100001, 99252, 13072, 102119, 53481, 107200, 101120, 101304, 104813, 101776, 102581, 33108, 99877, 100250, 99561, 3407, 101997, 105419, 33108, 104661, 108372, 3837, 104797, 99518, 100102, 99518, 99259, 3837, 47815, 102494, 104892, 99873, 17254, 107200, 3837, 111309, 99899, 99932, 9370, 102778, 72448, 115781, 104036, 1773, 64355, 120471, 99487, 99252, 13072, 115371, 101228, 99322, 39762, 3837, 108884, 53481, 9370, 101304, 101120, 3837, 101776, 102581, 26939, 100347, 34794, 100743, 3407, 100632, 3837, 104857, 115104, 33447, 99879, 3837, 100626, 18947, 99252, 13072, 101994, 104652, 100968, 3837, 105792, 14009, 121617, 120657, 119740, 527, 3837, 43288, 99252, 18493, 104823, 69249, 95411, 63367, 65101, 100137, 101120, 101304, 62926, 115563, 99194, 32948, 34794, 100743, 102072, 1773, 99652, 74763, 114369, 32108, 115623, 33108, 102010, 49185, 99696, 100001, 101368, 3407, 104170, 3837, 75882, 99670, 20412, 106084, 100102, 99259, 3837, 100673, 100102, 99586, 110540, 3837, 104661, 108372, 53153, 101515, 3837, 41146, 106141, 99185, 12857, 101893, 101304, 11319, 99799, 101042, 33447, 104288, 14009, 121617, 120657, 119740, 527, 99487, 99252, 13072, 101228, 102084, 101137, 1773, 151645, 198, 151644, 77091, 198, 151667, 198, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 8997, 151668, 271, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 1773, 151645, 198]\n",
        "inputs:\n",
        "<|im_start|>user\n",
        "æ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ è¯·é€æ­¥æ¨ç†å¹¶ç»™å‡ºç­”æ¡ˆã€‚\n",
        "è¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\n",
        "\n",
        "ç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\n",
        "\n",
        "ä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\n",
        "\n",
        "ç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\n",
        "\n",
        "ä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\n",
        "\n",
        "å›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\n",
        "\n",
        "å†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\n",
        "\n",
        "çœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\n",
        "\n",
        "ä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\n",
        "\n",
        "å“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚<|im_end|>\n",
        "<|im_start|>assistant\n",
        "<think>\n",
        "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
        "</think>\n",
        "\n",
        "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚<|im_end|>\n",
        "\n",
        "label_ids:\n",
        "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151667, 198, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 8997, 151668, 271, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 1773, 151645, 198]\n",
        "labels:\n",
        "<think>\n",
        "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
        "</think>\n",
        "\n",
        "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚<|im_end|>\n",
        "\n",
        "[INFO|configuration_utils.py:696] 2025-12-04 19:05:56,156 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
        "[INFO|configuration_utils.py:770] 2025-12-04 19:05:56,157 >> Model config Qwen3Config {\n",
        "  \"architectures\": [\n",
        "    \"Qwen3ForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"head_dim\": 128,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 12288,\n",
        "  \"max_position_embeddings\": 40960,\n",
        "  \"max_window_layers\": 36,\n",
        "  \"model_type\": \"qwen3\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 36,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 1000000,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.52.4\",\n",
        "  \"use_cache\": true,\n",
        "  \"use_sliding_window\": false,\n",
        "  \"vocab_size\": 151936\n",
        "}\n",
        "\n",
        "[INFO|2025-12-04 19:05:56] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
        "[INFO|2025-12-04 19:05:56] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
        "[INFO|modeling_utils.py:1148] 2025-12-04 19:05:57,751 >> loading weights file /mnt/workspace/Qwen3-8B/model.safetensors.index.json\n",
        "[INFO|modeling_utils.py:2241] 2025-12-04 19:05:57,752 >> Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.\n",
        "[INFO|configuration_utils.py:1135] 2025-12-04 19:05:57,753 >> Generate config GenerationConfig {\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"use_cache\": false\n",
        "}\n",
        "\n",
        "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.15s/it]\n",
        "[INFO|modeling_utils.py:5131] 2025-12-04 19:06:08,665 >> All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
        "\n",
        "[INFO|modeling_utils.py:5139] 2025-12-04 19:06:08,665 >> All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at /mnt/workspace/Qwen3-8B.\n",
        "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
        "[INFO|configuration_utils.py:1088] 2025-12-04 19:06:08,669 >> loading configuration file /mnt/workspace/Qwen3-8B/generation_config.json\n",
        "[INFO|configuration_utils.py:1135] 2025-12-04 19:06:08,669 >> Generate config GenerationConfig {\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"do_sample\": true,\n",
        "  \"eos_token_id\": [\n",
        "    151645,\n",
        "    151643\n",
        "  ],\n",
        "  \"pad_token_id\": 151643,\n",
        "  \"temperature\": 0.6,\n",
        "  \"top_k\": 20,\n",
        "  \"top_p\": 0.95\n",
        "}\n",
        "\n",
        "[INFO|2025-12-04 19:06:08] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
        "[INFO|2025-12-04 19:06:08] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
        "[INFO|2025-12-04 19:06:08] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
        "[INFO|2025-12-04 19:06:08] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
        "[INFO|2025-12-04 19:06:08] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,down_proj,o_proj,k_proj,v_proj,gate_proj,up_proj\n",
        "[INFO|2025-12-04 19:06:09] llamafactory.model.loader:143 >> trainable params: 21,823,488 || all params: 8,212,558,848 || trainable%: 0.2657\n",
        "[INFO|trainer.py:756] 2025-12-04 19:06:09,247 >> Using auto half precision backend\n",
        "[INFO|2025-12-04 19:06:09] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
        "[INFO|trainer.py:2409] 2025-12-04 19:06:09,529 >> ***** Running training *****\n",
        "[INFO|trainer.py:2410] 2025-12-04 19:06:09,529 >>   Num examples = 850\n",
        "[INFO|trainer.py:2411] 2025-12-04 19:06:09,529 >>   Num Epochs = 3\n",
        "[INFO|trainer.py:2412] 2025-12-04 19:06:09,529 >>   Instantaneous batch size per device = 1\n",
        "[INFO|trainer.py:2415] 2025-12-04 19:06:09,529 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
        "[INFO|trainer.py:2416] 2025-12-04 19:06:09,529 >>   Gradient Accumulation steps = 4\n",
        "[INFO|trainer.py:2417] 2025-12-04 19:06:09,529 >>   Total optimization steps = 639\n",
        "[INFO|trainer.py:2418] 2025-12-04 19:06:09,535 >>   Number of trainable parameters = 21,823,488\n",
        "  1%|â–                                          | 5/639 [00:20<42:31,  4.02s/it][INFO|2025-12-04 19:06:30] llamafactory.train.callbacks:143 >> {'loss': 0.6960, 'learning_rate': 9.9990e-05, 'epoch': 0.02, 'throughput': 697.13}\n",
        "{'loss': 0.696, 'grad_norm': 0.23502613604068756, 'learning_rate': 9.999033183566353e-05, 'epoch': 0.02, 'num_input_tokens_seen': 14376, 'train_runtime': 20.6278, 'train_tokens_per_second': 696.923}\n",
        "  2%|â–‹                                         | 10/639 [00:40<42:08,  4.02s/it][INFO|2025-12-04 19:06:50] llamafactory.train.callbacks:143 >> {'loss': 0.5438, 'learning_rate': 9.9951e-05, 'epoch': 0.05, 'throughput': 699.14}\n",
        "{'loss': 0.5438, 'grad_norm': 0.3318701684474945, 'learning_rate': 9.995106132599869e-05, 'epoch': 0.05, 'num_input_tokens_seen': 28296, 'train_runtime': 40.4784, 'train_tokens_per_second': 699.039}\n",
        "  2%|â–‰                                         | 15/639 [01:04<48:16,  4.64s/it][INFO|2025-12-04 19:07:13] llamafactory.train.callbacks:143 >> {'loss': 0.5772, 'learning_rate': 9.9882e-05, 'epoch': 0.07, 'throughput': 701.83}\n",
        "{'loss': 0.5772, 'grad_norm': 0.2540542483329773, 'learning_rate': 9.988160792165562e-05, 'epoch': 0.07, 'num_input_tokens_seen': 44992, 'train_runtime': 64.1129, 'train_tokens_per_second': 701.762}\n",
        "  3%|â–ˆâ–                                        | 20/639 [01:25<43:45,  4.24s/it][INFO|2025-12-04 19:07:34] llamafactory.train.callbacks:143 >> {'loss': 0.6002, 'learning_rate': 9.9782e-05, 'epoch': 0.09, 'throughput': 700.39}\n",
        "{'loss': 0.6002, 'grad_norm': 0.38846877217292786, 'learning_rate': 9.978201358980645e-05, 'epoch': 0.09, 'num_input_tokens_seen': 59576, 'train_runtime': 85.0673, 'train_tokens_per_second': 700.34}\n",
        "  4%|â–ˆâ–‹                                        | 25/639 [01:46<42:56,  4.20s/it][INFO|2025-12-04 19:07:55] llamafactory.train.callbacks:143 >> {'loss': 0.5981, 'learning_rate': 9.9652e-05, 'epoch': 0.12, 'throughput': 698.84}\n",
        "{'loss': 0.5981, 'grad_norm': 0.3376992642879486, 'learning_rate': 9.965233851025814e-05, 'epoch': 0.12, 'num_input_tokens_seen': 74240, 'train_runtime': 106.2394, 'train_tokens_per_second': 698.799}\n",
        "  5%|â–ˆâ–‰                                        | 30/639 [02:08<44:40,  4.40s/it][INFO|2025-12-04 19:08:18] llamafactory.train.callbacks:143 >> {'loss': 0.6136, 'learning_rate': 9.9493e-05, 'epoch': 0.14, 'throughput': 698.80}\n",
        "{'loss': 0.6136, 'grad_norm': 0.23448772728443146, 'learning_rate': 9.949266103908895e-05, 'epoch': 0.14, 'num_input_tokens_seen': 89944, 'train_runtime': 128.7181, 'train_tokens_per_second': 698.767}\n",
        "  5%|â–ˆâ–ˆâ–                                       | 35/639 [02:30<43:42,  4.34s/it][INFO|2025-12-04 19:08:40] llamafactory.train.callbacks:143 >> {'loss': 0.6039, 'learning_rate': 9.9303e-05, 'epoch': 0.16, 'throughput': 698.91}\n",
        "{'loss': 0.6039, 'grad_norm': 0.25201863050460815, 'learning_rate': 9.930307766130169e-05, 'epoch': 0.16, 'num_input_tokens_seen': 105288, 'train_runtime': 150.6526, 'train_tokens_per_second': 698.879}\n",
        "  6%|â–ˆâ–ˆâ–‹                                       | 40/639 [02:51<41:29,  4.16s/it][INFO|2025-12-04 19:09:00] llamafactory.train.callbacks:143 >> {'loss': 0.5387, 'learning_rate': 9.9084e-05, 'epoch': 0.19, 'throughput': 697.32}\n",
        "{'loss': 0.5387, 'grad_norm': 0.33039793372154236, 'learning_rate': 9.90837029325229e-05, 'epoch': 0.19, 'num_input_tokens_seen': 119432, 'train_runtime': 171.2783, 'train_tokens_per_second': 697.298}\n",
        "  7%|â–ˆâ–ˆâ–‰                                       | 45/639 [03:10<38:25,  3.88s/it][INFO|2025-12-04 19:09:19] llamafactory.train.callbacks:143 >> {'loss': 0.5764, 'learning_rate': 9.8835e-05, 'epoch': 0.21, 'throughput': 696.40}\n",
        "{'loss': 0.5764, 'grad_norm': 0.2545849680900574, 'learning_rate': 9.883466940978252e-05, 'epoch': 0.21, 'num_input_tokens_seen': 132576, 'train_runtime': 190.3782, 'train_tokens_per_second': 696.382}\n",
        "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 50/639 [03:29<39:13,  4.00s/it][INFO|2025-12-04 19:09:39] llamafactory.train.callbacks:143 >> {'loss': 0.5287, 'learning_rate': 9.8556e-05, 'epoch': 0.24, 'throughput': 695.89}\n",
        "{'loss': 0.5287, 'grad_norm': 0.28507527709007263, 'learning_rate': 9.855612757141655e-05, 'epoch': 0.24, 'num_input_tokens_seen': 146112, 'train_runtime': 209.9695, 'train_tokens_per_second': 695.872}\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–Œ                                      | 55/639 [03:52<43:40,  4.49s/it][INFO|2025-12-04 19:10:02] llamafactory.train.callbacks:143 >> {'loss': 0.5662, 'learning_rate': 9.8248e-05, 'epoch': 0.26, 'throughput': 695.63}\n",
        "{'loss': 0.5662, 'grad_norm': 0.3804457485675812, 'learning_rate': 9.824824572614051e-05, 'epoch': 0.26, 'num_input_tokens_seen': 162048, 'train_runtime': 232.9568, 'train_tokens_per_second': 695.614}\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 60/639 [04:13<40:41,  4.22s/it][INFO|2025-12-04 19:10:23] llamafactory.train.callbacks:143 >> {'loss': 0.6070, 'learning_rate': 9.7911e-05, 'epoch': 0.28, 'throughput': 693.66}\n",
        "{'loss': 0.607, 'grad_norm': 0.2477499544620514, 'learning_rate': 9.791120991134904e-05, 'epoch': 0.28, 'num_input_tokens_seen': 176064, 'train_runtime': 253.8233, 'train_tokens_per_second': 693.648}\n",
        " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 65/639 [04:34<39:16,  4.11s/it][INFO|2025-12-04 19:10:43] llamafactory.train.callbacks:143 >> {'loss': 0.5673, 'learning_rate': 9.7545e-05, 'epoch': 0.31, 'throughput': 692.77}\n",
        "{'loss': 0.5673, 'grad_norm': 0.2974146902561188, 'learning_rate': 9.754522378070297e-05, 'epoch': 0.31, 'num_input_tokens_seen': 190008, 'train_runtime': 274.2796, 'train_tokens_per_second': 692.753}\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 70/639 [04:53<36:35,  3.86s/it][INFO|2025-12-04 19:11:02] llamafactory.train.callbacks:143 >> {'loss': 0.5041, 'learning_rate': 9.7151e-05, 'epoch': 0.33, 'throughput': 691.84}\n",
        "{'loss': 0.5041, 'grad_norm': 0.35126590728759766, 'learning_rate': 9.715050848107168e-05, 'epoch': 0.33, 'num_input_tokens_seen': 203000, 'train_runtime': 293.4279, 'train_tokens_per_second': 691.822}\n",
        " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 75/639 [05:15<39:34,  4.21s/it][INFO|2025-12-04 19:11:24] llamafactory.train.callbacks:143 >> {'loss': 0.5521, 'learning_rate': 9.6727e-05, 'epoch': 0.35, 'throughput': 691.33}\n",
        "{'loss': 0.5521, 'grad_norm': 0.9560633897781372, 'learning_rate': 9.67273025189053e-05, 'epoch': 0.35, 'num_input_tokens_seen': 217824, 'train_runtime': 315.0869, 'train_tokens_per_second': 691.314}\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 80/639 [05:35<39:50,  4.28s/it][INFO|2025-12-04 19:11:45] llamafactory.train.callbacks:143 >> {'loss': 0.5734, 'learning_rate': 9.6276e-05, 'epoch': 0.38, 'throughput': 690.69}\n",
        "{'loss': 0.5734, 'grad_norm': 0.5834539532661438, 'learning_rate': 9.627586161611732e-05, 'epoch': 0.38, 'num_input_tokens_seen': 231792, 'train_runtime': 335.5991, 'train_tokens_per_second': 690.681}\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 85/639 [05:56<39:50,  4.32s/it][INFO|2025-12-04 19:12:06] llamafactory.train.callbacks:143 >> {'loss': 0.5538, 'learning_rate': 9.5796e-05, 'epoch': 0.40, 'throughput': 689.84}\n",
        "{'loss': 0.5538, 'grad_norm': 0.30696722865104675, 'learning_rate': 9.57964585555648e-05, 'epoch': 0.4, 'num_input_tokens_seen': 245992, 'train_runtime': 356.6004, 'train_tokens_per_second': 689.825}\n",
        " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 90/639 [06:17<37:45,  4.13s/it][INFO|2025-12-04 19:12:27] llamafactory.train.callbacks:143 >> {'loss': 0.5719, 'learning_rate': 9.5289e-05, 'epoch': 0.42, 'throughput': 689.06}\n",
        "{'loss': 0.5719, 'grad_norm': 0.2754083275794983, 'learning_rate': 9.528938301621956e-05, 'epoch': 0.42, 'num_input_tokens_seen': 260376, 'train_runtime': 377.8787, 'train_tokens_per_second': 689.046}\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 95/639 [06:39<39:08,  4.32s/it][INFO|2025-12-04 19:12:48] llamafactory.train.callbacks:143 >> {'loss': 0.5814, 'learning_rate': 9.4755e-05, 'epoch': 0.45, 'throughput': 688.40}\n",
        "{'loss': 0.5814, 'grad_norm': 0.2978734076023102, 'learning_rate': 9.475494139812979e-05, 'epoch': 0.45, 'num_input_tokens_seen': 274824, 'train_runtime': 399.2287, 'train_tokens_per_second': 688.387}\n",
        " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 100/639 [07:00<38:17,  4.26s/it][INFO|2025-12-04 19:13:09] llamafactory.train.callbacks:143 >> {'loss': 0.6039, 'learning_rate': 9.4193e-05, 'epoch': 0.47, 'throughput': 688.69}\n",
        "{'loss': 0.6039, 'grad_norm': 0.5732434988021851, 'learning_rate': 9.419345663727805e-05, 'epoch': 0.47, 'num_input_tokens_seen': 289392, 'train_runtime': 420.2147, 'train_tokens_per_second': 688.677}\n",
        " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 100/639 [07:00<38:17,  4.26s/it][INFO|trainer.py:4327] 2025-12-04 19:13:09,755 >>\n",
        "***** Running Evaluation *****\n",
        "[INFO|trainer.py:4329] 2025-12-04 19:13:09,755 >>   Num examples = 150\n",
        "[INFO|trainer.py:4332] 2025-12-04 19:13:09,755 >>   Batch size = 1\n",
        "\n",
        "  0%|                                                   | 0/150 [00:00<?, ?it/s]\n",
        "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.62it/s]\n",
        "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.32it/s]\n",
        "  3%|â–ˆâ–                                         | 4/150 [00:00<00:37,  3.94it/s]\n",
        "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.72it/s]\n",
        "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.60it/s]\n",
        "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.51it/s]\n",
        "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.44it/s]\n",
        "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.55it/s]\n",
        "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.49it/s]\n",
        "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.45it/s]\n",
        "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.21it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.09it/s]\n",
        " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.17it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.96it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.07it/s]\n",
        " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:42,  3.14it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.20it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:42,  3.09it/s]\n",
        " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.41it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.71it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.40it/s]\n",
        " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.38it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.36it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.48it/s]\n",
        " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.49it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.13it/s]\n",
        " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.23it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.37it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.71it/s]\n",
        " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.24it/s]\n",
        " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.54it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.22it/s]\n",
        " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.10it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.18it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.28it/s]\n",
        " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.35it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.37it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\n",
        " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:14<00:31,  3.29it/s]\n",
        " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.28it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.42it/s]\n",
        " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.46it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.04it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.25it/s]\n",
        " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.35it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.46it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.13it/s]\n",
        " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.05it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.16it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.29it/s]\n",
        " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.31it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:26,  3.31it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.31it/s]\n",
        " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.30it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:24,  3.58it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.52it/s]\n",
        " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.90it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.02it/s]\n",
        " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.35it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.45it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.72it/s]\n",
        " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.01it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.79it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.98it/s]\n",
        " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:19,  3.76it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.30it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.62it/s]\n",
        " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.59it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.52it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.57it/s]\n",
        " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:18,  3.83it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.48it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.50it/s]\n",
        " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.27it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.36it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.02it/s]\n",
        " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.05it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\n",
        " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.01it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.94it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.90it/s]\n",
        " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.24it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.34it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.33it/s]\n",
        " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.14it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.20it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.23it/s]\n",
        " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.12it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.88it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\n",
        " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.39it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.17it/s]\n",
        " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.34it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.78it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.59it/s]\n",
        " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.58it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.21it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.25it/s]\n",
        " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.28it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.33it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.17it/s]\n",
        " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.33it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.63it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.35it/s]\n",
        " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.06it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:10,  3.19it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.24it/s]\n",
        " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.09it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.20it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.05it/s]\n",
        " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.23it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.36it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.47it/s]\n",
        " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:06,  3.43it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.20it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.35it/s]\n",
        " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.63it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.54it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.30it/s]\n",
        " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.16it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.30it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.36it/s]\n",
        " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.42it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.40it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.98it/s]\n",
        " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.19it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:41<00:03,  3.05it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.17it/s]\n",
        " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.96it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.06it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.15it/s]\n",
        " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.22it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.28it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.61it/s]\n",
        " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.09it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.21it/s]\n",
        "                                                                                \n",
        "{'eval_loss': 0.5516486763954163, 'eval_runtime': 45.2801, 'eval_samples_per_second': 3.313, 'eval_steps_per_second': 3.313, 'epoch': 0.47, 'num_input_tokens_seen': 289392}\n",
        " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 100/639 [07:45<38:17,  4.26s/it]\n",
        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:45<00:00,  3.98it/s]\n",
        "                                                                                [INFO|trainer.py:3993] 2025-12-04 19:13:55,032 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-100\n",
        "[INFO|configuration_utils.py:696] 2025-12-04 19:13:55,079 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
        "[INFO|configuration_utils.py:770] 2025-12-04 19:13:55,080 >> Model config Qwen3Config {\n",
        "  \"architectures\": [\n",
        "    \"Qwen3ForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"head_dim\": 128,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 12288,\n",
        "  \"max_position_embeddings\": 40960,\n",
        "  \"max_window_layers\": 36,\n",
        "  \"model_type\": \"qwen3\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 36,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 1000000,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.52.4\",\n",
        "  \"use_cache\": true,\n",
        "  \"use_sliding_window\": false,\n",
        "  \"vocab_size\": 151936\n",
        "}\n",
        "\n",
        "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:13:55,198 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-100/chat_template.jinja\n",
        "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:13:55,199 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-100/tokenizer_config.json\n",
        "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:13:55,199 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-100/special_tokens_map.json\n",
        " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 105/639 [08:06<1:05:46,  7.39s/it][INFO|2025-12-04 19:14:16] llamafactory.train.callbacks:143 >> {'loss': 0.5618, 'learning_rate': 9.3605e-05, 'epoch': 0.49, 'throughput': 623.96}\n",
        "{'loss': 0.5618, 'grad_norm': 0.3641050159931183, 'learning_rate': 9.360526801044752e-05, 'epoch': 0.49, 'num_input_tokens_seen': 303552, 'train_runtime': 486.5013, 'train_tokens_per_second': 623.949}\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 110/639 [08:25<39:32,  4.48s/it][INFO|2025-12-04 19:14:35] llamafactory.train.callbacks:143 >> {'loss': 0.5425, 'learning_rate': 9.2991e-05, 'epoch': 0.52, 'throughput': 625.38}\n",
        "{'loss': 0.5425, 'grad_norm': 0.48988020420074463, 'learning_rate': 9.299073093021405e-05, 'epoch': 0.52, 'num_input_tokens_seen': 316176, 'train_runtime': 505.5794, 'train_tokens_per_second': 625.374}\n",
        " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 115/639 [08:46<37:11,  4.26s/it][INFO|2025-12-04 19:14:55] llamafactory.train.callbacks:143 >> {'loss': 0.5570, 'learning_rate': 9.2350e-05, 'epoch': 0.54, 'throughput': 627.53}\n",
        "{'loss': 0.557, 'grad_norm': 0.3276370167732239, 'learning_rate': 9.235021673018849e-05, 'epoch': 0.54, 'num_input_tokens_seen': 330344, 'train_runtime': 526.4242, 'train_tokens_per_second': 627.524}\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 120/639 [09:08<39:27,  4.56s/it][INFO|2025-12-04 19:15:17] llamafactory.train.callbacks:143 >> {'loss': 0.5373, 'learning_rate': 9.1684e-05, 'epoch': 0.56, 'throughput': 630.29}\n",
        "{'loss': 0.5373, 'grad_norm': 0.2878979444503784, 'learning_rate': 9.168411244063863e-05, 'epoch': 0.56, 'num_input_tokens_seen': 345608, 'train_runtime': 548.3389, 'train_tokens_per_second': 630.282}\n",
        " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 125/639 [09:28<35:34,  4.15s/it][INFO|2025-12-04 19:15:38] llamafactory.train.callbacks:143 >> {'loss': 0.5990, 'learning_rate': 9.0993e-05, 'epoch': 0.59, 'throughput': 631.77}\n",
        "{'loss': 0.599, 'grad_norm': 0.47103503346443176, 'learning_rate': 9.09928205546263e-05, 'epoch': 0.59, 'num_input_tokens_seen': 359168, 'train_runtime': 568.5138, 'train_tokens_per_second': 631.767}\n",
        " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 130/639 [09:52<41:07,  4.85s/it][INFO|2025-12-04 19:16:02] llamafactory.train.callbacks:143 >> {'loss': 0.6046, 'learning_rate': 9.0277e-05, 'epoch': 0.61, 'throughput': 634.20}\n",
        "{'loss': 0.6046, 'grad_norm': 0.37291550636291504, 'learning_rate': 9.027675878480131e-05, 'epoch': 0.61, 'num_input_tokens_seen': 375824, 'train_runtime': 592.6039, 'train_tokens_per_second': 634.191}\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 135/639 [10:13<35:24,  4.22s/it][INFO|2025-12-04 19:16:22] llamafactory.train.callbacks:143 >> {'loss': 0.5704, 'learning_rate': 8.9536e-05, 'epoch': 0.64, 'throughput': 635.79}\n",
        "{'loss': 0.5704, 'grad_norm': 0.4764707386493683, 'learning_rate': 8.953635981099887e-05, 'epoch': 0.64, 'num_input_tokens_seen': 389832, 'train_runtime': 613.1557, 'train_tokens_per_second': 635.78}\n",
        " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 140/639 [10:34<35:26,  4.26s/it][INFO|2025-12-04 19:16:43] llamafactory.train.callbacks:143 >> {'loss': 0.5996, 'learning_rate': 8.8772e-05, 'epoch': 0.66, 'throughput': 637.31}\n",
        "{'loss': 0.5996, 'grad_norm': 0.41063907742500305, 'learning_rate': 8.877207101879302e-05, 'epoch': 0.66, 'num_input_tokens_seen': 404320, 'train_runtime': 634.4235, 'train_tokens_per_second': 637.303}\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 145/639 [10:55<35:56,  4.37s/it][INFO|2025-12-04 19:17:05] llamafactory.train.callbacks:143 >> {'loss': 0.5650, 'learning_rate': 8.7984e-05, 'epoch': 0.68, 'throughput': 639.12}\n",
        "{'loss': 0.565, 'grad_norm': 0.28647473454475403, 'learning_rate': 8.798435422916425e-05, 'epoch': 0.68, 'num_input_tokens_seen': 419128, 'train_runtime': 655.7903, 'train_tokens_per_second': 639.119}\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 150/639 [11:17<35:15,  4.33s/it][INFO|2025-12-04 19:17:27] llamafactory.train.callbacks:143 >> {'loss': 0.5706, 'learning_rate': 8.7174e-05, 'epoch': 0.71, 'throughput': 640.55}\n",
        "{'loss': 0.5706, 'grad_norm': 0.35413092374801636, 'learning_rate': 8.717368541944452e-05, 'epoch': 0.71, 'num_input_tokens_seen': 434248, 'train_runtime': 677.9333, 'train_tokens_per_second': 640.547}\n",
        " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 155/639 [11:39<34:42,  4.30s/it][INFO|2025-12-04 19:17:48] llamafactory.train.callbacks:143 >> {'loss': 0.5786, 'learning_rate': 8.6341e-05, 'epoch': 0.73, 'throughput': 642.12}\n",
        "{'loss': 0.5786, 'grad_norm': 0.3721427023410797, 'learning_rate': 8.634055443570826e-05, 'epoch': 0.73, 'num_input_tokens_seen': 449104, 'train_runtime': 699.4179, 'train_tokens_per_second': 642.111}\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 160/639 [12:00<33:18,  4.17s/it][INFO|2025-12-04 19:18:10] llamafactory.train.callbacks:143 >> {'loss': 0.5315, 'learning_rate': 8.5485e-05, 'epoch': 0.75, 'throughput': 643.01}\n",
        "{'loss': 0.5315, 'grad_norm': 0.43377748131752014, 'learning_rate': 8.548546469678311e-05, 'epoch': 0.75, 'num_input_tokens_seen': 463328, 'train_runtime': 720.5617, 'train_tokens_per_second': 643.009}\n",
        " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 165/639 [12:20<31:17,  3.96s/it][INFO|2025-12-04 19:18:29] llamafactory.train.callbacks:143 >> {'loss': 0.5618, 'learning_rate': 8.4609e-05, 'epoch': 0.78, 'throughput': 644.04}\n",
        "{'loss': 0.5618, 'grad_norm': 0.40435293316841125, 'learning_rate': 8.460893289005965e-05, 'epoch': 0.78, 'num_input_tokens_seen': 476752, 'train_runtime': 740.2637, 'train_tokens_per_second': 644.03}\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 170/639 [12:41<31:33,  4.04s/it][INFO|2025-12-04 19:18:50] llamafactory.train.callbacks:143 >> {'loss': 0.6172, 'learning_rate': 8.3711e-05, 'epoch': 0.80, 'throughput': 644.94}\n",
        "{'loss': 0.6172, 'grad_norm': 0.755048394203186, 'learning_rate': 8.371148865928319e-05, 'epoch': 0.8, 'num_input_tokens_seen': 490888, 'train_runtime': 761.1456, 'train_tokens_per_second': 644.933}\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 175/639 [13:01<30:56,  4.00s/it][INFO|2025-12-04 19:19:11] llamafactory.train.callbacks:143 >> {'loss': 0.5638, 'learning_rate': 8.2794e-05, 'epoch': 0.82, 'throughput': 645.91}\n",
        "{'loss': 0.5638, 'grad_norm': 0.42955467104911804, 'learning_rate': 8.279367428451702e-05, 'epoch': 0.82, 'num_input_tokens_seen': 504944, 'train_runtime': 781.7652, 'train_tokens_per_second': 645.902}\n",
        " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 180/639 [13:23<33:39,  4.40s/it][INFO|2025-12-04 19:19:32] llamafactory.train.callbacks:143 >> {'loss': 0.5859, 'learning_rate': 8.1856e-05, 'epoch': 0.85, 'throughput': 646.72}\n",
        "{'loss': 0.5859, 'grad_norm': 0.35513269901275635, 'learning_rate': 8.185604435447002e-05, 'epoch': 0.85, 'num_input_tokens_seen': 519432, 'train_runtime': 803.182, 'train_tokens_per_second': 646.718}\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 185/639 [13:43<31:32,  4.17s/it][INFO|2025-12-04 19:19:53] llamafactory.train.callbacks:143 >> {'loss': 0.5548, 'learning_rate': 8.0899e-05, 'epoch': 0.87, 'throughput': 647.38}\n",
        "{'loss': 0.5548, 'grad_norm': 0.4681668281555176, 'learning_rate': 8.089916543138681e-05, 'epoch': 0.87, 'num_input_tokens_seen': 533328, 'train_runtime': 823.8372, 'train_tokens_per_second': 647.371}\n",
        " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 190/639 [14:04<31:26,  4.20s/it][INFO|2025-12-04 19:20:14] llamafactory.train.callbacks:143 >> {'loss': 0.5770, 'learning_rate': 7.9924e-05, 'epoch': 0.89, 'throughput': 648.45}\n",
        "{'loss': 0.577, 'grad_norm': 0.4571211636066437, 'learning_rate': 7.992361570870288e-05, 'epoch': 0.89, 'num_input_tokens_seen': 547712, 'train_runtime': 844.6484, 'train_tokens_per_second': 648.45}\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 195/639 [14:25<31:07,  4.21s/it][INFO|2025-12-04 19:20:35] llamafactory.train.callbacks:143 >> {'loss': 0.5777, 'learning_rate': 7.8930e-05, 'epoch': 0.92, 'throughput': 649.10}\n",
        "{'loss': 0.5777, 'grad_norm': 0.34551718831062317, 'learning_rate': 7.892998466167165e-05, 'epoch': 0.92, 'num_input_tokens_seen': 562024, 'train_runtime': 865.8566, 'train_tokens_per_second': 649.096}\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 200/639 [14:46<30:36,  4.18s/it][INFO|2025-12-04 19:20:56] llamafactory.train.callbacks:143 >> {'loss': 0.6595, 'learning_rate': 7.7919e-05, 'epoch': 0.94, 'throughput': 649.77}\n",
        "{'loss': 0.6595, 'grad_norm': 0.4461055099964142, 'learning_rate': 7.791887269117442e-05, 'epoch': 0.94, 'num_input_tokens_seen': 576232, 'train_runtime': 886.8286, 'train_tokens_per_second': 649.767}\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 200/639 [14:46<30:36,  4.18s/it][INFO|trainer.py:4327] 2025-12-04 19:20:56,369 >>\n",
        "***** Running Evaluation *****\n",
        "[INFO|trainer.py:4329] 2025-12-04 19:20:56,369 >>   Num examples = 150\n",
        "[INFO|trainer.py:4332] 2025-12-04 19:20:56,369 >>   Batch size = 1\n",
        "\n",
        "  0%|                                                   | 0/150 [00:00<?, ?it/s]\n",
        "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.63it/s]\n",
        "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.33it/s]\n",
        "  3%|â–ˆâ–                                         | 4/150 [00:00<00:37,  3.94it/s]\n",
        "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.72it/s]\n",
        "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.60it/s]\n",
        "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.52it/s]\n",
        "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.44it/s]\n",
        "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.55it/s]\n",
        "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.49it/s]\n",
        "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.46it/s]\n",
        "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.22it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.10it/s]\n",
        " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.18it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.98it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.09it/s]\n",
        " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:41,  3.15it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.22it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:41,  3.10it/s]\n",
        " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.42it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.72it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.39it/s]\n",
        " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.39it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.37it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.49it/s]\n",
        " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.49it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.14it/s]\n",
        " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.23it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.38it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.72it/s]\n",
        " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.24it/s]\n",
        " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.55it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.22it/s]\n",
        " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.10it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.18it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.29it/s]\n",
        " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.36it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.38it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\n",
        " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:31,  3.30it/s]\n",
        " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.28it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.42it/s]\n",
        " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.46it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.04it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.25it/s]\n",
        " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.34it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.44it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.13it/s]\n",
        " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.05it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.15it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.29it/s]\n",
        " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.31it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:26,  3.30it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.31it/s]\n",
        " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.31it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:24,  3.58it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.52it/s]\n",
        " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.91it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.03it/s]\n",
        " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.35it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.46it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.72it/s]\n",
        " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.00it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.79it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.97it/s]\n",
        " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:20,  3.75it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.28it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.61it/s]\n",
        " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.59it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.53it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.58it/s]\n",
        " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:18,  3.83it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.48it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.49it/s]\n",
        " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.27it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.35it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.01it/s]\n",
        " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.05it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\n",
        " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.01it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.94it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.91it/s]\n",
        " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.25it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.35it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.34it/s]\n",
        " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.15it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.21it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.24it/s]\n",
        " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.12it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.88it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\n",
        " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.40it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.16it/s]\n",
        " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.34it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.77it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.59it/s]\n",
        " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.58it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.19it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.22it/s]\n",
        " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.27it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.31it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.15it/s]\n",
        " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.32it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.63it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.35it/s]\n",
        " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.05it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:10,  3.18it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.23it/s]\n",
        " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.08it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.20it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.04it/s]\n",
        " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.22it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.37it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.46it/s]\n",
        " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:07,  3.43it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.20it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.35it/s]\n",
        " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.63it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.54it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.31it/s]\n",
        " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.16it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.31it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.37it/s]\n",
        " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.43it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.42it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.99it/s]\n",
        " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.20it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:41<00:03,  3.05it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.16it/s]\n",
        " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.96it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.05it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.15it/s]\n",
        " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.21it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.28it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.60it/s]\n",
        " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.09it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.21it/s]\n",
        "                                                                                \n",
        "{'eval_loss': 0.5446581840515137, 'eval_runtime': 45.2784, 'eval_samples_per_second': 3.313, 'eval_steps_per_second': 3.313, 'epoch': 0.94, 'num_input_tokens_seen': 576232}\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 200/639 [15:32<30:36,  4.18s/it]\n",
        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:45<00:00,  3.98it/s]\n",
        "                                                                                [INFO|trainer.py:3993] 2025-12-04 19:21:41,644 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-200\n",
        "[INFO|configuration_utils.py:696] 2025-12-04 19:21:41,692 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
        "[INFO|configuration_utils.py:770] 2025-12-04 19:21:41,693 >> Model config Qwen3Config {\n",
        "  \"architectures\": [\n",
        "    \"Qwen3ForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"head_dim\": 128,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 12288,\n",
        "  \"max_position_embeddings\": 40960,\n",
        "  \"max_window_layers\": 36,\n",
        "  \"model_type\": \"qwen3\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 36,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 1000000,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.52.4\",\n",
        "  \"use_cache\": true,\n",
        "  \"use_sliding_window\": false,\n",
        "  \"vocab_size\": 151936\n",
        "}\n",
        "\n",
        "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:21:41,777 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-200/chat_template.jinja\n",
        "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:21:41,778 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-200/tokenizer_config.json\n",
        "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:21:41,778 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-200/special_tokens_map.json\n",
        " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 205/639 [15:53<53:19,  7.37s/it][INFO|2025-12-04 19:22:03] llamafactory.train.callbacks:143 >> {'loss': 0.5866, 'learning_rate': 7.6891e-05, 'epoch': 0.96, 'throughput': 619.48}\n",
        "{'loss': 0.5866, 'grad_norm': 0.5498479604721069, 'learning_rate': 7.68908907609285e-05, 'epoch': 0.96, 'num_input_tokens_seen': 590944, 'train_runtime': 953.943, 'train_tokens_per_second': 619.475}\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 210/639 [16:15<34:05,  4.77s/it][INFO|2025-12-04 19:22:24] llamafactory.train.callbacks:143 >> {'loss': 0.5886, 'learning_rate': 7.5847e-05, 'epoch': 0.99, 'throughput': 620.87}\n",
        "{'loss': 0.5886, 'grad_norm': 0.42110908031463623, 'learning_rate': 7.584666002831296e-05, 'epoch': 0.99, 'num_input_tokens_seen': 605408, 'train_runtime': 975.0987, 'train_tokens_per_second': 620.868}\n",
        " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 215/639 [16:33<27:56,  3.95s/it][INFO|2025-12-04 19:22:42] llamafactory.train.callbacks:143 >> {'loss': 0.4728, 'learning_rate': 7.4787e-05, 'epoch': 1.01, 'throughput': 621.69}\n",
        "{'loss': 0.4728, 'grad_norm': 0.49210885167121887, 'learning_rate': 7.478681146903448e-05, 'epoch': 1.01, 'num_input_tokens_seen': 617520, 'train_runtime': 993.292, 'train_tokens_per_second': 621.69}\n",
        " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 220/639 [16:55<30:12,  4.33s/it][INFO|2025-12-04 19:23:05] llamafactory.train.callbacks:143 >> {'loss': 0.4468, 'learning_rate': 7.3712e-05, 'epoch': 1.03, 'throughput': 623.23}\n",
        "{'loss': 0.4468, 'grad_norm': 0.36256247758865356, 'learning_rate': 7.371198549586091e-05, 'epoch': 1.03, 'num_input_tokens_seen': 633016, 'train_runtime': 1015.7033, 'train_tokens_per_second': 623.229}\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 225/639 [17:17<30:13,  4.38s/it][INFO|2025-12-04 19:23:27] llamafactory.train.callbacks:143 >> {'loss': 0.3993, 'learning_rate': 7.2623e-05, 'epoch': 1.06, 'throughput': 624.65}\n",
        "{'loss': 0.3993, 'grad_norm': 0.43740350008010864, 'learning_rate': 7.262283157165219e-05, 'epoch': 1.06, 'num_input_tokens_seen': 648120, 'train_runtime': 1037.5816, 'train_tokens_per_second': 624.645}\n",
        " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 230/639 [17:38<28:09,  4.13s/it][INFO|2025-12-04 19:23:47] llamafactory.train.callbacks:143 >> {'loss': 0.4697, 'learning_rate': 7.1520e-05, 'epoch': 1.08, 'throughput': 625.75}\n",
        "{'loss': 0.4697, 'grad_norm': 0.402011513710022, 'learning_rate': 7.152000781692286e-05, 'epoch': 1.08, 'num_input_tokens_seen': 662304, 'train_runtime': 1058.4262, 'train_tokens_per_second': 625.744}\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 235/639 [18:00<30:19,  4.50s/it][INFO|2025-12-04 19:24:10] llamafactory.train.callbacks:143 >> {'loss': 0.4310, 'learning_rate': 7.0404e-05, 'epoch': 1.10, 'throughput': 627.18}\n",
        "{'loss': 0.431, 'grad_norm': 0.4554682970046997, 'learning_rate': 7.040418061217325e-05, 'epoch': 1.1, 'num_input_tokens_seen': 677896, 'train_runtime': 1080.8739, 'train_tokens_per_second': 627.174}\n",
        " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 240/639 [18:22<28:59,  4.36s/it][INFO|2025-12-04 19:24:31] llamafactory.train.callbacks:143 >> {'loss': 0.4295, 'learning_rate': 6.9276e-05, 'epoch': 1.13, 'throughput': 628.28}\n",
        "{'loss': 0.4295, 'grad_norm': 0.4741499423980713, 'learning_rate': 6.927602419522947e-05, 'epoch': 1.13, 'num_input_tokens_seen': 692648, 'train_runtime': 1102.4599, 'train_tokens_per_second': 628.275}\n",
        " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 245/639 [18:42<26:38,  4.06s/it][INFO|2025-12-04 19:24:52] llamafactory.train.callbacks:143 >> {'loss': 0.4233, 'learning_rate': 6.8136e-05, 'epoch': 1.15, 'throughput': 629.20}\n",
        "{'loss': 0.4233, 'grad_norm': 0.4026746451854706, 'learning_rate': 6.813622025383565e-05, 'epoch': 1.15, 'num_input_tokens_seen': 706472, 'train_runtime': 1122.8107, 'train_tokens_per_second': 629.2}\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 250/639 [19:02<25:34,  3.95s/it][INFO|2025-12-04 19:25:12] llamafactory.train.callbacks:143 >> {'loss': 0.3867, 'learning_rate': 6.6985e-05, 'epoch': 1.17, 'throughput': 630.00}\n",
        "{'loss': 0.3867, 'grad_norm': 0.48627060651779175, 'learning_rate': 6.698545751374465e-05, 'epoch': 1.17, 'num_input_tokens_seen': 719920, 'train_runtime': 1142.7429, 'train_tokens_per_second': 629.993}\n",
        " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 255/639 [19:22<24:42,  3.86s/it][INFO|2025-12-04 19:25:31] llamafactory.train.callbacks:143 >> {'loss': 0.3709, 'learning_rate': 6.5824e-05, 'epoch': 1.20, 'throughput': 630.53}\n",
        "{'loss': 0.3709, 'grad_norm': 0.7616415023803711, 'learning_rate': 6.582443132255592e-05, 'epoch': 1.2, 'num_input_tokens_seen': 732888, 'train_runtime': 1162.3342, 'train_tokens_per_second': 630.531}\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 260/639 [19:42<24:54,  3.94s/it][INFO|2025-12-04 19:25:52] llamafactory.train.callbacks:143 >> {'loss': 0.3777, 'learning_rate': 6.4654e-05, 'epoch': 1.22, 'throughput': 631.43}\n",
        "{'loss': 0.3777, 'grad_norm': 0.6388319730758667, 'learning_rate': 6.465384322955224e-05, 'epoch': 1.22, 'num_input_tokens_seen': 746904, 'train_runtime': 1182.8759, 'train_tokens_per_second': 631.431}\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 265/639 [20:04<26:48,  4.30s/it][INFO|2025-12-04 19:26:14] llamafactory.train.callbacks:143 >> {'loss': 0.3592, 'learning_rate': 6.3474e-05, 'epoch': 1.24, 'throughput': 632.61}\n",
        "{'loss': 0.3592, 'grad_norm': 0.4034753441810608, 'learning_rate': 6.347440056178904e-05, 'epoch': 1.24, 'num_input_tokens_seen': 762040, 'train_runtime': 1204.6071, 'train_tokens_per_second': 632.605}\n",
        " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 270/639 [20:26<26:59,  4.39s/it][INFO|2025-12-04 19:26:35] llamafactory.train.callbacks:143 >> {'loss': 0.4180, 'learning_rate': 6.2287e-05, 'epoch': 1.27, 'throughput': 633.40}\n",
        "{'loss': 0.418, 'grad_norm': 0.383634477853775, 'learning_rate': 6.228681599669248e-05, 'epoch': 1.27, 'num_input_tokens_seen': 776704, 'train_runtime': 1226.251, 'train_tokens_per_second': 633.397}\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 275/639 [20:45<23:33,  3.88s/it][INFO|2025-12-04 19:26:55] llamafactory.train.callbacks:143 >> {'loss': 0.3786, 'learning_rate': 6.1092e-05, 'epoch': 1.29, 'throughput': 634.00}\n",
        "{'loss': 0.3786, 'grad_norm': 0.5070011019706726, 'learning_rate': 6.109180713142465e-05, 'epoch': 1.29, 'num_input_tokens_seen': 789840, 'train_runtime': 1245.8012, 'train_tokens_per_second': 634.002}\n",
        " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 280/639 [21:06<24:55,  4.16s/it][INFO|2025-12-04 19:27:16] llamafactory.train.callbacks:143 >> {'loss': 0.3856, 'learning_rate': 5.9890e-05, 'epoch': 1.32, 'throughput': 635.08}\n",
        "{'loss': 0.3856, 'grad_norm': 0.4170735478401184, 'learning_rate': 5.989009604927587e-05, 'epoch': 1.32, 'num_input_tokens_seen': 804600, 'train_runtime': 1266.9238, 'train_tokens_per_second': 635.082}\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 285/639 [21:28<24:45,  4.20s/it][INFO|2025-12-04 19:27:37] llamafactory.train.callbacks:143 >> {'loss': 0.3766, 'learning_rate': 5.8682e-05, 'epoch': 1.34, 'throughput': 635.84}\n",
        "{'loss': 0.3766, 'grad_norm': 0.5109259486198425, 'learning_rate': 5.868240888334653e-05, 'epoch': 1.34, 'num_input_tokens_seen': 818976, 'train_runtime': 1288.0334, 'train_tokens_per_second': 635.834}\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 290/639 [21:50<26:38,  4.58s/it][INFO|2025-12-04 19:28:00] llamafactory.train.callbacks:143 >> {'loss': 0.4212, 'learning_rate': 5.7469e-05, 'epoch': 1.36, 'throughput': 636.65}\n",
        "{'loss': 0.4212, 'grad_norm': 0.4675813913345337, 'learning_rate': 5.74694753777815e-05, 'epoch': 1.36, 'num_input_tokens_seen': 834520, 'train_runtime': 1310.8044, 'train_tokens_per_second': 636.647}\n",
        " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 295/639 [22:12<25:00,  4.36s/it][INFO|2025-12-04 19:28:22] llamafactory.train.callbacks:143 >> {'loss': 0.4031, 'learning_rate': 5.6252e-05, 'epoch': 1.39, 'throughput': 637.57}\n",
        "{'loss': 0.4031, 'grad_norm': 0.4627811312675476, 'learning_rate': 5.62520284468228e-05, 'epoch': 1.39, 'num_input_tokens_seen': 849544, 'train_runtime': 1332.479, 'train_tokens_per_second': 637.567}\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 300/639 [22:34<25:17,  4.48s/it][INFO|2025-12-04 19:28:44] llamafactory.train.callbacks:143 >> {'loss': 0.4076, 'learning_rate': 5.5031e-05, 'epoch': 1.41, 'throughput': 638.29}\n",
        "{'loss': 0.4076, 'grad_norm': 0.4208119213581085, 'learning_rate': 5.5030803731946665e-05, 'epoch': 1.41, 'num_input_tokens_seen': 864576, 'train_runtime': 1354.5348, 'train_tokens_per_second': 638.283}\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 300/639 [22:34<25:17,  4.48s/it][INFO|trainer.py:4327] 2025-12-04 19:28:44,075 >>\n",
        "***** Running Evaluation *****\n",
        "[INFO|trainer.py:4329] 2025-12-04 19:28:44,075 >>   Num examples = 150\n",
        "[INFO|trainer.py:4332] 2025-12-04 19:28:44,075 >>   Batch size = 1\n",
        "\n",
        "  0%|                                                   | 0/150 [00:00<?, ?it/s]\n",
        "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.59it/s]\n",
        "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.30it/s]\n",
        "  3%|â–ˆâ–                                         | 4/150 [00:00<00:36,  3.96it/s]\n",
        "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.75it/s]\n",
        "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.62it/s]\n",
        "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.53it/s]\n",
        "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.44it/s]\n",
        "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.56it/s]\n",
        "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.49it/s]\n",
        "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.46it/s]\n",
        "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.22it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.11it/s]\n",
        " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.18it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.97it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.08it/s]\n",
        " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:42,  3.14it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.21it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:42,  3.09it/s]\n",
        " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.41it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.71it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.40it/s]\n",
        " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.39it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.36it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.48it/s]\n",
        " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.49it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.13it/s]\n",
        " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.24it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.38it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.72it/s]\n",
        " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.24it/s]\n",
        " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.54it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.23it/s]\n",
        " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.11it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.18it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.29it/s]\n",
        " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.35it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.37it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\n",
        " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:31,  3.30it/s]\n",
        " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.28it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.41it/s]\n",
        " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.47it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.05it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.25it/s]\n",
        " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.35it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.46it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.13it/s]\n",
        " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.05it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.16it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.29it/s]\n",
        " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.31it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:26,  3.31it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.31it/s]\n",
        " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.31it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:23,  3.59it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.53it/s]\n",
        " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.90it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.03it/s]\n",
        " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.36it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.47it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.72it/s]\n",
        " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.00it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.80it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.98it/s]\n",
        " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:19,  3.75it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.30it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.63it/s]\n",
        " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.60it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.53it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.59it/s]\n",
        " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:17,  3.84it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.48it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.49it/s]\n",
        " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.28it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.36it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.01it/s]\n",
        " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.04it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\n",
        " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.01it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.93it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.91it/s]\n",
        " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.25it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.35it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.34it/s]\n",
        " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.15it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.21it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.24it/s]\n",
        " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.13it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.89it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\n",
        " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.39it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.16it/s]\n",
        " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.33it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.78it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.60it/s]\n",
        " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.58it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.21it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.25it/s]\n",
        " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.29it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.33it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.17it/s]\n",
        " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.34it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.64it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.36it/s]\n",
        " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.05it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:10,  3.18it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.24it/s]\n",
        " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.09it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.20it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.05it/s]\n",
        " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.23it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.36it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.47it/s]\n",
        " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:06,  3.43it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.20it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.35it/s]\n",
        " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.62it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.54it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.31it/s]\n",
        " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.16it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.31it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.37it/s]\n",
        " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.43it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.42it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.99it/s]\n",
        " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.20it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:41<00:03,  3.05it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.17it/s]\n",
        " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.96it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.06it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.14it/s]\n",
        " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.21it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.28it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.60it/s]\n",
        " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.09it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.21it/s]\n",
        "                                                                                \n",
        "{'eval_loss': 0.5768356323242188, 'eval_runtime': 45.2451, 'eval_samples_per_second': 3.315, 'eval_steps_per_second': 3.315, 'epoch': 1.41, 'num_input_tokens_seen': 864576}\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 300/639 [23:19<25:17,  4.48s/it]\n",
        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:44<00:00,  3.97it/s]\n",
        "                                                                                [INFO|trainer.py:3993] 2025-12-04 19:29:29,317 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-300\n",
        "[INFO|configuration_utils.py:696] 2025-12-04 19:29:29,365 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
        "[INFO|configuration_utils.py:770] 2025-12-04 19:29:29,366 >> Model config Qwen3Config {\n",
        "  \"architectures\": [\n",
        "    \"Qwen3ForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"head_dim\": 128,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 12288,\n",
        "  \"max_position_embeddings\": 40960,\n",
        "  \"max_window_layers\": 36,\n",
        "  \"model_type\": \"qwen3\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 36,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 1000000,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.52.4\",\n",
        "  \"use_cache\": true,\n",
        "  \"use_sliding_window\": false,\n",
        "  \"vocab_size\": 151936\n",
        "}\n",
        "\n",
        "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:29:29,450 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-300/chat_template.jinja\n",
        "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:29:29,451 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-300/tokenizer_config.json\n",
        "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:29:29,451 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-300/special_tokens_map.json\n",
        " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 305/639 [23:41<42:16,  7.59s/it][INFO|2025-12-04 19:29:50] llamafactory.train.callbacks:143 >> {'loss': 0.3732, 'learning_rate': 5.3807e-05, 'epoch': 1.43, 'throughput': 618.50}\n",
        "{'loss': 0.3732, 'grad_norm': 0.5276778340339661, 'learning_rate': 5.380653915735272e-05, 'epoch': 1.43, 'num_input_tokens_seen': 879120, 'train_runtime': 1421.3896, 'train_tokens_per_second': 618.493}\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 310/639 [24:01<26:25,  4.82s/it][INFO|2025-12-04 19:30:11] llamafactory.train.callbacks:143 >> {'loss': 0.3614, 'learning_rate': 5.2580e-05, 'epoch': 1.46, 'throughput': 619.33}\n",
        "{'loss': 0.3614, 'grad_norm': 0.4933299124240875, 'learning_rate': 5.2579974484073655e-05, 'epoch': 1.46, 'num_input_tokens_seen': 892880, 'train_runtime': 1441.6816, 'train_tokens_per_second': 619.332}\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 315/639 [24:22<23:23,  4.33s/it][INFO|2025-12-04 19:30:32] llamafactory.train.callbacks:143 >> {'loss': 0.3661, 'learning_rate': 5.1352e-05, 'epoch': 1.48, 'throughput': 620.15}\n",
        "{'loss': 0.3661, 'grad_norm': 0.5306238532066345, 'learning_rate': 5.1351850862975315e-05, 'epoch': 1.48, 'num_input_tokens_seen': 906960, 'train_runtime': 1462.4961, 'train_tokens_per_second': 620.145}\n",
        " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 320/639 [24:41<20:42,  3.89s/it][INFO|2025-12-04 19:30:51] llamafactory.train.callbacks:143 >> {'loss': 0.3770, 'learning_rate': 5.0123e-05, 'epoch': 1.50, 'throughput': 620.90}\n",
        "{'loss': 0.377, 'grad_norm': 0.5807981491088867, 'learning_rate': 5.0122910386916656e-05, 'epoch': 1.5, 'num_input_tokens_seen': 920144, 'train_runtime': 1481.9612, 'train_tokens_per_second': 620.896}\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 325/639 [25:02<21:51,  4.18s/it][INFO|2025-12-04 19:31:12] llamafactory.train.callbacks:143 >> {'loss': 0.3885, 'learning_rate': 4.8894e-05, 'epoch': 1.53, 'throughput': 621.83}\n",
        "{'loss': 0.3885, 'grad_norm': 0.6108599901199341, 'learning_rate': 4.889389564234066e-05, 'epoch': 1.53, 'num_input_tokens_seen': 934536, 'train_runtime': 1502.8902, 'train_tokens_per_second': 621.826}\n",
        " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 330/639 [25:23<22:08,  4.30s/it][INFO|2025-12-04 19:31:33] llamafactory.train.callbacks:143 >> {'loss': 0.4047, 'learning_rate': 4.7666e-05, 'epoch': 1.55, 'throughput': 622.72}\n",
        "{'loss': 0.4047, 'grad_norm': 0.4433024525642395, 'learning_rate': 4.766554926056707e-05, 'epoch': 1.55, 'num_input_tokens_seen': 948784, 'train_runtime': 1523.6097, 'train_tokens_per_second': 622.721}\n",
        " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 335/639 [25:44<20:33,  4.06s/it][INFO|2025-12-04 19:31:53] llamafactory.train.callbacks:143 >> {'loss': 0.3847, 'learning_rate': 4.6439e-05, 'epoch': 1.57, 'throughput': 623.36}\n",
        "{'loss': 0.3847, 'grad_norm': 0.4564903974533081, 'learning_rate': 4.643861346905781e-05, 'epoch': 1.57, 'num_input_tokens_seen': 962552, 'train_runtime': 1544.1407, 'train_tokens_per_second': 623.358}\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 340/639 [26:05<20:51,  4.18s/it][INFO|2025-12-04 19:32:14] llamafactory.train.callbacks:143 >> {'loss': 0.4056, 'learning_rate': 4.5214e-05, 'epoch': 1.60, 'throughput': 624.08}\n",
        "{'loss': 0.4056, 'grad_norm': 0.5551087856292725, 'learning_rate': 4.521382964292663e-05, 'epoch': 1.6, 'num_input_tokens_seen': 976848, 'train_runtime': 1565.2702, 'train_tokens_per_second': 624.076}\n",
        " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 345/639 [26:28<22:02,  4.50s/it][INFO|2025-12-04 19:32:38] llamafactory.train.callbacks:143 >> {'loss': 0.4301, 'learning_rate': 4.3992e-05, 'epoch': 1.62, 'throughput': 624.96}\n",
        "{'loss': 0.4301, 'grad_norm': 0.40703701972961426, 'learning_rate': 4.399193785696366e-05, 'epoch': 1.62, 'num_input_tokens_seen': 992824, 'train_runtime': 1588.6224, 'train_tokens_per_second': 624.959}\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 350/639 [26:48<19:13,  3.99s/it][INFO|2025-12-04 19:32:57] llamafactory.train.callbacks:143 >> {'loss': 0.3942, 'learning_rate': 4.2774e-05, 'epoch': 1.64, 'throughput': 625.40}\n",
        "{'loss': 0.3942, 'grad_norm': 0.6319884657859802, 'learning_rate': 4.277367643844574e-05, 'epoch': 1.64, 'num_input_tokens_seen': 1005672, 'train_runtime': 1608.055, 'train_tokens_per_second': 625.397}\n",
        " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 355/639 [27:09<20:05,  4.24s/it][INFO|2025-12-04 19:33:18] llamafactory.train.callbacks:143 >> {'loss': 0.4638, 'learning_rate': 4.1560e-05, 'epoch': 1.67, 'throughput': 626.10}\n",
        "{'loss': 0.4638, 'grad_norm': 0.5046573281288147, 'learning_rate': 4.1559781521002664e-05, 'epoch': 1.67, 'num_input_tokens_seen': 1020032, 'train_runtime': 1629.198, 'train_tokens_per_second': 626.095}\n",
        " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 360/639 [27:29<18:43,  4.03s/it][INFO|2025-12-04 19:33:38] llamafactory.train.callbacks:143 >> {'loss': 0.3601, 'learning_rate': 4.0351e-05, 'epoch': 1.69, 'throughput': 626.74}\n",
        "{'loss': 0.3601, 'grad_norm': 0.44419023394584656, 'learning_rate': 4.035098659980891e-05, 'epoch': 1.69, 'num_input_tokens_seen': 1033576, 'train_runtime': 1649.1295, 'train_tokens_per_second': 626.74}\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 365/639 [27:51<19:06,  4.18s/it][INFO|2025-12-04 19:34:00] llamafactory.train.callbacks:143 >> {'loss': 0.4354, 'learning_rate': 3.9148e-05, 'epoch': 1.72, 'throughput': 627.66}\n",
        "{'loss': 0.4354, 'grad_norm': 0.5245075225830078, 'learning_rate': 3.914802208836973e-05, 'epoch': 1.72, 'num_input_tokens_seen': 1048952, 'train_runtime': 1671.2104, 'train_tokens_per_second': 627.66}\n",
        " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 370/639 [28:12<19:29,  4.35s/it][INFO|2025-12-04 19:34:22] llamafactory.train.callbacks:143 >> {'loss': 0.3863, 'learning_rate': 3.7952e-05, 'epoch': 1.74, 'throughput': 628.56}\n",
        "{'loss': 0.3863, 'grad_norm': 0.5564009547233582, 'learning_rate': 3.7951614877169284e-05, 'epoch': 1.74, 'num_input_tokens_seen': 1063992, 'train_runtime': 1692.7572, 'train_tokens_per_second': 628.556}\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 375/639 [28:35<20:16,  4.61s/it][INFO|2025-12-04 19:34:45] llamafactory.train.callbacks:143 >> {'loss': 0.3883, 'learning_rate': 3.6762e-05, 'epoch': 1.76, 'throughput': 629.44}\n",
        "{'loss': 0.3883, 'grad_norm': 0.3564496338367462, 'learning_rate': 3.67624878944475e-05, 'epoch': 1.76, 'num_input_tokens_seen': 1080056, 'train_runtime': 1715.9085, 'train_tokens_per_second': 629.437}\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 380/639 [28:57<19:05,  4.42s/it][INFO|2025-12-04 19:35:07] llamafactory.train.callbacks:143 >> {'loss': 0.4027, 'learning_rate': 3.5581e-05, 'epoch': 1.79, 'throughput': 630.13}\n",
        "{'loss': 0.4027, 'grad_norm': 0.5695515871047974, 'learning_rate': 3.558135966937123e-05, 'epoch': 1.79, 'num_input_tokens_seen': 1095056, 'train_runtime': 1737.8423, 'train_tokens_per_second': 630.124}\n",
        " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 385/639 [29:19<18:08,  4.29s/it][INFO|2025-12-04 19:35:28] llamafactory.train.callbacks:143 >> {'loss': 0.3612, 'learning_rate': 3.4409e-05, 'epoch': 1.81, 'throughput': 630.77}\n",
        "{'loss': 0.3612, 'grad_norm': 0.45643237233161926, 'learning_rate': 3.440894389786352e-05, 'epoch': 1.81, 'num_input_tokens_seen': 1109552, 'train_runtime': 1759.0607, 'train_tokens_per_second': 630.764}\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 390/639 [29:39<16:53,  4.07s/it][INFO|2025-12-04 19:35:49] llamafactory.train.callbacks:143 >> {'loss': 0.3676, 'learning_rate': 3.3246e-05, 'epoch': 1.83, 'throughput': 631.35}\n",
        "{'loss': 0.3676, 'grad_norm': 0.6372836828231812, 'learning_rate': 3.3245949011353264e-05, 'epoch': 1.83, 'num_input_tokens_seen': 1123480, 'train_runtime': 1779.4816, 'train_tokens_per_second': 631.352}\n",
        " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 395/639 [29:59<16:25,  4.04s/it][INFO|2025-12-04 19:36:09] llamafactory.train.callbacks:143 >> {'loss': 0.4137, 'learning_rate': 3.2093e-05, 'epoch': 1.86, 'throughput': 632.08}\n",
        "{'loss': 0.4137, 'grad_norm': 0.5252777934074402, 'learning_rate': 3.209307774870603e-05, 'epoch': 1.86, 'num_input_tokens_seen': 1137664, 'train_runtime': 1799.8748, 'train_tokens_per_second': 632.08}\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 400/639 [30:20<16:37,  4.17s/it][INFO|2025-12-04 19:36:30] llamafactory.train.callbacks:143 >> {'loss': 0.3426, 'learning_rate': 3.0951e-05, 'epoch': 1.88, 'throughput': 632.60}\n",
        "{'loss': 0.3426, 'grad_norm': 0.5461771488189697, 'learning_rate': 3.0951026731594635e-05, 'epoch': 1.88, 'num_input_tokens_seen': 1151792, 'train_runtime': 1820.7239, 'train_tokens_per_second': 632.601}\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 400/639 [30:20<16:37,  4.17s/it][INFO|trainer.py:4327] 2025-12-04 19:36:30,264 >>\n",
        "***** Running Evaluation *****\n",
        "[INFO|trainer.py:4329] 2025-12-04 19:36:30,264 >>   Num examples = 150\n",
        "[INFO|trainer.py:4332] 2025-12-04 19:36:30,264 >>   Batch size = 1\n",
        "\n",
        "  0%|                                                   | 0/150 [00:00<?, ?it/s]\n",
        "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.62it/s]\n",
        "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.30it/s]\n",
        "  3%|â–ˆâ–                                         | 4/150 [00:00<00:37,  3.93it/s]\n",
        "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.74it/s]\n",
        "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.61it/s]\n",
        "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.51it/s]\n",
        "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.43it/s]\n",
        "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.55it/s]\n",
        "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.49it/s]\n",
        "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.46it/s]\n",
        "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.22it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.11it/s]\n",
        " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.18it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.98it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.08it/s]\n",
        " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:42,  3.14it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.20it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:42,  3.09it/s]\n",
        " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.41it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.71it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.40it/s]\n",
        " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.38it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.36it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.47it/s]\n",
        " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.49it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.13it/s]\n",
        " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.23it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.37it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.71it/s]\n",
        " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.24it/s]\n",
        " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.54it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.23it/s]\n",
        " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.11it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.18it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.28it/s]\n",
        " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.35it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.37it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\n",
        " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:14<00:31,  3.29it/s]\n",
        " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.28it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.41it/s]\n",
        " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.47it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.05it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.25it/s]\n",
        " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.36it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.46it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.13it/s]\n",
        " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.05it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.16it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.29it/s]\n",
        " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.31it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:26,  3.31it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.31it/s]\n",
        " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.31it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:23,  3.59it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.53it/s]\n",
        " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.90it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.03it/s]\n",
        " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.36it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.47it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.73it/s]\n",
        " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.00it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.80it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.97it/s]\n",
        " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:19,  3.75it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.29it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.62it/s]\n",
        " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.59it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.53it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.59it/s]\n",
        " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:18,  3.83it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.48it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.50it/s]\n",
        " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.28it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.35it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.01it/s]\n",
        " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.05it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\n",
        " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.01it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.95it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.91it/s]\n",
        " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.24it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.34it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.34it/s]\n",
        " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.14it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.20it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.24it/s]\n",
        " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.12it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.88it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\n",
        " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.39it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.17it/s]\n",
        " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.34it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.78it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.60it/s]\n",
        " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.59it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.21it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.25it/s]\n",
        " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.29it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.33it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.17it/s]\n",
        " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.34it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.64it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.36it/s]\n",
        " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.07it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:09,  3.20it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.25it/s]\n",
        " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.10it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.21it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.06it/s]\n",
        " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.23it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.37it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.48it/s]\n",
        " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:06,  3.44it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.20it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.34it/s]\n",
        " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.63it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.55it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.32it/s]\n",
        " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.17it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.31it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.38it/s]\n",
        " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.43it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.41it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.99it/s]\n",
        " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.20it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:41<00:03,  3.06it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.18it/s]\n",
        " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.96it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.06it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.15it/s]\n",
        " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.21it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.28it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.61it/s]\n",
        " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.08it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.21it/s]\n",
        "                                                                                \n",
        "{'eval_loss': 0.582048237323761, 'eval_runtime': 45.2294, 'eval_samples_per_second': 3.316, 'eval_steps_per_second': 3.316, 'epoch': 1.88, 'num_input_tokens_seen': 1151792}\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 400/639 [31:05<16:37,  4.17s/it]\n",
        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:44<00:00,  3.98it/s]\n",
        "                                                                                [INFO|trainer.py:3993] 2025-12-04 19:37:15,490 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-400\n",
        "[INFO|configuration_utils.py:696] 2025-12-04 19:37:15,538 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
        "[INFO|configuration_utils.py:770] 2025-12-04 19:37:15,538 >> Model config Qwen3Config {\n",
        "  \"architectures\": [\n",
        "    \"Qwen3ForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"head_dim\": 128,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 12288,\n",
        "  \"max_position_embeddings\": 40960,\n",
        "  \"max_window_layers\": 36,\n",
        "  \"model_type\": \"qwen3\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 36,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 1000000,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.52.4\",\n",
        "  \"use_cache\": true,\n",
        "  \"use_sliding_window\": false,\n",
        "  \"vocab_size\": 151936\n",
        "}\n",
        "\n",
        "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:37:15,623 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-400/chat_template.jinja\n",
        "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:37:15,623 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-400/tokenizer_config.json\n",
        "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:37:15,623 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-400/special_tokens_map.json\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 405/639 [31:26<28:30,  7.31s/it][INFO|2025-12-04 19:37:35] llamafactory.train.callbacks:143 >> {'loss': 0.3577, 'learning_rate': 2.9820e-05, 'epoch': 1.90, 'throughput': 617.66}\n",
        "{'loss': 0.3577, 'grad_norm': 0.7871028184890747, 'learning_rate': 2.9820486043565854e-05, 'epoch': 1.9, 'num_input_tokens_seen': 1165048, 'train_runtime': 1886.2362, 'train_tokens_per_second': 617.658}\n",
        " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 410/639 [31:47<17:55,  4.70s/it][INFO|2025-12-04 19:37:56] llamafactory.train.callbacks:143 >> {'loss': 0.3717, 'learning_rate': 2.8702e-05, 'epoch': 1.93, 'throughput': 618.36}\n",
        "{'loss': 0.3717, 'grad_norm': 0.5074209570884705, 'learning_rate': 2.870213881305802e-05, 'epoch': 1.93, 'num_input_tokens_seen': 1179328, 'train_runtime': 1907.2045, 'train_tokens_per_second': 618.354}\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 415/639 [32:08<16:37,  4.45s/it][INFO|2025-12-04 19:38:17] llamafactory.train.callbacks:143 >> {'loss': 0.4490, 'learning_rate': 2.7597e-05, 'epoch': 1.95, 'throughput': 618.92}\n",
        "{'loss': 0.449, 'grad_norm': 0.47436830401420593, 'learning_rate': 2.7596660800621078e-05, 'epoch': 1.95, 'num_input_tokens_seen': 1193472, 'train_runtime': 1928.3147, 'train_tokens_per_second': 618.92}\n",
        " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 420/639 [32:28<15:05,  4.13s/it][INFO|2025-12-04 19:38:37] llamafactory.train.callbacks:143 >> {'loss': 0.3816, 'learning_rate': 2.6505e-05, 'epoch': 1.97, 'throughput': 619.43}\n",
        "{'loss': 0.3816, 'grad_norm': 0.5366332530975342, 'learning_rate': 2.650471999058875e-05, 'epoch': 1.97, 'num_input_tokens_seen': 1206920, 'train_runtime': 1948.436, 'train_tokens_per_second': 619.43}\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 425/639 [32:50<14:34,  4.09s/it][INFO|2025-12-04 19:38:59] llamafactory.train.callbacks:143 >> {'loss': 0.3983, 'learning_rate': 2.5427e-05, 'epoch': 2.00, 'throughput': 620.33}\n",
        "{'loss': 0.3983, 'grad_norm': 0.5117286443710327, 'learning_rate': 2.542697618744945e-05, 'epoch': 2.0, 'num_input_tokens_seen': 1222080, 'train_runtime': 1970.0385, 'train_tokens_per_second': 620.333}\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 430/639 [33:07<13:01,  3.74s/it][INFO|2025-12-04 19:39:17] llamafactory.train.callbacks:143 >> {'loss': 0.2464, 'learning_rate': 2.4364e-05, 'epoch': 2.02, 'throughput': 620.99}\n",
        "{'loss': 0.2464, 'grad_norm': 0.8380260467529297, 'learning_rate': 2.4364080617159886e-05, 'epoch': 2.02, 'num_input_tokens_seen': 1234488, 'train_runtime': 1987.9502, 'train_tokens_per_second': 620.985}\n",
        " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 435/639 [33:28<13:56,  4.10s/it][INFO|2025-12-04 19:39:38] llamafactory.train.callbacks:143 >> {'loss': 0.2237, 'learning_rate': 2.3317e-05, 'epoch': 2.04, 'throughput': 621.53}\n",
        "{'loss': 0.2237, 'grad_norm': 0.4559262990951538, 'learning_rate': 2.3316675533642214e-05, 'epoch': 2.04, 'num_input_tokens_seen': 1248328, 'train_runtime': 2008.4729, 'train_tokens_per_second': 621.531}\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 440/639 [33:50<14:28,  4.36s/it][INFO|2025-12-04 19:40:00] llamafactory.train.callbacks:143 >> {'loss': 0.2394, 'learning_rate': 2.2285e-05, 'epoch': 2.07, 'throughput': 622.10}\n",
        "{'loss': 0.2394, 'grad_norm': 0.5217824578285217, 'learning_rate': 2.22853938307025e-05, 'epoch': 2.07, 'num_input_tokens_seen': 1263224, 'train_runtime': 2030.5946, 'train_tokens_per_second': 622.096}\n",
        " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 445/639 [34:09<12:38,  3.91s/it][INFO|2025-12-04 19:40:18] llamafactory.train.callbacks:143 >> {'loss': 0.2701, 'learning_rate': 2.1271e-05, 'epoch': 2.09, 'throughput': 622.51}\n",
        "{'loss': 0.2701, 'grad_norm': 0.9445173144340515, 'learning_rate': 2.1270858659605158e-05, 'epoch': 2.09, 'num_input_tokens_seen': 1275792, 'train_runtime': 2049.4348, 'train_tokens_per_second': 622.509}\n",
        " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 450/639 [34:28<12:30,  3.97s/it][INFO|2025-12-04 19:40:38] llamafactory.train.callbacks:143 >> {'loss': 0.2105, 'learning_rate': 2.0274e-05, 'epoch': 2.11, 'throughput': 622.88}\n",
        "{'loss': 0.2105, 'grad_norm': 0.5436469912528992, 'learning_rate': 2.0273683052534175e-05, 'epoch': 2.11, 'num_input_tokens_seen': 1288592, 'train_runtime': 2068.766, 'train_tokens_per_second': 622.88}\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 455/639 [34:50<12:50,  4.19s/it][INFO|2025-12-04 19:40:59] llamafactory.train.callbacks:143 >> {'loss': 0.2344, 'learning_rate': 1.9294e-05, 'epoch': 2.14, 'throughput': 623.59}\n",
        "{'loss': 0.2344, 'grad_norm': 0.6321210265159607, 'learning_rate': 1.9294469552168813e-05, 'epoch': 2.14, 'num_input_tokens_seen': 1303376, 'train_runtime': 2090.1165, 'train_tokens_per_second': 623.59}\n",
        " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 460/639 [35:11<12:15,  4.11s/it][INFO|2025-12-04 19:41:20] llamafactory.train.callbacks:143 >> {'loss': 0.2572, 'learning_rate': 1.8334e-05, 'epoch': 2.16, 'throughput': 624.18}\n",
        "{'loss': 0.2572, 'grad_norm': 0.6212900876998901, 'learning_rate': 1.8333809847597642e-05, 'epoch': 2.16, 'num_input_tokens_seen': 1317856, 'train_runtime': 2111.3592, 'train_tokens_per_second': 624.174}\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 465/639 [35:32<12:09,  4.19s/it][INFO|2025-12-04 19:41:41] llamafactory.train.callbacks:143 >> {'loss': 0.2222, 'learning_rate': 1.7392e-05, 'epoch': 2.18, 'throughput': 624.64}\n",
        "{'loss': 0.2222, 'grad_norm': 0.8060994148254395, 'learning_rate': 1.739228441679081e-05, 'epoch': 2.18, 'num_input_tokens_seen': 1331792, 'train_runtime': 2132.1068, 'train_tokens_per_second': 624.637}\n",
        " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 470/639 [35:53<12:13,  4.34s/it][INFO|2025-12-04 19:42:03] llamafactory.train.callbacks:143 >> {'loss': 0.2205, 'learning_rate': 1.6470e-05, 'epoch': 2.21, 'throughput': 625.32}\n",
        "{'loss': 0.2205, 'grad_norm': 0.4554629921913147, 'learning_rate': 1.647046217584661e-05, 'epoch': 2.21, 'num_input_tokens_seen': 1346640, 'train_runtime': 2153.5175, 'train_tokens_per_second': 625.321}\n",
        " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 475/639 [36:13<11:07,  4.07s/it][INFO|2025-12-04 19:42:23] llamafactory.train.callbacks:143 >> {'loss': 0.2254, 'learning_rate': 1.5569e-05, 'epoch': 2.23, 'throughput': 625.93}\n",
        "{'loss': 0.2254, 'grad_norm': 0.6985599398612976, 'learning_rate': 1.556890013522428e-05, 'epoch': 2.23, 'num_input_tokens_seen': 1360720, 'train_runtime': 2173.9366, 'train_tokens_per_second': 625.924}\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 480/639 [36:36<11:25,  4.31s/it][INFO|2025-12-04 19:42:45] llamafactory.train.callbacks:143 >> {'loss': 0.2295, 'learning_rate': 1.4688e-05, 'epoch': 2.25, 'throughput': 626.58}\n",
        "{'loss': 0.2295, 'grad_norm': 0.6668505072593689, 'learning_rate': 1.4688143063170923e-05, 'epoch': 2.25, 'num_input_tokens_seen': 1375984, 'train_runtime': 2196.0303, 'train_tokens_per_second': 626.578}\n",
        " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 485/639 [36:57<10:36,  4.13s/it][INFO|2025-12-04 19:43:06] llamafactory.train.callbacks:143 >> {'loss': 0.2024, 'learning_rate': 1.3829e-05, 'epoch': 2.28, 'throughput': 627.21}\n",
        "{'loss': 0.2024, 'grad_norm': 0.5283108949661255, 'learning_rate': 1.3828723156545553e-05, 'epoch': 2.28, 'num_input_tokens_seen': 1390624, 'train_runtime': 2217.1625, 'train_tokens_per_second': 627.209}\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 490/639 [37:20<11:41,  4.71s/it][INFO|2025-12-04 19:43:30] llamafactory.train.callbacks:143 >> {'loss': 0.2335, 'learning_rate': 1.2991e-05, 'epoch': 2.30, 'throughput': 627.73}\n",
        "{'loss': 0.2335, 'grad_norm': 0.6252283453941345, 'learning_rate': 1.2991159719239582e-05, 'epoch': 2.3, 'num_input_tokens_seen': 1406672, 'train_runtime': 2240.8972, 'train_tokens_per_second': 627.727}\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 495/639 [37:42<10:32,  4.39s/it][INFO|2025-12-04 19:43:52] llamafactory.train.callbacks:143 >> {'loss': 0.2429, 'learning_rate': 1.2176e-05, 'epoch': 2.32, 'throughput': 628.42}\n",
        "{'loss': 0.2429, 'grad_norm': 0.7144733667373657, 'learning_rate': 1.2175958848387765e-05, 'epoch': 2.32, 'num_input_tokens_seen': 1421880, 'train_runtime': 2262.6184, 'train_tokens_per_second': 628.422}\n",
        " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 500/639 [38:04<10:01,  4.33s/it][INFO|2025-12-04 19:44:13] llamafactory.train.callbacks:143 >> {'loss': 0.2311, 'learning_rate': 1.1384e-05, 'epoch': 2.35, 'throughput': 629.07}\n",
        "{'loss': 0.2311, 'grad_norm': 0.7061449289321899, 'learning_rate': 1.1383613128559306e-05, 'epoch': 2.35, 'num_input_tokens_seen': 1437056, 'train_runtime': 2284.4098, 'train_tokens_per_second': 629.071}\n",
        " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 500/639 [38:04<10:01,  4.33s/it][INFO|trainer.py:4327] 2025-12-04 19:44:13,950 >>\n",
        "***** Running Evaluation *****\n",
        "[INFO|trainer.py:4329] 2025-12-04 19:44:13,950 >>   Num examples = 150\n",
        "[INFO|trainer.py:4332] 2025-12-04 19:44:13,950 >>   Batch size = 1\n",
        "\n",
        "  0%|                                                   | 0/150 [00:00<?, ?it/s]\n",
        "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.63it/s]\n",
        "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.33it/s]\n",
        "  3%|â–ˆâ–                                         | 4/150 [00:00<00:36,  3.95it/s]\n",
        "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.74it/s]\n",
        "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.61it/s]\n",
        "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.52it/s]\n",
        "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.44it/s]\n",
        "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.56it/s]\n",
        "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.50it/s]\n",
        "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.47it/s]\n",
        "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.22it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.11it/s]\n",
        " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.18it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.97it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.08it/s]\n",
        " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:41,  3.15it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.22it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:41,  3.10it/s]\n",
        " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.42it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.71it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.40it/s]\n",
        " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.38it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.37it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.48it/s]\n",
        " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.49it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.13it/s]\n",
        " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.23it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.38it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.71it/s]\n",
        " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.25it/s]\n",
        " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.55it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.22it/s]\n",
        " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.11it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.17it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.28it/s]\n",
        " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.35it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.37it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\n",
        " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:31,  3.29it/s]\n",
        " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.29it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.42it/s]\n",
        " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.47it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.04it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.25it/s]\n",
        " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.35it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.46it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.12it/s]\n",
        " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.05it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.15it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.29it/s]\n",
        " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.31it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:26,  3.31it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.31it/s]\n",
        " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.31it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:24,  3.58it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.52it/s]\n",
        " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.90it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.03it/s]\n",
        " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.35it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.45it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.72it/s]\n",
        " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.02it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.80it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.98it/s]\n",
        " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:19,  3.76it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.30it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.62it/s]\n",
        " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.58it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.52it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.57it/s]\n",
        " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:18,  3.82it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.48it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.49it/s]\n",
        " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.27it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.35it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.02it/s]\n",
        " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.04it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\n",
        " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.00it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.93it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.90it/s]\n",
        " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.24it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.34it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.33it/s]\n",
        " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.14it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.21it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.24it/s]\n",
        " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.12it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.88it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\n",
        " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.39it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.16it/s]\n",
        " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.33it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.77it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.59it/s]\n",
        " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.58it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.21it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.25it/s]\n",
        " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.28it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.32it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.17it/s]\n",
        " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.33it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.64it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.35it/s]\n",
        " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.06it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:10,  3.18it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.25it/s]\n",
        " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.09it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.20it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.05it/s]\n",
        " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.23it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.36it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.47it/s]\n",
        " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:06,  3.43it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.20it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.35it/s]\n",
        " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.63it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.55it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.30it/s]\n",
        " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.16it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.31it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.37it/s]\n",
        " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.43it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.41it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.99it/s]\n",
        " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.20it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:41<00:03,  3.05it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.17it/s]\n",
        " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.96it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.06it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.15it/s]\n",
        " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.21it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.28it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.60it/s]\n",
        " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.08it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.20it/s]\n",
        "                                                                                \n",
        "{'eval_loss': 0.6827391386032104, 'eval_runtime': 45.2589, 'eval_samples_per_second': 3.314, 'eval_steps_per_second': 3.314, 'epoch': 2.35, 'num_input_tokens_seen': 1437056}\n",
        " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 500/639 [38:49<10:01,  4.33s/it]\n",
        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:44<00:00,  3.98it/s]\n",
        "                                                                                [INFO|trainer.py:3993] 2025-12-04 19:44:59,205 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-500\n",
        "[INFO|configuration_utils.py:696] 2025-12-04 19:44:59,254 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
        "[INFO|configuration_utils.py:770] 2025-12-04 19:44:59,254 >> Model config Qwen3Config {\n",
        "  \"architectures\": [\n",
        "    \"Qwen3ForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"head_dim\": 128,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 12288,\n",
        "  \"max_position_embeddings\": 40960,\n",
        "  \"max_window_layers\": 36,\n",
        "  \"model_type\": \"qwen3\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 36,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 1000000,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.52.4\",\n",
        "  \"use_cache\": true,\n",
        "  \"use_sliding_window\": false,\n",
        "  \"vocab_size\": 151936\n",
        "}\n",
        "\n",
        "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:44:59,339 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-500/chat_template.jinja\n",
        "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:44:59,340 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-500/tokenizer_config.json\n",
        "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:44:59,340 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-500/special_tokens_map.json\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 505/639 [39:11<16:56,  7.59s/it][INFO|2025-12-04 19:45:20] llamafactory.train.callbacks:143 >> {'loss': 0.2051, 'learning_rate': 1.0615e-05, 'epoch': 2.37, 'throughput': 617.34}\n",
        "{'loss': 0.2051, 'grad_norm': 0.7082416415214539, 'learning_rate': 1.0614601334114099e-05, 'epoch': 2.37, 'num_input_tokens_seen': 1451560, 'train_runtime': 2351.3321, 'train_tokens_per_second': 617.335}\n",
        " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 510/639 [39:32<10:29,  4.88s/it][INFO|2025-12-04 19:45:42] llamafactory.train.callbacks:143 >> {'loss': 0.2161, 'learning_rate': 9.8694e-06, 'epoch': 2.40, 'throughput': 617.87}\n",
        "{'loss': 0.2161, 'grad_norm': 0.5055545568466187, 'learning_rate': 9.869388139903496e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1466016, 'train_runtime': 2372.7021, 'train_tokens_per_second': 617.868}\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 515/639 [39:51<08:21,  4.05s/it][INFO|2025-12-04 19:46:01] llamafactory.train.callbacks:143 >> {'loss': 0.1847, 'learning_rate': 9.1484e-06, 'epoch': 2.42, 'throughput': 618.32}\n",
        "{'loss': 0.1847, 'grad_norm': 0.6534410715103149, 'learning_rate': 9.148423840490954e-06, 'epoch': 2.42, 'num_input_tokens_seen': 1478944, 'train_runtime': 2391.883, 'train_tokens_per_second': 618.318}\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 520/639 [40:14<08:35,  4.33s/it][INFO|2025-12-04 19:46:23] llamafactory.train.callbacks:143 >> {'loss': 0.2416, 'learning_rate': 8.4521e-06, 'epoch': 2.44, 'throughput': 618.99}\n",
        "{'loss': 0.2416, 'grad_norm': 0.543830156326294, 'learning_rate': 8.452144078061818e-06, 'epoch': 2.44, 'num_input_tokens_seen': 1494488, 'train_runtime': 2414.415, 'train_tokens_per_second': 618.986}\n",
        " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 525/639 [40:35<08:14,  4.34s/it][INFO|2025-12-04 19:46:45] llamafactory.train.callbacks:143 >> {'loss': 0.2410, 'learning_rate': 7.7810e-06, 'epoch': 2.47, 'throughput': 619.55}\n",
        "{'loss': 0.241, 'grad_norm': 0.5543383359909058, 'learning_rate': 7.780969579186814e-06, 'epoch': 2.47, 'num_input_tokens_seen': 1509128, 'train_runtime': 2435.8485, 'train_tokens_per_second': 619.549}\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 530/639 [40:58<08:27,  4.66s/it][INFO|2025-12-04 19:47:07] llamafactory.train.callbacks:143 >> {'loss': 0.2784, 'learning_rate': 7.1353e-06, 'epoch': 2.49, 'throughput': 620.17}\n",
        "{'loss': 0.2784, 'grad_norm': 0.37712037563323975, 'learning_rate': 7.135305900598321e-06, 'epoch': 2.49, 'num_input_tokens_seen': 1524464, 'train_runtime': 2458.1382, 'train_tokens_per_second': 620.17}\n",
        " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 535/639 [41:21<07:54,  4.57s/it][INFO|2025-12-04 19:47:30] llamafactory.train.callbacks:143 >> {'loss': 0.2794, 'learning_rate': 6.5155e-06, 'epoch': 2.51, 'throughput': 620.91}\n",
        "{'loss': 0.2794, 'grad_norm': 0.7365447878837585, 'learning_rate': 6.515543184132999e-06, 'epoch': 2.51, 'num_input_tokens_seen': 1540688, 'train_runtime': 2481.3489, 'train_tokens_per_second': 620.907}\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 540/639 [41:41<06:39,  4.04s/it][INFO|2025-12-04 19:47:50] llamafactory.train.callbacks:143 >> {'loss': 0.2341, 'learning_rate': 5.9221e-06, 'epoch': 2.54, 'throughput': 621.39}\n",
        "{'loss': 0.2341, 'grad_norm': 0.5706935524940491, 'learning_rate': 5.922055920988817e-06, 'epoch': 2.54, 'num_input_tokens_seen': 1554264, 'train_runtime': 2501.2651, 'train_tokens_per_second': 621.391}\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 545/639 [42:02<06:35,  4.21s/it][INFO|2025-12-04 19:48:11] llamafactory.train.callbacks:143 >> {'loss': 0.2178, 'learning_rate': 5.3552e-06, 'epoch': 2.56, 'throughput': 621.88}\n",
        "{'loss': 0.2178, 'grad_norm': 0.7282590270042419, 'learning_rate': 5.355202725439046e-06, 'epoch': 2.56, 'num_input_tokens_seen': 1568584, 'train_runtime': 2522.3372, 'train_tokens_per_second': 621.877}\n",
        " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 550/639 [42:22<05:57,  4.02s/it][INFO|2025-12-04 19:48:32] llamafactory.train.callbacks:143 >> {'loss': 0.2050, 'learning_rate': 4.8153e-06, 'epoch': 2.58, 'throughput': 622.38}\n",
        "{'loss': 0.205, 'grad_norm': 0.8727412819862366, 'learning_rate': 4.8153261181398125e-06, 'epoch': 2.58, 'num_input_tokens_seen': 1582712, 'train_runtime': 2542.9866, 'train_tokens_per_second': 622.383}\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 555/639 [42:42<05:27,  3.90s/it][INFO|2025-12-04 19:48:52] llamafactory.train.callbacks:143 >> {'loss': 0.2161, 'learning_rate': 4.3028e-06, 'epoch': 2.61, 'throughput': 622.80}\n",
        "{'loss': 0.2161, 'grad_norm': 0.5880314111709595, 'learning_rate': 4.302752319162212e-06, 'epoch': 2.61, 'num_input_tokens_seen': 1596192, 'train_runtime': 2562.9466, 'train_tokens_per_second': 622.796}\n",
        " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 560/639 [43:04<05:49,  4.42s/it][INFO|2025-12-04 19:49:14] llamafactory.train.callbacks:143 >> {'loss': 0.2147, 'learning_rate': 3.8178e-06, 'epoch': 2.63, 'throughput': 623.33}\n",
        "{'loss': 0.2147, 'grad_norm': 0.4431954324245453, 'learning_rate': 3.81779105087407e-06, 'epoch': 2.63, 'num_input_tokens_seen': 1611088, 'train_runtime': 2584.6615, 'train_tokens_per_second': 623.326}\n",
        " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 565/639 [43:26<05:10,  4.20s/it][INFO|2025-12-04 19:49:35] llamafactory.train.callbacks:143 >> {'loss': 0.2420, 'learning_rate': 3.3607e-06, 'epoch': 2.65, 'throughput': 623.83}\n",
        "{'loss': 0.242, 'grad_norm': 0.5944216847419739, 'learning_rate': 3.3607353507904283e-06, 'epoch': 2.65, 'num_input_tokens_seen': 1625696, 'train_runtime': 2606.0146, 'train_tokens_per_second': 623.825}\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 570/639 [43:48<05:19,  4.64s/it][INFO|2025-12-04 19:49:58] llamafactory.train.callbacks:143 >> {'loss': 0.2359, 'learning_rate': 2.9319e-06, 'epoch': 2.68, 'throughput': 624.29}\n",
        "{'loss': 0.2359, 'grad_norm': 0.5675414204597473, 'learning_rate': 2.931861394505764e-06, 'epoch': 2.68, 'num_input_tokens_seen': 1641048, 'train_runtime': 2628.6622, 'train_tokens_per_second': 624.29}\n",
        " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 575/639 [44:10<04:48,  4.51s/it][INFO|2025-12-04 19:50:20] llamafactory.train.callbacks:143 >> {'loss': 0.2082, 'learning_rate': 2.5314e-06, 'epoch': 2.70, 'throughput': 624.80}\n",
        "{'loss': 0.2082, 'grad_norm': 0.6842307448387146, 'learning_rate': 2.531428328815155e-06, 'epoch': 2.7, 'num_input_tokens_seen': 1656160, 'train_runtime': 2650.7123, 'train_tokens_per_second': 624.798}\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 580/639 [44:30<03:56,  4.01s/it][INFO|2025-12-04 19:50:39] llamafactory.train.callbacks:143 >> {'loss': 0.2095, 'learning_rate': 2.1597e-06, 'epoch': 2.72, 'throughput': 625.15}\n",
        "{'loss': 0.2095, 'grad_norm': 0.8588398098945618, 'learning_rate': 2.1596781151249524e-06, 'epoch': 2.72, 'num_input_tokens_seen': 1669320, 'train_runtime': 2670.2655, 'train_tokens_per_second': 625.151}\n",
        " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 585/639 [44:51<03:37,  4.04s/it][INFO|2025-12-04 19:51:00] llamafactory.train.callbacks:143 >> {'loss': 0.2409, 'learning_rate': 1.8168e-06, 'epoch': 2.75, 'throughput': 625.59}\n",
        "{'loss': 0.2409, 'grad_norm': 0.777917206287384, 'learning_rate': 1.8168353832477947e-06, 'epoch': 2.75, 'num_input_tokens_seen': 1683616, 'train_runtime': 2691.2351, 'train_tokens_per_second': 625.592}\n",
        " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 590/639 [45:13<03:31,  4.32s/it][INFO|2025-12-04 19:51:22] llamafactory.train.callbacks:143 >> {'loss': 0.2346, 'learning_rate': 1.5031e-06, 'epoch': 2.77, 'throughput': 626.06}\n",
        "{'loss': 0.2346, 'grad_norm': 0.6048610210418701, 'learning_rate': 1.5031072956701697e-06, 'epoch': 2.77, 'num_input_tokens_seen': 1698728, 'train_runtime': 2713.3782, 'train_tokens_per_second': 626.056}\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 595/639 [45:32<02:51,  3.90s/it][INFO|2025-12-04 19:51:41] llamafactory.train.callbacks:143 >> {'loss': 0.2282, 'learning_rate': 1.2187e-06, 'epoch': 2.80, 'throughput': 626.36}\n",
        "{'loss': 0.2282, 'grad_norm': 0.5462923049926758, 'learning_rate': 1.2186834223746612e-06, 'epoch': 2.8, 'num_input_tokens_seen': 1711320, 'train_runtime': 2732.1752, 'train_tokens_per_second': 626.358}\n",
        " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 600/639 [45:54<02:50,  4.37s/it][INFO|2025-12-04 19:52:03] llamafactory.train.callbacks:143 >> {'loss': 0.2684, 'learning_rate': 9.6374e-07, 'epoch': 2.82, 'throughput': 626.84}\n",
        "{'loss': 0.2684, 'grad_norm': 0.5676615238189697, 'learning_rate': 9.637356262923725e-07, 'epoch': 2.82, 'num_input_tokens_seen': 1726344, 'train_runtime': 2754.0608, 'train_tokens_per_second': 626.836}\n",
        " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 600/639 [45:54<02:50,  4.37s/it][INFO|trainer.py:4327] 2025-12-04 19:52:03,601 >>\n",
        "***** Running Evaluation *****\n",
        "[INFO|trainer.py:4329] 2025-12-04 19:52:03,601 >>   Num examples = 150\n",
        "[INFO|trainer.py:4332] 2025-12-04 19:52:03,601 >>   Batch size = 1\n",
        "\n",
        "  0%|                                                   | 0/150 [00:00<?, ?it/s]\n",
        "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.60it/s]\n",
        "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.31it/s]\n",
        "  3%|â–ˆâ–                                         | 4/150 [00:00<00:36,  3.96it/s]\n",
        "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.73it/s]\n",
        "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.62it/s]\n",
        "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.52it/s]\n",
        "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.44it/s]\n",
        "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.56it/s]\n",
        "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.50it/s]\n",
        "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.46it/s]\n",
        "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.21it/s]\n",
        "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.10it/s]\n",
        " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.17it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.97it/s]\n",
        " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.08it/s]\n",
        " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:41,  3.15it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.21it/s]\n",
        " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:41,  3.10it/s]\n",
        " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.42it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.71it/s]\n",
        " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.39it/s]\n",
        " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.38it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.37it/s]\n",
        " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.48it/s]\n",
        " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.50it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\n",
        " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.13it/s]\n",
        " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.23it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.38it/s]\n",
        " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.72it/s]\n",
        " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\n",
        " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.24it/s]\n",
        " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.54it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\n",
        " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.21it/s]\n",
        " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.10it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.18it/s]\n",
        " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.29it/s]\n",
        " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.36it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.37it/s]\n",
        " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\n",
        " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\n",
        " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:31,  3.30it/s]\n",
        " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.28it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\n",
        " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.42it/s]\n",
        " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.46it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.04it/s]\n",
        " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.24it/s]\n",
        " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.33it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.43it/s]\n",
        " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.12it/s]\n",
        " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.03it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.15it/s]\n",
        " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.28it/s]\n",
        " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.30it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:27,  3.30it/s]\n",
        " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.30it/s]\n",
        " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.30it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:24,  3.57it/s]\n",
        " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.52it/s]\n",
        " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.90it/s]\n",
        " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.02it/s]\n",
        " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.35it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.45it/s]\n",
        " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.72it/s]\n",
        " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.01it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.79it/s]\n",
        " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.98it/s]\n",
        " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:19,  3.76it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.29it/s]\n",
        " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.60it/s]\n",
        " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.58it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.52it/s]\n",
        " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.57it/s]\n",
        " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:18,  3.83it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.49it/s]\n",
        " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.50it/s]\n",
        " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.27it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.36it/s]\n",
        " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.02it/s]\n",
        " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.05it/s]\n",
        " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\n",
        " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.00it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.93it/s]\n",
        " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.90it/s]\n",
        " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.24it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.33it/s]\n",
        " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.33it/s]\n",
        " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.14it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.21it/s]\n",
        " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.24it/s]\n",
        " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.12it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.88it/s]\n",
        " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\n",
        " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.39it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\n",
        " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.17it/s]\n",
        " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.33it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.78it/s]\n",
        " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.59it/s]\n",
        " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.58it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.20it/s]\n",
        " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.23it/s]\n",
        " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.27it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.31it/s]\n",
        " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.16it/s]\n",
        " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.32it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.63it/s]\n",
        " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.35it/s]\n",
        " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.05it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:10,  3.18it/s]\n",
        " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.25it/s]\n",
        " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.09it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.20it/s]\n",
        " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.06it/s]\n",
        " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.23it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.37it/s]\n",
        " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.47it/s]\n",
        " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:06,  3.43it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.19it/s]\n",
        " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.33it/s]\n",
        " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.63it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.55it/s]\n",
        " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.30it/s]\n",
        " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.16it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.31it/s]\n",
        " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.36it/s]\n",
        " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.41it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.41it/s]\n",
        " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.97it/s]\n",
        " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.19it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:42<00:03,  3.04it/s]\n",
        " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.16it/s]\n",
        " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.95it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.05it/s]\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.14it/s]\n",
        " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.21it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.27it/s]\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.60it/s]\n",
        " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.08it/s]\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.20it/s]\n",
        "                                                                                \n",
        "{'eval_loss': 0.6894978284835815, 'eval_runtime': 45.301, 'eval_samples_per_second': 3.311, 'eval_steps_per_second': 3.311, 'epoch': 2.82, 'num_input_tokens_seen': 1726344}\n",
        " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 600/639 [46:39<02:50,  4.37s/it]\n",
        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:45<00:00,  3.98it/s]\n",
        "                                                                                [INFO|trainer.py:3993] 2025-12-04 19:52:48,899 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-600\n",
        "[INFO|configuration_utils.py:696] 2025-12-04 19:52:48,948 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
        "[INFO|configuration_utils.py:770] 2025-12-04 19:52:48,949 >> Model config Qwen3Config {\n",
        "  \"architectures\": [\n",
        "    \"Qwen3ForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"head_dim\": 128,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 12288,\n",
        "  \"max_position_embeddings\": 40960,\n",
        "  \"max_window_layers\": 36,\n",
        "  \"model_type\": \"qwen3\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 36,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 1000000,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.52.4\",\n",
        "  \"use_cache\": true,\n",
        "  \"use_sliding_window\": false,\n",
        "  \"vocab_size\": 151936\n",
        "}\n",
        "\n",
        "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:52:49,046 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-600/chat_template.jinja\n",
        "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:52:49,047 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-600/tokenizer_config.json\n",
        "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:52:49,047 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-600/special_tokens_map.json\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 605/639 [47:00<04:06,  7.24s/it][INFO|2025-12-04 19:53:09] llamafactory.train.callbacks:143 >> {'loss': 0.2441, 'learning_rate': 7.3842e-07, 'epoch': 2.84, 'throughput': 617.14}\n",
        "{'loss': 0.2441, 'grad_norm': 0.8173639178276062, 'learning_rate': 7.384179594548957e-07, 'epoch': 2.84, 'num_input_tokens_seen': 1740472, 'train_runtime': 2820.2348, 'train_tokens_per_second': 617.137}\n",
        " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 610/639 [47:20<02:12,  4.56s/it][INFO|2025-12-04 19:53:30] llamafactory.train.callbacks:143 >> {'loss': 0.2168, 'learning_rate': 5.4287e-07, 'epoch': 2.87, 'throughput': 617.50}\n",
        "{'loss': 0.2168, 'grad_norm': 0.5683459639549255, 'learning_rate': 5.428665699084789e-07, 'epoch': 2.87, 'num_input_tokens_seen': 1754136, 'train_runtime': 2840.7246, 'train_tokens_per_second': 617.496}\n",
        " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 615/639 [47:42<01:46,  4.44s/it][INFO|2025-12-04 19:53:51] llamafactory.train.callbacks:143 >> {'loss': 0.2421, 'learning_rate': 3.7720e-07, 'epoch': 2.89, 'throughput': 618.01}\n",
        "{'loss': 0.2421, 'grad_norm': 0.5586677193641663, 'learning_rate': 3.7719961944664985e-07, 'epoch': 2.89, 'num_input_tokens_seen': 1768960, 'train_runtime': 2862.3719, 'train_tokens_per_second': 618.005}\n",
        " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 620/639 [48:02<01:16,  4.05s/it][INFO|2025-12-04 19:54:12] llamafactory.train.callbacks:143 >> {'loss': 0.2027, 'learning_rate': 2.4152e-07, 'epoch': 2.91, 'throughput': 618.39}\n",
        "{'loss': 0.2027, 'grad_norm': 0.7121002674102783, 'learning_rate': 2.415172122110343e-07, 'epoch': 2.91, 'num_input_tokens_seen': 1782728, 'train_runtime': 2882.8828, 'train_tokens_per_second': 618.384}\n",
        " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 625/639 [48:24<00:59,  4.24s/it][INFO|2025-12-04 19:54:34] llamafactory.train.callbacks:143 >> {'loss': 0.2514, 'learning_rate': 1.3590e-07, 'epoch': 2.94, 'throughput': 618.89}\n",
        "{'loss': 0.2514, 'grad_norm': 0.6133226752281189, 'learning_rate': 1.3590133420350315e-07, 'epoch': 2.94, 'num_input_tokens_seen': 1797592, 'train_runtime': 2904.5693, 'train_tokens_per_second': 618.884}\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 630/639 [48:45<00:38,  4.25s/it][INFO|2025-12-04 19:54:55] llamafactory.train.callbacks:143 >> {'loss': 0.2103, 'learning_rate': 6.0416e-08, 'epoch': 2.96, 'throughput': 619.38}\n",
        "{'loss': 0.2103, 'grad_norm': 0.6335474848747253, 'learning_rate': 6.041580374618328e-08, 'epoch': 2.96, 'num_input_tokens_seen': 1812104, 'train_runtime': 2925.6652, 'train_tokens_per_second': 619.382}\n",
        " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 635/639 [49:05<00:15,  3.93s/it][INFO|2025-12-04 19:55:14] llamafactory.train.callbacks:143 >> {'loss': 0.2202, 'learning_rate': 1.5106e-08, 'epoch': 2.98, 'throughput': 619.76}\n",
        "{'loss': 0.2202, 'grad_norm': 0.7536317110061646, 'learning_rate': 1.5106232919276375e-08, 'epoch': 2.98, 'num_input_tokens_seen': 1825320, 'train_runtime': 2945.211, 'train_tokens_per_second': 619.759}\n",
        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 639/639 [49:19<00:00,  3.48s/it][INFO|trainer.py:3993] 2025-12-04 19:55:29,244 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-639\n",
        "[INFO|configuration_utils.py:696] 2025-12-04 19:55:29,292 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
        "[INFO|configuration_utils.py:770] 2025-12-04 19:55:29,293 >> Model config Qwen3Config {\n",
        "  \"architectures\": [\n",
        "    \"Qwen3ForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"head_dim\": 128,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 12288,\n",
        "  \"max_position_embeddings\": 40960,\n",
        "  \"max_window_layers\": 36,\n",
        "  \"model_type\": \"qwen3\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 36,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 1000000,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.52.4\",\n",
        "  \"use_cache\": true,\n",
        "  \"use_sliding_window\": false,\n",
        "  \"vocab_size\": 151936\n",
        "}\n",
        "\n",
        "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:55:29,378 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-639/chat_template.jinja\n",
        "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:55:29,378 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-639/tokenizer_config.json\n",
        "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:55:29,378 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-639/special_tokens_map.json\n",
        "[INFO|trainer.py:2676] 2025-12-04 19:55:29,734 >>\n",
        "\n",
        "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
        "\n",
        "\n",
        "{'train_runtime': 2960.1995, 'train_samples_per_second': 0.861, 'train_steps_per_second': 0.216, 'train_loss': 0.40086729491074136, 'epoch': 3.0, 'num_input_tokens_seen': 1835184}\n",
        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 639/639 [49:20<00:00,  4.63s/it]\n",
        "[INFO|trainer.py:3993] 2025-12-04 19:55:29,736 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58\n",
        "[INFO|configuration_utils.py:696] 2025-12-04 19:55:29,785 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
        "[INFO|configuration_utils.py:770] 2025-12-04 19:55:29,786 >> Model config Qwen3Config {\n",
        "  \"architectures\": [\n",
        "    \"Qwen3ForCausalLM\"\n",
        "  ],\n",
        "  \"attention_bias\": false,\n",
        "  \"attention_dropout\": 0.0,\n",
        "  \"bos_token_id\": 151643,\n",
        "  \"eos_token_id\": 151645,\n",
        "  \"head_dim\": 128,\n",
        "  \"hidden_act\": \"silu\",\n",
        "  \"hidden_size\": 4096,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 12288,\n",
        "  \"max_position_embeddings\": 40960,\n",
        "  \"max_window_layers\": 36,\n",
        "  \"model_type\": \"qwen3\",\n",
        "  \"num_attention_heads\": 32,\n",
        "  \"num_hidden_layers\": 36,\n",
        "  \"num_key_value_heads\": 8,\n",
        "  \"rms_norm_eps\": 1e-06,\n",
        "  \"rope_scaling\": null,\n",
        "  \"rope_theta\": 1000000,\n",
        "  \"sliding_window\": null,\n",
        "  \"tie_word_embeddings\": false,\n",
        "  \"torch_dtype\": \"bfloat16\",\n",
        "  \"transformers_version\": \"4.52.4\",\n",
        "  \"use_cache\": true,\n",
        "  \"use_sliding_window\": false,\n",
        "  \"vocab_size\": 151936\n",
        "}\n",
        "\n",
        "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:55:29,870 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/chat_template.jinja\n",
        "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:55:29,871 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/tokenizer_config.json\n",
        "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:55:29,871 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/special_tokens_map.json\n",
        "***** train metrics *****\n",
        "  epoch                    =        3.0\n",
        "  num_input_tokens_seen    =    1835184\n",
        "  total_flos               = 77836961GF\n",
        "  train_loss               =     0.4009\n",
        "  train_runtime            = 0:49:20.19\n",
        "  train_samples_per_second =      0.861\n",
        "  train_steps_per_second   =      0.216\n",
        "Figure saved at: saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/training_loss.png\n",
        "Figure saved at: saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/training_eval_loss.png\n",
        "[WARNING|2025-12-04 19:55:30] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
        "[INFO|trainer.py:4327] 2025-12-04 19:55:30,222 >>\n",
        "***** Running Evaluation *****\n",
        "[INFO|trainer.py:4329] 2025-12-04 19:55:30,222 >>   Num examples = 150\n",
        "[INFO|trainer.py:4332] 2025-12-04 19:55:30,222 >>   Batch size = 1\n",
        "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:44<00:00,  3.34it/s]\n",
        "***** eval metrics *****\n",
        "  epoch                   =        3.0\n",
        "  eval_loss               =      0.688\n",
        "  eval_runtime            = 0:00:45.23\n",
        "  eval_samples_per_second =      3.316\n",
        "  eval_steps_per_second   =      3.316\n",
        "  num_input_tokens_seen   =    1835184\n",
        "[INFO|modelcard.py:450] 2025-12-04 19:56:15,453 >> Dropping the following result as it does not have all the necessary fields:\n",
        "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "#### 3.3.1 è®­ç»ƒæ•ˆç‡ä¸ç¨³å®šæ€§æ·±åº¦è§£æ (Efficiency & Stability)\n",
        "\n",
        "è¿™éƒ¨åˆ†ä¸»è¦è¯„ä¼°æ‚¨çš„ç¡¬ä»¶èµ„æºæ˜¯å¦è¢«å……åˆ†åˆ©ç”¨ï¼Œä»¥åŠè®­ç»ƒè¿‡ç¨‹æ˜¯å¦å¥åº·ã€‚\n",
        "\n",
        "- **ç¡¬ä»¶é€‚é…åº¦ï¼šæ»¡åˆ†**\n",
        "  - **æ˜¾å­˜çŠ¶æ€**ï¼šæ—¥å¿—å…¨ç§°æ—  OOM (Out Of Memory) æŠ¥é”™ï¼Œè¯´æ˜ `4-bit é‡åŒ–` + `æ¢¯åº¦æ£€æŸ¥ç‚¹` çš„ç­–ç•¥éå¸¸æˆåŠŸã€‚åœ¨ A10 (24G) æ˜¾å¡ä¸Šï¼Œæ˜¾å­˜å ç”¨è¢«ç‰¢ç‰¢æ§åˆ¶åœ¨å®‰å…¨çº¿ä»¥å†…ã€‚\n",
        "  - **è®¡ç®—ç²¾åº¦**ï¼šæ—¥å¿—æ˜¾ç¤º `compute dtype: torch.bfloat16`ã€‚A10 æ˜¾å¡åŸç”Ÿæ”¯æŒ `bf16`ï¼Œè¿™æ¯” `fp16` æ›´ç¨³å®šï¼Œèƒ½æœ‰æ•ˆé˜²æ­¢è®­ç»ƒä¸­å‡ºç° Loss=NaNï¼ˆæ•°å€¼æº¢å‡ºï¼‰çš„ç¾éš¾æ€§é”™è¯¯ã€‚\n",
        "- **è®­ç»ƒé€Ÿåº¦ï¼šæå¿«**\n",
        "  - **ååé‡**ï¼šç¨³å®šåœ¨ **620 - 630 tokens/s**ã€‚å¯¹äº 8B å‚æ•°é‡çš„æ¨¡å‹ï¼Œåœ¨å•å¡ä¸Šè¾¾åˆ°è¿™ä¸ªé€Ÿåº¦å±äº**ç¬¬ä¸€æ¢¯é˜Ÿ**çš„æ•ˆç‡ã€‚\n",
        "  - **æ ·æœ¬å¤„ç†**ï¼šçº¦ `0.86 samples/s`ã€‚è¿™æ„å‘³ç€å¤„ç†ä¸€æ¡å‡ åƒå­—çš„å¤æ‚åŒ»ç–—æ¨ç†æ•°æ®ï¼Œåªéœ€è¦ 1.2 ç§’å·¦å³ã€‚\n",
        "- **ç»“è®º**ï¼šæ‚¨çš„ç¯å¢ƒé…ç½®ï¼ˆPyTorch 2.6 + CUDA 12.4 + A10ï¼‰éå¸¸å¥å£®ï¼Œå®Œå…¨å¯ä»¥æ”¯æ’‘åç»­æ›´å¤§è§„æ¨¡çš„è®­ç»ƒã€‚\n",
        "\n",
        "#### 3.3.2 æŸå¤±å‡½æ•°ä¸è¿‡æ‹Ÿåˆæ·±åº¦è¯Šæ–­ (Loss Analysis) â€”â€” **æœ€å…³é”®çš„å‘ç°**\n",
        "> è¾“å‡ºç›®å½•ï¼ˆåœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºçš„ train_2025-12-04-19-04-58ï¼‰ï¼Œè·¯å¾„é€šå¸¸æ˜¯ï¼š saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/\n",
        "![image-20251204201640965](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042016031.png)\n",
        "![image-20251204201557168](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042015285.png)\n",
        "\n",
        "è¿™æ˜¯æ•´ä¸ªæ—¥å¿—ä¸­æœ€æœ‰ä»·å€¼çš„æ•°æ®ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬æ¨¡å‹åˆ°åº•å­¦å¾—æ€ä¹ˆæ ·ã€‚è¯·é‡ç‚¹å…³æ³¨ **Eval Loss (æ¨¡æ‹Ÿè€ƒè¯•æˆç»©)** çš„å˜åŒ–è½¨è¿¹ã€‚\n",
        "\n",
        "- **ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿå­¦ä¹ æœŸ (Step 0 - 200)**\n",
        "  - **è¡¨ç°**ï¼šTrain Loss è¿…é€Ÿä¸‹é™ï¼ŒEval Loss ä¹Ÿä» `0.5516` é™è‡³ **`0.5446`** (Step 200)ã€‚\n",
        "  - **è§£è¯»**ï¼šæ­¤æ—¶æ¨¡å‹æ­£åœ¨åƒæµ·ç»µä¸€æ ·å¸æ”¶çŸ¥è¯†ï¼Œå®ƒå­¦ä¼šäº†å¦‚ä½•æ¨¡ä»¿åŒ»ç”Ÿçš„è¯­æ°”ï¼Œä¹ŸæŒæ¡äº†åŸºæœ¬çš„æ¨ç†é€»è¾‘ã€‚\n",
        "  - **çŠ¶æ€**ï¼š**æœ€ä½³ (Optimal)**ã€‚\n",
        "- **ç¬¬äºŒé˜¶æ®µï¼šæ­»è®°ç¡¬èƒŒæœŸ (Step 200 - 400)**\n",
        "  - **è¡¨ç°**ï¼šTrain Loss ç»§ç»­ä¸‹é™ï¼ˆå¹³æ—¶ä½œä¸šå…¨å¯¹ï¼‰ï¼Œä½† Eval Loss å¼€å§‹åå¼¹è‡³ `0.57` - `0.60`ã€‚\n",
        "  - **è§£è¯»**ï¼šæ¨¡å‹å¼€å§‹ä¸ºäº†åˆ·ä½è®­ç»ƒé›†çš„ Lossï¼Œå»æ­»è®°ç¡¬èƒŒæŸäº›ç‰¹å®šçš„ç—…å†æè¿°ï¼Œè€Œä¸æ˜¯ç†è§£å…¶ä¸­çš„åŒ»ç†ã€‚å®ƒå¼€å§‹â€œåç§‘â€äº†ã€‚\n",
        "- **ç¬¬ä¸‰é˜¶æ®µï¼šä¸¥é‡è¿‡æ‹ŸåˆæœŸ (Step 400 - 639)**\n",
        "  - **è¡¨ç°**ï¼šTrain Loss é™åˆ°äº†æä½çš„ `0.21`ï¼Œä½† Eval Loss é£™å‡è‡³ **`0.688`**ï¼Œç”šè‡³æ¯”åˆšå¼€å§‹è®­ç»ƒæ—¶è¿˜è¦å·®ã€‚\n",
        "  - **è§£è¯»**ï¼šæ¨¡å‹å½»åº•â€œå­¦å‚»äº†â€ã€‚å¦‚æœä½ é—®å®ƒè®­ç»ƒé›†é‡Œä¸€æ¨¡ä¸€æ ·çš„é—®é¢˜ï¼Œå®ƒèƒ½ç­”å¾—å¾ˆå®Œç¾ï¼›ä½†å¦‚æœä½ é—®ä¸€ä¸ªæ–°çš„ç—…äººæƒ…å†µï¼Œå®ƒå¯èƒ½ä¼šèƒ¡è¨€ä¹±è¯­ï¼Œå› ä¸ºå®ƒçš„é€šç”¨èƒ½åŠ›è¢«ç ´åäº†ã€‚\n",
        "\n",
        "#### 3.3.3 å­¦ä¹ å†…å®¹ä¸æ•°æ®è´¨é‡éªŒè¯ (Data Quality Verification)\n",
        "\n",
        "æ—¥å¿—ä¸­æœ‰ä¸€æ®µæ•°æ®çš„ Input/Output é¢„è§ˆï¼Œè¿™è¯å®äº†æ‚¨çš„æ•°æ®æ¸…æ´—å·¥ä½œæ˜¯æˆåŠŸçš„ã€‚\n",
        "\n",
        "- **æ ‡ç­¾è¯†åˆ«**ï¼šæ—¥å¿—æ˜¾ç¤º `labels: <think>ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹...</think>`ã€‚\n",
        "- **æ·±åº¦è§£è¯»**ï¼š\n",
        "  - æ¨¡å‹ä¸ä»…å­¦åˆ°äº†â€œç­”æ¡ˆâ€ï¼Œè¿˜å­¦åˆ°äº†è¢« `<think>` æ ‡ç­¾åŒ…è£¹çš„**â€œéšå¼æ€ç»´é“¾â€**ã€‚\n",
        "  - è¿™æ„å‘³ç€åœ¨æ¨ç†æ—¶ï¼Œæ¨¡å‹ä¼šå…ˆåœ¨å†…éƒ¨è¿›è¡Œä¸€æ­¥æ­¥çš„é€»è¾‘æ¨å¯¼ï¼ˆæ¯”å¦‚å…ˆåˆ†æç—…å› ï¼Œå†å®šæ€§ï¼Œæœ€åç»™æ–¹æ¡ˆï¼‰ï¼Œè¿™æ­£æ˜¯ OpenAI o1 ç­‰æ¨ç†æ¨¡å‹çš„æ ¸å¿ƒç‰¹æ€§ã€‚\n",
        "\n",
        "#### 3.3.4 æœ€ä½³æ¨¡å‹é”å®š (Model Selection)\n",
        "\n",
        "è¿™æ˜¯æ–°æ‰‹æœ€å®¹æ˜“çŠ¯é”™çš„åœ°æ–¹ï¼š**åƒä¸‡ä¸è¦ä»¥ä¸ºè·‘å®Œæœ€åä¸€æ­¥çš„æ¨¡å‹å°±æ˜¯æœ€å¥½çš„ã€‚**\n",
        "\n",
        "- **å½“å‰é™·é˜±**ï¼šé»˜è®¤è¾“å‡ºçš„ `checkpoint-639` æ˜¯ä¸€ä¸ªè¿‡æ‹Ÿåˆçš„æ¨¡å‹ï¼Œæ³›åŒ–èƒ½åŠ›å¾ˆå·®ã€‚\n",
        "- **æ­£ç¡®é€‰æ‹©**ï¼šè¯·å›æº¯åˆ° **`checkpoint-200`**ã€‚\n",
        "- **æ“ä½œå»ºè®®**ï¼šåœ¨ WebUI ä¸­åŠ è½½æ¨¡å‹æ—¶ï¼Œè¯·æ‰‹åŠ¨é€‰æ‹© `checkpoint-200` æ–‡ä»¶å¤¹ã€‚å¦‚æœå¯èƒ½ï¼Œå»ºè®®æ‚¨æŠŠè¿™ä¸ªæ–‡ä»¶å¤¹å•ç‹¬å¤åˆ¶å‡ºæ¥å¤‡ä»½ï¼Œé‡å‘½åä¸º `Qwen3-Medical-Best`ã€‚\n",
        "\n",
        "------\n",
        "\n",
        "#### 3.3.5 æ ¸å¿ƒæŒ‡æ ‡æ€»ç»“è¡¨ (Summary Table)\n",
        "\n",
        "ä¸ºäº†æ–¹ä¾¿æ‚¨å­˜æ¡£å’Œå¯¹æ¯”ï¼Œæˆ‘ä¸ºæ‚¨æ€»ç»“äº†è¿™ä»½è¯¦ç»†çš„è®­ç»ƒæŠ¥å‘Šè¡¨ï¼š\n",
        "\n",
        "| **æ ¸å¿ƒç»´åº¦**   | **ç»†åˆ†æŒ‡æ ‡** | **ç»“æœæ•°å€¼/çŠ¶æ€**      | **åˆå­¦è€…è§£è¯»ä¸å»ºè®®**                                         |\n",
        "| -------------- | ------------ | ---------------------- | ------------------------------------------------------------ |\n",
        "| **ç¡¬ä»¶ä¸ç¯å¢ƒ** | æ˜¾å¡èµ„æº     | A10 (24G) å•å¡         | **å®Œç¾**ã€‚æ˜¾å­˜å ç”¨åˆç†ï¼Œæ— æº¢å‡ºï¼Œåˆ©ç”¨ç‡é«˜ã€‚                   |\n",
        "|                | è®¡ç®—ç²¾åº¦     | bfloat16 (bf16)        | **ä¼˜ç§€**ã€‚æ¯” fp16 æ›´ç¨³å®šï¼Œé€‚åˆ A10 æ¶æ„ã€‚                    |\n",
        "|                | é‡åŒ–ç­–ç•¥     | 4-bit (QLoRA)          | **å…³é”®**ã€‚è¿™æ˜¯å®ç° 620+ tokens/s é«˜é€Ÿè®­ç»ƒçš„æ ¸å¿ƒåŸå› ã€‚        |\n",
        "| **æ•°æ®æƒ…å†µ**   | æ•°æ®æ€»é‡     | 850 æ¡                 | **åå°‘**ã€‚æ•°æ®è¶Šå°‘ï¼Œè¶Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆæœ¬æ¬¡ Step 200 å°±è¿‡æ‹Ÿåˆäº†ï¼‰ã€‚ |\n",
        "|                | æ•°æ®æ ¼å¼     | CoT (æ€ç»´é“¾)           | **æ­£ç¡®**ã€‚æˆåŠŸè¯†åˆ« `<think>` æ ‡ç­¾ï¼Œæ¨¡å‹å­¦ä¼šäº†æ¨ç†æ­¥éª¤ã€‚      |\n",
        "| **è®­ç»ƒè¡¨ç°**   | æ€»è€—æ—¶       | 49 åˆ†é’Ÿ                | æ•ˆç‡å¾ˆé«˜ï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£å®éªŒã€‚                                 |\n",
        "|                | æœ€ä½³è½®æ•°     | Epoch ~0.94 (Step 200) | **é‡è¦çŸ¥è¯†ç‚¹**ï¼šå¹¶ä¸æ˜¯è½®æ•°è¶Šå¤šè¶Šå¥½ï¼Œå¯¹äºå°æ•°æ®ï¼Œ1 è½®å¾€å¾€å°±å¤Ÿäº†ã€‚ |\n",
        "| **æ¨¡å‹æ•ˆæœ**   | æœ€ä½³ Loss    | **0.5446** (éªŒè¯é›†)    | å‡ºç°åœ¨ **Step 200**ã€‚è¿™æ˜¯æ¨¡å‹â€œæ™ºåŠ›â€çš„å·…å³°æ—¶åˆ»ã€‚              |\n",
        "|                | æœ€ç»ˆ Loss    | 0.6880 (éªŒè¯é›†)        | å‡ºç°åœ¨ç»“æŸæ—¶ã€‚è¯´æ˜æ¨¡å‹åæœŸåœ¨â€œè´Ÿä¼˜åŒ–â€ã€‚                       |\n",
        "| **æœ€ç»ˆå†³ç­–**   | **æœ€ä½³å­˜æ¡£** | **checkpoint-200**     | **è¯·ä½¿ç”¨æ­¤ç‰ˆæœ¬ï¼** å®ƒæ˜¯æœ¬æ¬¡è®­ç»ƒçš„ç²¾åã€‚                      |\n",
        "|                | **é¿å‘å­˜æ¡£** | checkpoint-639         | å·²è¿‡æ‹Ÿåˆï¼Œå»ºè®®ä»…ä½œå¯¹æ¯”ç ”ç©¶ï¼Œä¸å»ºè®®ç”¨äºç”Ÿäº§ã€‚                 |\n",
        "| **åç»­ä¼˜åŒ–**   | ç­–ç•¥è°ƒæ•´     | å‡å°‘ Epoch / å¢åŠ æ•°æ®  | å»ºè®®ä¸‹æ¬¡å°† `Epoch` è®¾ä¸º 1ï¼Œæˆ–è€…å°†æ•°æ®æ‰©å……åˆ° 3000 æ¡ä»¥ä¸Šã€‚    |\n",
        "\n",
        "**ä¸€å¥è¯æ€»ç»“**ï¼šè¿™æ˜¯ä¸€æ¬¡æŠ€æœ¯ä¸Šéå¸¸æˆåŠŸçš„è¿è¡Œï¼Œç¯å¢ƒä¸å‚æ•°é…ç½®å®Œç¾ï¼Œä½†ç”±äºæ•°æ®é‡è¾ƒå°‘ï¼ˆ850æ¡ï¼‰ï¼Œæ¨¡å‹åœ¨ç¬¬ 1 è½®ï¼ˆStep 200ï¼‰å°±è¾¾åˆ°äº†æœ€ä½³çŠ¶æ€ï¼Œåç»­çš„è®­ç»ƒå±äºè¿‡åº¦å­¦ä¹ ã€‚è¯·ä½¿ç”¨ **Checkpoint-200** ä½œä¸ºæœ€ç»ˆæˆæœã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2484967d-f3a1-4f44-851e-39b378dda452",
      "metadata": {
        "id": "2484967d-f3a1-4f44-851e-39b378dda452"
      },
      "source": [
        "## 4. æ¨¡å‹è¯„ä¼°\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c1647de-2b53-4874-917e-9469aaa645c0",
      "metadata": {
        "id": "8c1647de-2b53-4874-917e-9469aaa645c0"
      },
      "source": [
        "> å¦‚æœè®­ç»ƒæ—¶å€™ï¼Œæœ‰é…ç½®è¯„ä¼°æ•°æ®é›†ï¼Œå…¶å®å·²ç»è¯„ä¼°è¿‡ï¼Œå¯ä»¥è·³è¿‡\n",
        "\n",
        "> æ³¨æ„ï¼šè®­ç»ƒæ•°æ®é›†å’Œè¯„ä¼°é›†ä¸è¦ä¸€æ ·ï¼ˆå› ä¸ºè®­ç»ƒæ—¶å€™å·²ç»è¯„ä¼°è¿‡ï¼Œè¿™é‡Œåªæ˜¯ä¸ºäº†æ¼”ç¤ºï¼‰\n",
        "\n",
        "å¾®è°ƒå®Œæˆåï¼Œç‚¹å‡»é¡µé¢é¡¶éƒ¨çš„ã€Œæ£€æŸ¥ç‚¹è·¯å¾„ã€ï¼Œç„¶åç‚¹å‡»æ£€æŸ¥ç‚¹è·¯å¾„è·¯å¾„ï¼Œå³å¯å¼¹å‡ºåˆšåˆšè®­ç»ƒå®Œæˆçš„LoRAæƒé‡ï¼Œç‚¹å‡»é€‰æ‹©ä¸‹æ‹‰åˆ—è¡¨ä¸­çš„é€‰é¡¹ï¼Œåœ¨æ¨¡å‹å¯åŠ¨æ—¶å³å¯åŠ è½½å¾®è°ƒç»“æœã€‚\n",
        "\n",
        "é€‰æ‹©ã€ŒEvaluate&Predictã€æ ï¼Œåœ¨æ•°æ®é›†ä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©ã€Œevalã€ï¼ˆéªŒè¯é›†ï¼‰è¯„ä¼°æ¨¡å‹ã€‚å¯ä»¥æ‰‹åŠ¨æ›´æ”¹è¾“å‡ºç›®å½•ä¸ºï¼Œæ¨¡å‹è¯„ä¼°ç»“æœå°†ä¼šä¿å­˜åœ¨è¯¥ç›®å½•ä¸­ã€‚æœ€åç‚¹å‡»å¼€å§‹æŒ‰é’®å¯åŠ¨æ¨¡å‹è¯„ä¼°ã€‚\n",
        "![image-20251204215633395](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042156629.png)\n",
        "\n",
        "\n",
        "\n",
        "æ¨¡å‹è¯„ä¼°å¤§çº¦éœ€è¦5åˆ†é’Ÿå·¦å³ï¼Œè¯„ä¼°å®Œæˆåä¼šåœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºéªŒè¯é›†çš„åˆ†æ•°ã€‚å…¶ä¸­ROUGEåˆ†æ•°è¡¡é‡äº†æ¨¡å‹è¾“å‡ºç­”æ¡ˆï¼ˆpredictï¼‰å’ŒéªŒè¯é›†ä¸­æ ‡å‡†ç­”æ¡ˆï¼ˆlabelï¼‰çš„ç›¸ä¼¼åº¦ï¼ŒROUGEåˆ†æ•°è¶Šé«˜ä»£è¡¨æ¨¡å‹å­¦ä¹ å¾—æ›´å¥½ã€‚\n",
        "![image-20251205183330224](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512051833451.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "743316f1-e46c-4b29-85aa-c999f5182625",
      "metadata": {
        "id": "743316f1-e46c-4b29-85aa-c999f5182625"
      },
      "source": [
        "| **ç‰¹æ€§**         | **BLEU**                                          | **ROUGE**                                         |\n",
        "| ---------------- | ------------------------------------------------- | ------------------------------------------------- |\n",
        "| **å…¨ç§°**         | Bilingual Evaluation Understudy                   | Recall-Oriented Understudy for Gisting Evaluation |\n",
        "| **ä¾§é‡ç‚¹**       | **ç²¾ç¡®ç‡ (Precision)** æˆ‘è¯´çš„æ¯ä¸€å¥è¯æ˜¯å¦éƒ½æ­£ç¡®ï¼Ÿ | **å¬å›ç‡ (Recall)** è¯¥è¯´çš„é‡ç‚¹æˆ‘æ˜¯å¦éƒ½è¯´åˆ°äº†ï¼Ÿ    |\n",
        "| **æƒ©ç½šæœºåˆ¶**     | æƒ©ç½šâ€œèƒ¡ç¼–ä¹±é€ â€ï¼ˆå¤šä½™çš„é”™è¯¯è¯ï¼‰                    | æƒ©ç½šâ€œé—æ¼ä¿¡æ¯â€ï¼ˆæ²¡æåˆ°çš„å…³é”®è¯ï¼‰                  |\n",
        "| **æœ€æ“…é•¿é¢†åŸŸ**   | æœºå™¨ç¿»è¯‘ã€ä»£ç ç”Ÿæˆ                                | æ–‡æœ¬æ‘˜è¦ã€é—®ç­”ç³»ç»Ÿï¼ˆQAï¼‰ã€é˜…è¯»ç†è§£                |\n",
        "| **ä½ çš„å¾®è°ƒåœºæ™¯** | è¡¡é‡åŒ»ç–—æœ¯è¯­æ˜¯å¦ä¸¥è°¨ï¼Œæ²¡æœ‰ä¹±ç”¨è¯ã€‚                | è¡¡é‡åŒ»ç–—è¯Šæ–­æ˜¯å¦é—æ¼äº†å…³é”®çš„æ²»ç–—æªæ–½ã€‚            |\n",
        "### 4.1. è´¨é‡è¯„ä¼°æŒ‡æ ‡ï¼ˆæ ¸å¿ƒå…³æ³¨ï¼‰\n",
        "\n",
        "è¿™äº›åˆ†æ•°ä¸»è¦åæ˜ æ¨¡å‹ç”Ÿæˆçš„å›ç­”ä¸â€œæ ‡å‡†ç­”æ¡ˆâ€æœ‰å¤šåƒã€‚åˆ†æ•°è¶Šé«˜ï¼Œä»£è¡¨å¾®è°ƒæ•ˆæœè¶Šå¥½ã€‚\n",
        "\n",
        "| æŒ‡æ ‡åç§° | æ•°å€¼ | æ ¸å¿ƒå«ä¹‰ | è¯¦ç»†è§£é‡Šä¸è¯„ä»· |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **predict\\_bleu-4** | **32.18** | **â€œè¿è¯â€ç²¾å‡†åº¦** | **è¯„ä»·ï¼šä¼˜ç§€**<br>å®ƒæ£€æŸ¥æ¨¡å‹æ˜¯å¦èƒ½è¿ç»­è¯´å¯¹4ä¸ªè¯ã€‚åœ¨ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¶…è¿‡30åˆ†é€šå¸¸æ„å‘³ç€æ¨¡å‹ä¸ä»…é€»è¾‘é€šé¡ºï¼Œè€Œä¸”**ä¸“ä¸šæœ¯è¯­ã€æˆè¯­ã€å›ºå®šæ­é…**ï¼ˆå¦‚â€œå­å®«æ”¶ç¼©ä¹åŠ›â€ï¼‰ä½¿ç”¨å¾—éå¸¸å‡†ç¡®ï¼Œæ²¡æœ‰å‡ºç°ä¸¥é‡çš„èƒ¡è¨€ä¹±è¯­ã€‚ |\n",
        "| **predict\\_rouge-1** | **58.75** | **å…³é”®è¯è¦†ç›–ç‡** | **è¯„ä»·ï¼šéå¸¸ä¼˜ç§€**<br>å®ƒè¡¡é‡æ ‡å‡†ç­”æ¡ˆé‡Œçš„å­—/è¯ï¼Œæœ‰å¤šå°‘è¢«æ¨¡å‹è¯´å‡ºæ¥äº†ã€‚æ¥è¿‘60åˆ†è¯´æ˜**æ ¸å¿ƒä¿¡æ¯ç‚¹ä¸¢å¤±æå°‘**ã€‚æ¯”å¦‚æ ‡å‡†ç­”æ¡ˆé‡Œæœ‰â€œæ­¢è¡€ã€è¾“è¡€ã€æŠ—ç”Ÿç´ â€ï¼Œæ¨¡å‹åŸºæœ¬ä¸Šå…¨è¦†ç›–äº†ã€‚ |\n",
        "| **predict\\_rouge-2** | **34.10** | **çŸ­è¯­åŒ¹é…åº¦** | **è¯„ä»·ï¼šè‰¯å¥½**<br>å®ƒè¡¡é‡è¿ç»­ä¸¤ä¸ªè¯çš„åŒ¹é…æƒ…å†µã€‚æ¯”å•çº¯çš„å…³é”®è¯åŒ¹é…æ›´éš¾ï¼Œå®ƒè¯´æ˜æ¨¡å‹ä¸ä»…è®°ä½äº†è¯ï¼Œè¿˜å­¦ä¼šäº†æ­£ç¡®çš„**ä¿®é¥°å…³ç³»**ï¼ˆä¾‹å¦‚æ˜¯â€œé¢„é˜²-æ„ŸæŸ“â€è€Œä¸æ˜¯â€œæ²»ç–—-æ„ŸæŸ“â€ï¼‰ã€‚ |\n",
        "| **predict\\_rouge-l** | **43.00** | **å¥å­ç»“æ„ç›¸ä¼¼åº¦** | **è¯„ä»·ï¼šä¼˜ç§€**<br>å®ƒè¡¡é‡å¥å­çº§åˆ«çš„é€»è¾‘ç»“æ„ã€‚43åˆ†è¯´æ˜æ¨¡å‹çš„**å™è¿°é€»è¾‘**ï¼ˆå¦‚ï¼šå…ˆè¯Šæ–­ -\\> åæ²»ç–— -\\> å†é¢„é˜²ï¼‰ä¸æ ‡å‡†ç­”æ¡ˆé«˜åº¦ä¸€è‡´ï¼Œè¿™æ­£æ˜¯ä½ çœ‹åˆ°çš„â€œåŒ»å˜±æ ¼å¼â€å¯¹é½çš„ä½“ç°ã€‚ |\n",
        "\n",
        "-----\n",
        "\n",
        "### 4.2. æ€§èƒ½ä¸æ•ˆç‡æŒ‡æ ‡ï¼ˆå‚è€ƒå…³æ³¨ï¼‰\n",
        "\n",
        "è¿™äº›æŒ‡æ ‡ä¸æ¨¡å‹çš„â€œæ™ºå•†â€æ— å…³ï¼Œåªä¸ä½ çš„ç¡¬ä»¶ï¼ˆGPUï¼‰å’Œå‚æ•°è®¾ç½®æœ‰å…³ã€‚\n",
        "\n",
        "| æŒ‡æ ‡åç§° | æ•°å€¼ | æ ¸å¿ƒå«ä¹‰ | è¯¦ç»†è§£é‡Šä¸è¯„ä»· |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **predict\\_samples\\_per\\_second** | **0.114** | **æ¨ç†é€Ÿåº¦** | **è¯„ä»·ï¼šè¾ƒæ…¢**<br>æ„å‘³ç€æ¯ç§’åªèƒ½å¤„ç†0.114æ¡æ•°æ®ï¼Œæ¢ç®—ä¸‹æ¥å¤„ç†ä¸€æ¡æ•°æ®å¤§çº¦éœ€è¦ **8.7ç§’**ã€‚ |\n",
        "| **predict\\_runtime** | **877.78** | **æ€»è€—æ—¶** | **è¯„ä»·ï¼šæ­£å¸¸**<br>è¯„ä¼°å®Œè¿™100æ¡æ•°æ®ï¼ˆä½ è®¾ç½®çš„éªŒè¯é›†å¤§å°ï¼‰æ€»å…±èŠ±äº†877ç§’ï¼Œçº¦14.6åˆ†é’Ÿã€‚ |\n",
        "| **predict\\_model\\_preparation\\_time** | **0.0061** | **æ¨¡å‹å‡†å¤‡æ—¶é—´** | **è¯„ä»·ï¼šæå¿«**<br>åŠ è½½æ¨¡å‹åˆ°æ˜¾å­˜å‡†å¤‡å¼€å§‹æ¨ç†çš„æ—¶é—´ï¼Œå‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚ |\n",
        "\n",
        "-----\n",
        "\n",
        "### 4.3. ç»¼åˆä¸“å®¶ç‚¹è¯„\n",
        "\n",
        "**ç»“è®ºï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸æˆåŠŸçš„å¾®è°ƒï¼ˆSFTï¼‰ã€‚**\n",
        "\n",
        "**ä¸ºä»€ä¹ˆè¿™ä¹ˆè¯´ï¼Ÿ**\n",
        "åœ¨å¼€æ”¾åŸŸå¯¹è¯ï¼ˆOpen Chatï¼‰å¾®è°ƒä¸­ï¼ŒBLEU-4 èƒ½è¾¾åˆ° 20-25 åˆ†å°±å¾ˆä¸é”™äº†ã€‚ä½ çš„æ¨¡å‹åœ¨åŒ»ç–—æ•°æ®é›†ä¸Šè·‘å‡ºäº† **32+** çš„åˆ†æ•°ï¼Œä¸” ROUGE-1 æ¥è¿‘ **60**ï¼Œè¿™è¯´æ˜ï¼š\n",
        "\n",
        "1.  **å¼ºå¯¹é½ï¼ˆStrong Alignmentï¼‰ï¼š** æ¨¡å‹éå¸¸å¬è¯ï¼Œå®ƒå‡ ä¹æ˜¯åœ¨â€œå¤åˆ»â€ä½ æä¾›çš„æ•°æ®é›†é£æ ¼ã€‚\n",
        "2.  **ä½å¹»è§‰ï¼ˆLow Hallucinationï¼‰ï¼š** å› ä¸º ROUGE åˆ†æ•°å¾ˆé«˜ï¼Œè¯´æ˜æ¨¡å‹æ²¡æœ‰å¤§é‡ç¼–é€ æ ‡å‡†ç­”æ¡ˆä»¥å¤–çš„åºŸè¯ã€‚\n",
        "3.  **å…³äºé€Ÿåº¦ï¼š** æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼ˆ8ç§’/æ¡ï¼‰å¯èƒ½æ˜¯å› ä¸ºä½ è®¾ç½®çš„ `æœ€å¤§ç”Ÿæˆé•¿åº¦` (2048) è¾ƒé•¿ï¼Œæˆ–è€…æ˜¯å› ä¸ºä½¿ç”¨äº† A10 æ˜¾å¡è¿›è¡Œ FP16/BF16 æ¨ç†æ—¶çš„æ­£å¸¸è´Ÿè½½è¡¨ç°ã€‚è¿™ä¸å½±å“æ¨¡å‹è´¨é‡ï¼Œåªå½±å“ç”Ÿäº§æ•ˆç‡ã€‚\n",
        "\n",
        "**ä¸€å¥è¯æ€»ç»“ï¼š** ä½ çš„æ¨¡å‹ç°åœ¨å°±åƒä¸€ä¸ª**èƒŒä¹¦å¾ˆç†Ÿç»ƒçš„åŒ»å­¦ç”Ÿ**ï¼Œèƒ½å¤ŸæŒ‰ç…§æ•™ç§‘ä¹¦ï¼ˆæ•°æ®é›†ï¼‰çš„æ ‡å‡†è§„èŒƒæ¥å›ç­”é—®é¢˜ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb18065-a0f2-4106-8c98-e35e4455e4d4",
      "metadata": {
        "id": "4cb18065-a0f2-4106-8c98-e35e4455e4d4"
      },
      "source": [
        "## 5. æ¨¡å‹å¯¹è¯"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df83adb-3fdc-4029-9ee9-24eec0a38b0e",
      "metadata": {
        "id": "4df83adb-3fdc-4029-9ee9-24eec0a38b0e"
      },
      "source": [
        "é€‰æ‹©ã€ŒChatã€æ ï¼Œç¡®ä¿é€‚é…å™¨è·¯å¾„æ˜¯`checkpoint-200`ï¼ˆè¯„ä¼°æ•ˆæœæœ€å¥½çš„æ£€æŸ¥ç‚¹ï¼‰ï¼Œç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€å³å¯åœ¨Web UIä¸­å’Œå¾®è°ƒæ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚\n",
        "\n",
        "åœ¨é¡µé¢åº•éƒ¨çš„å¯¹è¯æ¡†è¾“å…¥æƒ³è¦å’Œæ¨¡å‹å¯¹è¯çš„å†…å®¹ï¼Œç‚¹å‡»ã€Œæäº¤ã€å³å¯å‘é€æ¶ˆæ¯ã€‚å‘é€åæ¨¡å‹ä¼šé€å­—ç”Ÿæˆå›ç­”ï¼Œä»å›ç­”ä¸­å¯ä»¥å‘ç°æ¨¡å‹å­¦ä¹ åˆ°äº†æ•°æ®é›†ä¸­çš„å†…å®¹ï¼Œèƒ½å¤Ÿæ°å½“åœ°æ¨¡ä»¿è¯¸è‘›äº®çš„è¯­æ°”å¯¹è¯ã€‚\n",
        "è¿™å››ä¸ªå…³é”®é…ç½®é¡¹ã€‚æ˜¯è¿›å…¥â€œæ¨¡å‹å¯¹è¯ï¼ˆChatï¼‰â€ç•Œé¢çš„å…¥å£ï¼Œé…ç½®æ­£ç¡®ä¸å¦ç›´æ¥å†³å®šäº†æ‚¨èƒ½å¦ä¸åˆšåˆšè®­ç»ƒå¥½çš„â€œæœ€ä½³æ¨¡å‹â€è¿›è¡Œäº¤äº’ã€‚\n",
        "\n",
        "### 5.1 æ¨¡å‹åç§° (Model Name)\n",
        "\n",
        "- **å½“å‰è®¾ç½®**ï¼š`Qwen3-8B-Instruct`\n",
        "- **å‚æ•°è§£é‡Š**ï¼šè¿™æ˜¯ç»™ LLaMA-Factory çš„ä¸€ä¸ªâ€œæš—å·â€ã€‚\n",
        "  - å®ƒå‘Šè¯‰ç³»ç»Ÿï¼šâ€œæˆ‘è¦åŠ è½½çš„æ¨¡å‹æ˜¯ Qwen3 ç³»åˆ—çš„ Instruct ç‰ˆæœ¬â€ã€‚\n",
        "  - ç³»ç»Ÿæ”¶åˆ°è¿™ä¸ªæš—å·åï¼Œä¼šè‡ªåŠ¨å»æŸ¥æ‰¾å¯¹åº”çš„**æ¨¡å‹ç»“æ„æ¨¡æ¿**ï¼ˆæ¯”å¦‚çŸ¥é“ Qwen3 ç”¨çš„æ˜¯ `swiglu` æ¿€æ´»å‡½æ•°ï¼Œç”¨çš„æ˜¯ `rope` ä½ç½®ç¼–ç ç­‰ï¼‰ã€‚\n",
        "  - **æ³¨æ„**ï¼šè¿™ä¸ªåç§°å¿…é¡»ä¸æ‚¨è®­ç»ƒæ—¶é€‰æ‹©çš„åç§°ä¿æŒä¸€è‡´ï¼Œå¦åˆ™å¯èƒ½ä¼šå› ä¸ºæ¨¡æ¿ä¸åŒ¹é…å¯¼è‡´åŠ è½½å¤±è´¥ã€‚\n",
        "\n",
        "### 5.2 æ¨¡å‹è·¯å¾„ (Model Path)\n",
        "\n",
        "- **å½“å‰è®¾ç½®**ï¼š`/mnt/workspace/Qwen3-8B`\n",
        "- **å‚æ•°è§£é‡Š**ï¼šè¿™æ˜¯åŸºåº§æ¨¡å‹ï¼ˆBase Modelï¼‰æƒé‡çš„ç‰©ç†å­˜å‚¨ä½ç½®ã€‚\n",
        "- **å½¢è±¡æ¯”å–»**ï¼š\n",
        "  - **æ¨¡å‹è·¯å¾„** = åŸç‰ˆæ•™ç§‘ä¹¦ï¼ˆQwen3 åŸºåº§ï¼‰ã€‚\n",
        "  - **æ£€æŸ¥ç‚¹è·¯å¾„** = æ‚¨åšçš„åŒ»ç–—ç¬”è®°ï¼ˆLoRA æƒé‡ï¼‰ã€‚\n",
        "  - **åŠ è½½è¿‡ç¨‹** = ç³»ç»Ÿå…ˆæ‰“å¼€â€œæ•™ç§‘ä¹¦â€ï¼Œç„¶åæŠŠæ‚¨çš„â€œç¬”è®°â€è´´ä¸Šå»ï¼Œç»„åˆæˆä¸€ä¸ªæ‡‚åŒ»ç–—çš„ä¸“å®¶æ¨¡å‹ã€‚\n",
        "- **å¸¸è§é”™è¯¯**ï¼šåƒä¸‡ä¸è¦æŠŠè¿™é‡Œå¡«æˆ LoRA çš„è·¯å¾„ï¼Œå¦åˆ™ç³»ç»Ÿä¼šå› ä¸ºæ‰¾ä¸åˆ°åŸºåº§æ¨¡å‹çš„é…ç½®æ–‡ä»¶ï¼ˆconfig.jsonï¼‰è€ŒæŠ¥é”™ã€‚\n",
        "\n",
        "### 5.3 æ£€æŸ¥ç‚¹è·¯å¾„ (Checkpoint Path)\n",
        "\n",
        "- å½“å‰è®¾ç½®ï¼š\n",
        "\n",
        "  `/mnt/workspace/LLaMA-Factory/saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-200`\n",
        "\n",
        "- **å‚æ•°è§£é‡Š**ï¼šè¿™æ˜¯æ‚¨åˆšåˆšè®­ç»ƒå‡ºæ¥çš„â€œåŒ»ç–—å¤–æŒ‚å¤§è„‘â€çš„å­˜æ”¾ä½ç½®ã€‚\n",
        "\n",
        "- **ä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ªè·¯å¾„**ï¼š\n",
        "\n",
        "  - å›é¡¾ä¹‹å‰çš„åˆ†æï¼Œæ‚¨çš„æ¨¡å‹åœ¨ **Step 200** æ—¶æ•ˆæœæœ€å¥½ï¼ˆéªŒè¯é›† Loss æœ€ä½ï¼‰ï¼Œä¹‹åå°±è¿‡æ‹Ÿåˆäº†ã€‚\n",
        "  - æ‚¨å·²ç»æˆåŠŸæ‰§è¡Œäº† Python è„šæœ¬æˆ– Shell å‘½ä»¤ï¼Œå°† `checkpoint-200` ä»æ·±å±‚ç›®å½•ä¸­æå–å‡ºæ¥ï¼ˆæˆ–è€…ç›´æ¥é€‰ä¸­äº†è¯¥å­æ–‡ä»¶å¤¹ï¼‰ï¼Œè¿™éå¸¸å…³é”®ã€‚\n",
        "  - **å¦‚æœä¸é€‰**ï¼šWebUI é»˜è®¤åªä¼šåŠ è½½åŸºåº§æ¨¡å‹ï¼ˆQwen3ï¼‰ï¼Œé‚£æ‚¨ä¹‹å‰çš„ 49 åˆ†é’Ÿå°±ç™½ç»ƒäº†ï¼Œæ¨¡å‹è¿˜æ˜¯é‚£ä¸ªä¸æ‡‚åŒ»ç–—çš„é€šç”¨æ¨¡å‹ã€‚\n",
        "\n",
        "### 5.4 æ¨ç†å¼•æ“ (Inference Engine)\n",
        "\n",
        "- **å½“å‰è®¾ç½®**ï¼š`huggingface`\n",
        "- **å‚æ•°è§£é‡Š**ï¼šè¿™æ˜¯å†³å®šâ€œè°æ¥è´Ÿè´£è®¡ç®—â€çš„æ ¸å¿ƒé€‰é¡¹ã€‚\n",
        "  - **huggingface**ï¼š\n",
        "    - **æ˜¯ä»€ä¹ˆ**ï¼šä½¿ç”¨ PyTorch åŸç”Ÿçš„ `transformers` åº“è¿›è¡Œæ¨ç†ã€‚\n",
        "    - **ç‰¹ç‚¹**ï¼š**å…¼å®¹æ€§ä¹‹ç‹**ã€‚å®ƒå‡ ä¹æ”¯æŒæ‰€æœ‰æ¨¡å‹æ¶æ„å’Œ LoRA æƒé‡ï¼Œè™½ç„¶é€Ÿåº¦ä¸å¦‚ vLLM æè‡´ï¼Œä½†åœ¨åŠ è½½ LoRA é€‚é…å™¨ï¼ˆå°¤å…¶æ˜¯æ‚¨åˆšåˆšæ‰‹åŠ¨æå–çš„ `checkpoint-200`ï¼‰æ—¶æœ€ä¸å®¹æ˜“æŠ¥é”™ã€‚\n",
        "    - **ä¸ºä»€ä¹ˆé€‰å®ƒ**ï¼šä¹‹å‰å°è¯• `vllm` æ—¶é‡åˆ°äº† `LoRALRUCache` çš„å…¼å®¹æ€§é”™è¯¯ï¼ˆAttributeErrorï¼‰ï¼Œåˆ‡æ¢åˆ° `huggingface` æ˜¯æœ€ç¨³å¦¥çš„è§£å†³ä¹‹é“ã€‚\n",
        "  - **vLLM**ï¼š\n",
        "    - **æ˜¯ä»€ä¹ˆ**ï¼šä¸€ä¸ªè¿½æ±‚æé€Ÿæ¨ç†çš„å¼•æ“ï¼Œé€šå¸¸ç”¨äºç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ã€‚\n",
        "    - **ç¼ºç‚¹**ï¼šå¯¹ LoRA çš„åŠ¨æ€åŠ è½½æ”¯æŒæœ‰æ—¶ä¸ç¨³å®šï¼Œä¸”å¯¹æ˜¾å¡é©±åŠ¨å’Œ CUDA ç‰ˆæœ¬è¦æ±‚æ›´ä¸¥è‹›ã€‚\n",
        "\n",
        "![image-20251204211702449](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042117609.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49fb1988-339f-4656-9d36-dbbbe5ef5e9c",
      "metadata": {
        "id": "49fb1988-339f-4656-9d36-dbbbe5ef5e9c"
      },
      "source": [
        "### 5.5 å¾®è°ƒæ¨¡å‹-å¯¹è¯æµ‹è¯•\n",
        "\n",
        "åŸºäºæˆ‘ä»¬çš„ `medical-o1-reasoning-SFT` æ•°æ®é›†æ ·ä¾‹ï¼Œè¿™äº›æ•°æ®å…·æœ‰å¾ˆå¼ºçš„**â€œç—…ä¾‹æè¿° -> é€æ­¥æ¨ç† -> æœ€ç»ˆè¯Šæ–­/æ–¹æ¡ˆâ€**çš„ç‰¹å¾ã€‚\n",
        "\n",
        "ä¸ºäº†éªŒè¯æ¨¡å‹æ˜¯å¦çœŸçš„å­¦ä¼šäº†**â€œæ€ç»´é“¾ï¼ˆCoTï¼‰â€**æ¨ç†èƒ½åŠ›ï¼Œè€Œä¸ä»…ä»…æ˜¯æ­»è®°ç¡¬èƒŒï¼Œæˆ‘è®¾è®¡äº† **3 ç±»** æµ‹è¯•æç¤ºè¯ï¼ˆPromptsï¼‰ã€‚\n",
        "\n",
        "å»ºè®®ä½ åœ¨ Chat ç•Œé¢ä¸­è¾“å…¥è¿™äº›é—®é¢˜ï¼Œå¹¶**é‡ç‚¹è§‚å¯Ÿæ¨¡å‹è¾“å‡ºä¸­æ˜¯å¦åŒ…å« `<think>` æ ‡ç­¾æˆ–æ˜æ˜¾çš„æ¨ç†è¿‡ç¨‹**ã€‚\n",
        "\n",
        "------\n",
        "\n",
        "#### ç¬¬ä¸€ç±»ï¼šä¸­åŒ»è¾¨è¯æ¨ç†æµ‹è¯•ï¼ˆéªŒè¯é€»è¾‘å¤ç°ï¼‰\n",
        "\n",
        "*è¿™ç±»æ•°æ®åœ¨ä½ çš„è®­ç»ƒé›†ä¸­å æ¯”è¾ƒå¤§ï¼ˆå¦‚è¼è›„ç––ã€è‚ºé˜´è™šç­‰ï¼‰ã€‚*\n",
        "\n",
        "**æµ‹è¯•è¾“å…¥ 1ï¼š**\n",
        "\n",
        "> æ‚£è€…è¡¨ç°ä¸ºä¹…å’³ä¸æ­¢ï¼Œç—°å°‘è€Œç²˜ï¼Œä¸æ˜“å’³å‡ºï¼Œå£é¼»å¹²ç‡¥ï¼Œå’½å–‰å¹²ç—›ï¼Œåˆåæ½®çƒ­ï¼Œé¢§çº¢ï¼Œç›—æ±—ï¼ŒèˆŒçº¢å°‘è‹”ï¼Œè„‰ç»†æ•°ã€‚è¯·é—®è¯¥æ‚£è€…çš„ä¸­åŒ»è¾¨è¯æ˜¯ä»€ä¹ˆï¼Ÿè¯·é€æ­¥æ¨ç†å¹¶ç»™å‡ºç­”æ¡ˆã€‚\n",
        "\n",
        "**é¢„æœŸæ¨¡å‹è¡¨ç°ï¼š**\n",
        "\n",
        "1. **æ¨ç†è¿‡ç¨‹ (`<think>`)**ï¼šæ¨¡å‹åº”åˆ†æâ€œç—°å°‘è€Œç²˜ã€å£é¼»å¹²ç‡¥â€æŒ‡å‘**é˜´è™š**ï¼ˆæ´¥æ¶²ä¸è¶³ï¼‰ï¼›â€œåˆåæ½®çƒ­ã€ç›—æ±—ã€é¢§çº¢ã€è„‰ç»†æ•°â€æŒ‡å‘**å†…çƒ­**ï¼ˆé˜´è™šç”Ÿå†…çƒ­ï¼‰ï¼›ç—…ä½åœ¨è‚ºï¼ˆå’³å—½ï¼‰ã€‚\n",
        "\n",
        "2. **æœ€ç»ˆç»“è®º**ï¼š**è‚ºé˜´äºè€—**ï¼ˆæˆ–è‚ºé˜´è™šè¯ï¼‰ã€‚\n",
        "\n",
        "   ![image-20251204210831894](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042108984.png)\n",
        "\n",
        "   ![image-20251204210917029](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042109178.png)\n",
        "\n",
        "------\n",
        "\n",
        "#### ç¬¬äºŒç±»ï¼šæ€¥è¯Š/ä¸´åºŠå¤„ç½®å†³ç­–æµ‹è¯•ï¼ˆéªŒè¯ä¼˜å…ˆçº§åˆ¤æ–­ï¼‰\n",
        "\n",
        "*å¯¹åº”æ•°æ®é›†ä¸­â€œæ€¥æ€§å·¦å¿ƒè¡°â€ã€â€œé«˜è¡€å‹æ€¥ç—‡â€ç­‰æ¡ˆä¾‹ã€‚*\n",
        "\n",
        "**æµ‹è¯•è¾“å…¥ 2ï¼š**\n",
        "\n",
        "> ä¸€ä½65å²ç”·æ€§æ‚£è€…ï¼Œæ—¢å¾€æœ‰å† å¿ƒç—…å²ã€‚çªå‘èƒ¸éª¨åå‹æ¦¨æ€§ç–¼ç—›2å°æ—¶ï¼Œä¼´å¤§æ±—æ·‹æ¼“ã€æ¶å¿ƒå‘•åã€‚å¿ƒç”µå›¾æ˜¾ç¤ºIIã€IIIã€aVFå¯¼è”STæ®µå¼“èƒŒå‘ä¸ŠæŠ¬é«˜ã€‚æ­¤æ—¶é¦–é€‰çš„å†çŒæ³¨æ²»ç–—ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿè¯·é€æ­¥æ¨ç†ã€‚\n",
        "\n",
        "**é¢„æœŸæ¨¡å‹è¡¨ç°ï¼š**\n",
        "\n",
        "1. **æ¨ç†è¿‡ç¨‹ (`<think>`)**ï¼šé¦–å…ˆè¯†åˆ«ç—‡çŠ¶ï¼ˆèƒ¸ç—›ã€å¤§æ±—ï¼‰å’Œå¿ƒç”µå›¾ç‰¹å¾ï¼ˆII, III, aVFæŠ¬é«˜ï¼‰è¯Šæ–­ä¸º**æ€¥æ€§ä¸‹å£å¿ƒè‚Œæ¢—æ­»**ã€‚æ¥ç€åˆ¤æ–­æ—¶é—´çª—ï¼ˆå‘ç—…2å°æ—¶ï¼‰ï¼Œå±äºå†çŒæ³¨æ²»ç–—çš„æœ€ä½³çª—å£æœŸã€‚æ¯”è¾ƒPCIï¼ˆæ”¯æ¶ï¼‰å’Œæº¶æ “ã€‚å¦‚æœæœ‰æ¡ä»¶ï¼ŒPCIæ˜¯é¦–é€‰ï¼›æ— æ¡ä»¶åˆ™æº¶æ “ã€‚\n",
        "\n",
        "2. **æœ€ç»ˆç»“è®º**ï¼šé¦–é€‰**æ€¥è¯ŠPCI**ï¼ˆç»çš®å† çŠ¶åŠ¨è„‰ä»‹å…¥æ²»ç–—ï¼‰ã€‚\n",
        "\n",
        "   ![image-20251204211850523](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042118603.png)\n",
        "\n",
        "   ![image-20251204211934793](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042119983.png)\n",
        "\n",
        "------\n",
        "\n",
        "#### ç¬¬ä¸‰ç±»ï¼šå¤æ‚å¤–ç§‘/å¦‡äº§ç§‘é‰´åˆ«ï¼ˆéªŒè¯å› æœåˆ†æï¼‰\n",
        "\n",
        "*å¯¹åº”æ•°æ®é›†ä¸­â€œäº§åå‡ºè¡€â€ã€â€œé˜‘å°¾åŒ…å—â€ç­‰æ¡ˆä¾‹ã€‚*\n",
        "\n",
        "**æµ‹è¯•è¾“å…¥ 3ï¼š**\n",
        "\n",
        "> ä¸€ä½åˆäº§å¦‡ï¼Œèƒå„¿å¨©å‡ºå10åˆ†é’Ÿï¼Œå‡ºç°é˜´é“æµè¡€ï¼Œè‰²æš—çº¢ï¼Œä¸”èƒç›˜æœªå¨©å‡ºã€‚æ£€æŸ¥å‘ç°å­å®«è½®å»“ä¸æ¸…ï¼ŒæŒ‰å‹å®«åº•æœ‰å¤§é‡è¡€å—æ¶Œå‡ºã€‚è¯·æ¨æ–­å‡ºè¡€åŸå› åŠå¤„ç†åŸåˆ™ã€‚\n",
        "\n",
        "**é¢„æœŸæ¨¡å‹è¡¨ç°ï¼š**\n",
        "\n",
        "1. **æ¨ç†è¿‡ç¨‹ (`<think>`)**ï¼šåˆ†æå‡ºè¡€æ—¶æœºï¼ˆèƒå„¿å¨©å‡ºåï¼Œèƒç›˜æœªå¨©å‡ºå‰ï¼‰ã€‚åˆ†æå‡ºè¡€ç‰¹å¾ï¼ˆæš—çº¢ï¼Œéé²œçº¢ï¼Œæ’é™¤è½¯äº§é“è£‚ä¼¤ï¼‰ã€‚åˆ†æä½“å¾ï¼ˆå­å®«è½®å»“ä¸æ¸… -> å®«ç¼©ä¹åŠ›ï¼Ÿæˆ–è€…èƒç›˜å‰¥ç¦»ä¸å…¨ï¼Ÿï¼‰ã€‚ç”±äºèƒç›˜æœªå¨©å‡ºï¼Œé¦–å…ˆè€ƒè™‘**èƒç›˜å› ç´ **ï¼ˆæ»ç•™æˆ–å‰¥ç¦»ä¸å…¨ï¼‰å¯¼è‡´çš„å‡ºè¡€ï¼Œç»§å‘å®«ç¼©ä¹åŠ›ã€‚\n",
        "\n",
        "2. **æœ€ç»ˆç»“è®º**ï¼š**èƒç›˜å› ç´ **ï¼ˆèƒç›˜æ»ç•™/å‰¥ç¦»ä¸å…¨ï¼‰ã€‚å¤„ç†åŸåˆ™æ˜¯**ååŠ©èƒç›˜å¨©å‡º**ï¼ˆå¦‚æ‰‹å–èƒç›˜ï¼‰å¹¶åŠ å¼ºå®«ç¼©ã€‚\n",
        "\n",
        "   ![image-20251204212040659](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042120821.png)\n",
        "\n",
        "   ![image-20251204212206132](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042122202.png)\n",
        "\n",
        "------\n",
        "\n",
        "#### ğŸŸ¢ å¦‚ä½•åˆ¤æ–­æ¨¡å‹æ˜¯å¦â€œèªæ˜â€ï¼Ÿ\n",
        "\n",
        "åœ¨ä½ åŠ è½½äº† `checkpoint-200` åï¼Œè§‚å¯Ÿå›ç­”ï¼š\n",
        "\n",
        "1. **æ»¡åˆ†å›ç­”**ï¼š\n",
        "   - è¾“å‡ºåŒ…å« `<think>...</think>` å†…å®¹ï¼ˆå¦‚æœ WebUI æ¸²æŸ“äº† HTML æ ‡ç­¾å¯èƒ½çœ‹ä¸è§ï¼Œä½†èƒ½çœ‹åˆ°ä¸€æ®µæ˜æ˜¾çš„åˆ†ææ–‡æœ¬ï¼‰ã€‚\n",
        "   - æ¨ç†é€»è¾‘æ˜¯ï¼š**ç—‡çŠ¶æå– -> ç—…ç†æœºåˆ¶åˆ†æ -> æ’é™¤å…¶ä»–å¯èƒ½ -> é”å®šè¯Šæ–­ -> ç»™å‡ºæ–¹æ¡ˆ**ã€‚\n",
        "   - è¿™è¯´æ˜æ¨¡å‹å­¦åˆ°äº†æ•°æ®é›†çš„**æ€ç»´é€»è¾‘ (Reasoning)**ã€‚\n",
        "2. **åŠæ ¼å›ç­”**ï¼š\n",
        "   - ç›´æ¥ç»™å‡ºäº†æ­£ç¡®ç­”æ¡ˆï¼Œä½†æ²¡æœ‰è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ã€‚\n",
        "   - è¿™è¯´æ˜æ¨¡å‹å­¦åˆ°äº†**çŸ¥è¯† (Knowledge)**ï¼Œä½†ä¸¢å¤±äº†**æ€ç»´é“¾ (CoT)**ï¼Œå¯èƒ½æ˜¯ Epoch è®­ç»ƒè¿‡å¤šå¯¼è‡´çš„è½»å¾®è¿‡æ‹Ÿåˆï¼Œæˆ–è€…æ˜¯ Prompt æ²¡æœ‰è§¦å‘æ¨ç†ã€‚\n",
        "3. **å¤±è´¥å›ç­”**ï¼š\n",
        "   - ç­”éæ‰€é—®ï¼Œæˆ–è€…åƒå¤è¯»æœºä¸€æ ·é‡å¤é—®é¢˜ã€‚\n",
        "   - è¿™æ˜¯ä¸¥é‡çš„è¿‡æ‹Ÿåˆæˆ–è®­ç»ƒå¤±è´¥ï¼ˆé€šå¸¸å‡ºç°åœ¨ checkpoint-639ï¼‰ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780affa7-b233-47e8-b1ca-573106f8c50d",
      "metadata": {
        "id": "780affa7-b233-47e8-b1ca-573106f8c50d"
      },
      "source": [
        "### 5.6 å¸è½½æ¨¡å‹ï¼šä¸å¾®è°ƒå‰çš„åŸå§‹æ¨¡å‹èŠå¤©\n",
        "ç‚¹å‡»ã€Œå¸è½½æ¨¡å‹ã€ï¼Œ`æ£€æŸ¥ç‚¹è·¯å¾„`ä¸ºç©ºï¼Œå†æ¬¡ç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€ï¼Œå³å¯ä¸å¾®è°ƒå‰çš„åŸå§‹æ¨¡å‹èŠå¤©ã€‚\n",
        "![image-20251205183418359](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512051834516.png)\n",
        "é‡æ–°å‘æ¨¡å‹å‘é€ç›¸åŒçš„å†…å®¹ã€‚\n",
        "\n",
        "![image-20251204214828861](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042148948.png)\n",
        "\n",
        "å‘ç°å·®å¼‚ä¸å¤§ï¼Œå…¶å®æ˜¯**å¥½äº‹**ï¼Œè¯´æ˜å¾®è°ƒæ²¡æœ‰ç ´åæ¨¡å‹çš„çŸ¥è¯†å‡†ç¡®æ€§ã€‚å…·ä½“åŸå› å¦‚ä¸‹ï¼š\n",
        "\n",
        "1. åŒ»å­¦ç­”æ¡ˆçš„å”¯ä¸€æ€§ï¼ˆä¿åº•çº¿ï¼‰\n",
        "\n",
        "   æ²»ç–—æ–¹æ¡ˆï¼ˆå¦‚ç¼©å®«ç´ ã€æŒ‰æ‘©å­å®«ï¼‰æ˜¯æ ‡å‡†åŒ»å­¦çŸ¥è¯†ï¼Œæ ¸å¿ƒå†…å®¹å¿…é¡»ä¸€è‡´ã€‚å¦‚æœå¾®è°ƒåçç¼–å‡ºä¸€ç§æ–°ç–—æ³•ï¼Œåè€Œæ˜¯ä¸¥é‡çš„â€œå¹»è§‰â€ã€‚\n",
        "\n",
        "2. **LoRA æ”¹çš„æ˜¯â€œè§„çŸ©â€è€Œéâ€œçŸ¥è¯†â€**\n",
        "\n",
        "   - **å›¾1ï¼ˆå¾®è°ƒå‰ï¼‰**ï¼šåƒç™¾ç§‘å…¨ä¹¦ï¼Œæ³›æ³›è€Œè°ˆï¼Œç½—åˆ—æ‰€æœ‰çŸ¥è¯†ã€‚\n",
        "   - **å›¾2ï¼ˆå¾®è°ƒåï¼‰**ï¼šåƒ**ä¸´åºŠåŒ»ç”Ÿ**ã€‚å®ƒç»“åˆäº†**å…·ä½“ç—…ä¾‹**ï¼ˆâ€œæ ¹æ®æè¿°...â€ï¼‰ï¼Œä¸”æ’ç‰ˆå˜æˆäº†æ›´æ¸…æ™°çš„**åŒ»å˜±æ ¼å¼**ï¼ˆåŠ ç²—å…³é”®è¯ã€ä¼˜å…ˆçº§æ’åºï¼‰ã€‚\n",
        "\n",
        "**ç»“è®º**ï¼šå¾®è°ƒç”Ÿæ•ˆäº†ã€‚æ¨¡å‹ä»â€œèƒŒä¹¦æ¨¡å¼â€åˆ‡æ¢åˆ°äº†â€œçœ‹ç—…æ¨¡å¼â€ï¼Œè¿™æ­£æ˜¯ä½ æƒ³è¦çš„å‚ç›´é¢†åŸŸå¯¹é½ï¼ˆAlignmentï¼‰ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20bf8e55-95ce-460c-ace9-a7996824b9e3",
      "metadata": {
        "id": "20bf8e55-95ce-460c-ace9-a7996824b9e3"
      },
      "source": [
        "## 6. æ€»ç»“\n",
        "\n",
        "æœ¬æ¬¡æ•™ç¨‹ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨PAIå’ŒLLaMA Factoryæ¡†æ¶ï¼ŒåŸºäºè½»é‡åŒ–LoRAæ–¹æ³•å¾®è°ƒQwen3æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè¿›è¡Œä¸­æ–‡é—®ç­”å’Œè§’è‰²æ‰®æ¼”ï¼ŒåŒæ—¶é€šè¿‡éªŒè¯é›†ROUGEåˆ†æ•°å’Œäººå·¥æµ‹è¯•éªŒè¯äº†å¾®è°ƒçš„æ•ˆæœã€‚åœ¨åç»­å®è·µä¸­ï¼Œå¯ä»¥ä½¿ç”¨å®é™…ä¸šåŠ¡æ•°æ®é›†ï¼Œå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°èƒ½å¤Ÿè§£å†³å®é™…ä¸šåŠ¡åœºæ™¯é—®é¢˜çš„æœ¬åœ°é¢†åŸŸå¤§æ¨¡å‹ã€‚"
      ]
    }
  ],
  "metadata": {
    "dsw_sample": {
      "buildId": "1175",
      "pipeline": "pai-dsw-examples-master"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "share": {
      "datetime": "2024-05-08T07:37:10.242Z",
      "image": {
        "name": "modelscope:1.14.0-pytorch2.1.2-gpu-py310-cu121-ubuntu22.04",
        "url": "dsw-registry-vpc.cn-hangzhou.cr.aliyuncs.com/pai/modelscope:1.14.0-pytorch2.1.2-gpu-py310-cu121-ubuntu22.04"
      },
      "instance": "dsw-d03c4949959b7041",
      "spec": {
        "id": "ecs.gn7i-c8g1.2xlarge",
        "type": "GPU"
      },
      "uid": "1157703270994901"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}