{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkB14e1duUxg",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### OpenAI 函数调用（tools Schema）回顾\n",
    "\n",
    "在 Chat Completions API 中，通过 `tools` 字段向模型暴露可调用的函数（工具）。每个工具定义如下：\n",
    "\n",
    "- `type`: 固定为 `function`\n",
    "- `function`:\n",
    "  - `name` (string): 工具名称（小写、下划线风格更稳妥，长度 ≤ 64 常见做法）\n",
    "  - `description` (string): 工具用途的自然语言描述（有助于模型选择正确工具，建议简洁清晰）\n",
    "  - `parameters` (object): 满足 JSON Schema Draft-07 的参数定义，用于指导模型正确组装入参\n",
    "    - `type`: 通常为 `object`\n",
    "    - `properties`: 各字段的类型与描述\n",
    "    - `required`: 必填字段名列表\n",
    "    - `additionalProperties`: 是否允许未声明字段（建议 `false` 以提高鲁棒性）\n",
    "\n",
    "示例（与本笔记 `Action` 转换保持一致）：\n",
    "```json\n",
    "{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"read_project_file\",\n",
    "    \"description\": \"Reads a file from the project.\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"name\": { \"type\": \"string\", \"description\": \"The file path to read\" }\n",
    "      },\n",
    "      \"required\": [\"name\"],\n",
    "      \"additionalProperties\": false\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "模型返回时会在 `message.tool_calls` 中给出调用的 `function.name` 与 `function.arguments`（JSON 字符串）。你的代码需：\n",
    "- 解析 `arguments`（`json.loads`）\n",
    "- 路由到本地实现函数执行\n",
    "- 将执行结果写回对话历史（便于下一轮推理）\n",
    "\n",
    "可选高级配置（按实际接口版本支持情况使用）：\n",
    "- `tool_choice`: 强制使用某个工具或允许模型自由选择\n",
    "- `parallel_tool_calls`: 是否允许并行调用（若可用）\n",
    "- `response_format`: 强制 JSON 输出等（如需要结构化）\n",
    "\n",
    "参考文档（官方）：\n",
    "- OpenAI 工具/函数调用总览（Chat Completions）: [Function calling & tools](https://platform.openai.com/docs/guides/function-calling)\n",
    "- JSON Schema 规范（参考）: [JSON Schema](https://json-schema.org/)\n",
    "- Chat Completions 消息与工具调用字段说明: [Chat Completions API](https://platform.openai.com/docs/api-reference/chat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8WI3so-YCs-"
   },
   "source": [
    "# GAME框架：AI智能体设计架构\n",
    "\n",
    "本笔记实现了一个“可复用的智能体（Agent）最小框架”，用来演示如何基于 GAME 设计法将智能体拆分为四个核心部件，并以可插拔方式组织：\n",
    "\n",
    "- **G（Goals / Instructions）**：目标与指令。描述智能体要实现的结果，以及实现策略/规则。\n",
    "- **A（Actions）**：动作/工具。定义智能体可以调用的能力（如读取文件、列目录、结束会话等）。\n",
    "- **M（Memory）**：记忆。跨回合保留上下文（用户输入、助手决策、工具执行结果），支持后续回合继续推理。\n",
    "- **E（Environment）**：环境。动作在真实世界中的执行载体，负责真正“落地执行”动作并返回结果（含时间戳与错误信息）。\n",
    "\n",
    "本框架通过一个统一的 **Agent** 循环（Loop）把 G/A/M/E 串起来：\n",
    "1. 构造 Prompt（包含 Goals、可用 Actions 的函数调用Schema、Memory 历史）。\n",
    "2. 发送给 LLM，得到“选择的动作以及参数”（函数调用）。」\n",
    "3. 在 **Environment** 中执行该动作，得到结果（或错误）。\n",
    "4. 将决策与结果写入 **Memory**，进入下一轮。\n",
    "5. 如果动作为终止类动作（如 `terminate`），则结束循环。\n",
    "\n",
    "你可以把 **Actions** 看成“能力接口”，把 **Environment** 看成“执行实现”。这种解耦使得：\n",
    "- 你可以替换不同环境（本地、云端、GitHub Actions、容器等），而无需修改智能体决策逻辑；\n",
    "- 你可以更换一组 Actions（比如从文件工具换成 Web API 工具），而无需修改主循环；\n",
    "- 你可以替换/扩展 **AgentLanguage**（Prompt 格式与解析逻辑），以适配“函数调用/纯文本解析”等不同LLM交互方式。\n",
    "\n",
    "本笔记下半部分提供了一个最小示例：\n",
    "- 定义了 3 个动作：`list_project_files`、`read_project_file`、`terminate`\n",
    "- 目标：读取项目文件并在结束时输出 README 内容（示例运行环境为空目录时会直接终止）\n",
    "- 使用 OpenAI 调用 `gpt-4o`，但可轻松替换为任意 LLM 提供商\n",
    "\n",
    "### GAME 智能体业务架构\n",
    "```mermaid\n",
    "graph TB\n",
    "    subgraph \"初始化阶段\"\n",
    "        A[设置环境变量] --> B[OpenAI API Key]\n",
    "        A --> C[OpenAI Base URL]\n",
    "    end\n",
    "    \n",
    "    subgraph \"核心组件定义 - GAME\"\n",
    "        D[Goals 目标定义] --> |包含优先级/名称/描述| D1[Goal对象]\n",
    "        E[Actions 动作定义] --> |包含函数/描述/参数Schema| E1[Action对象]\n",
    "        F[Memory 记忆管理] --> |存储user/assistant/environment消息| F1[Memory对象]\n",
    "        G[Environment 环境执行] --> |执行动作/捕获异常/格式化结果| G1[Environment对象]\n",
    "    end\n",
    "    \n",
    "    subgraph \"动作注册\"\n",
    "        E1 --> H[ActionRegistry]\n",
    "        H --> H1[list_project_files]\n",
    "        H --> H2[read_project_file]\n",
    "        H --> H3[terminate]\n",
    "    end\n",
    "    \n",
    "    subgraph \"语言适配层\"\n",
    "        I[AgentLanguage] --> J[AgentFunctionCallingActionLanguage]\n",
    "        J --> J1[构造Prompt: Goals+Memory+Tools]\n",
    "        J --> J2[解析Response: 提取工具调用]\n",
    "    end\n",
    "    \n",
    "    subgraph \"Agent 主控制器\"\n",
    "        K[Agent实例化] --> |注入G/A/M/E| L[Agent.run方法]\n",
    "    end\n",
    "    \n",
    "    B --> K\n",
    "    C --> K\n",
    "    D1 --> K\n",
    "    H --> K\n",
    "    F1 --> K\n",
    "    G1 --> K\n",
    "    J --> K\n",
    "    \n",
    "    L --> M[执行Agent循环]\n",
    "    M --> N[返回最终Memory]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iLUK5fflYeVS",
    "outputId": "58b33400-98cd-4258-e806-5c6e2f29544d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple',\n",
       " 'Requirement already satisfied: openai==1.107.0 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (1.107.0)',\n",
       " 'Requirement already satisfied: anyio<5,>=3.5.0 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from openai==1.107.0) (4.11.0)',\n",
       " 'Requirement already satisfied: distro<2,>=1.7.0 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from openai==1.107.0) (1.9.0)',\n",
       " 'Requirement already satisfied: httpx<1,>=0.23.0 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from openai==1.107.0) (0.28.1)',\n",
       " 'Requirement already satisfied: jiter<1,>=0.4.0 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from openai==1.107.0) (0.11.0)',\n",
       " 'Requirement already satisfied: pydantic<3,>=1.9.0 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from openai==1.107.0) (2.11.9)',\n",
       " 'Requirement already satisfied: sniffio in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from openai==1.107.0) (1.3.1)',\n",
       " 'Requirement already satisfied: tqdm>4 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from openai==1.107.0) (4.67.1)',\n",
       " 'Requirement already satisfied: typing-extensions<5,>=4.11 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from openai==1.107.0) (4.14.1)',\n",
       " 'Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.107.0) (1.3.0)',\n",
       " 'Requirement already satisfied: idna>=2.8 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.107.0) (3.10)',\n",
       " 'Requirement already satisfied: certifi in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.107.0) (2025.10.5)',\n",
       " 'Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.107.0) (1.0.9)',\n",
       " 'Requirement already satisfied: h11>=0.16 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.107.0) (0.16.0)',\n",
       " 'Requirement already satisfied: annotated-types>=0.6.0 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.107.0) (0.7.0)',\n",
       " 'Requirement already satisfied: pydantic-core==2.33.2 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.107.0) (2.33.2)',\n",
       " 'Requirement already satisfied: typing-inspection>=0.4.0 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.107.0) (0.4.2)',\n",
       " \"\\x1b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\\x1b[0m\\x1b[33m\",\n",
       " '\\x1b[0m']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 安装必要的依赖包\n",
    "!!pip install openai==1.107.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEYrzG2vB8Ip",
    "outputId": "034e18df-4c10-43ed-d1e9-e914e409be3b"
   },
   "outputs": [],
   "source": [
    "# 导入必要的模块\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    设置环境变量的辅助函数\n",
    "\n",
    "    参数:\n",
    "        var (str): 要设置的环境变量名称\n",
    "\n",
    "    功能:\n",
    "        - 检查环境变量是否已存在\n",
    "        - 如果不存在，则提示用户输入并设置\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):  # 检查环境变量是否已设置\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")  # 安全地获取用户输入\n",
    "\n",
    "# 设置 OpenAI API 密钥\n",
    "# 这是使用 OpenAI 模型所必需的\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Mwe2eeOQB0cC"
   },
   "outputs": [],
   "source": [
    "# =============================== 核心框架：导入与类型定义 ===============================\n",
    "# 说明：以下代码实现了一个最小可复用的智能体框架（面向函数调用工具）。\n",
    "# - 不修改任何原有逻辑，仅通过中文注释解释设计意图与用法。\n",
    "# - 关键模块：Prompt 数据结构、LLM 响应函数、Goal/Action/ActionRegistry、Memory、Environment、AgentLanguage、Agent。\n",
    "\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "from openai import OpenAI # 用于调用OpenAI API\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Callable, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# 大语言模型\n",
    "client=OpenAI(\n",
    "    base_url=os.environ['OPENAI_BASE_URL'],\n",
    "    api_key=os.environ['OPENAI_API_KEY']\n",
    ")\n",
    "\n",
    "# Prompt：封装要发给 LLM 的消息与工具定义\n",
    "# - messages：对话上下文（系统/用户/助手三类）\n",
    "# - tools：工具（函数）调用的 JSON Schema 描述（让 LLM 能“看见”可用的动作）\n",
    "# - metadata：元数据（可选扩展，用 dict 保存）\n",
    "@dataclass\n",
    "class Prompt:\n",
    "    messages: List[Dict] = field(default_factory=list)\n",
    "    tools: List[Dict] = field(default_factory=list)\n",
    "    metadata: dict = field(default_factory=dict)\n",
    "\n",
    "\n",
    "# generate_response：统一的 LLM 调用入口\n",
    "# - 入参是 Prompt，内部自动根据是否提供 tools 来决定是否启用函数调用能力\n",
    "# - 目标：把模型提供商与主循环解耦；将来切换模型时无需改 Agent 逻辑\n",
    "# - 返回：\n",
    "#   * 无工具时：直接返回助手文本\n",
    "#   * 有工具时：优先解析 tool_calls（并转为 {tool, args} 的 JSON 字符串）\n",
    "#               若无工具调用，则退化为普通文本回复\n",
    "def generate_response(prompt: Prompt) -> str:\n",
    "    \"\"\"调用大语言模型（LLM）生成响应：\n",
    "    - 当未提供 tools（函数调用能力）时，作为普通对话返回文本\n",
    "    - 当提供 tools 时，优先解析函数调用的结构化结果；若无函数调用则退化为普通文本\n",
    "    \"\"\"\n",
    "\n",
    "    messages = prompt.messages\n",
    "    tools = prompt.tools\n",
    "\n",
    "    result = None\n",
    "\n",
    "    if not tools:\n",
    "        # 无工具：普通对话\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",   # 指定使用的模型\n",
    "            messages=messages,  # 发送消息历史\n",
    "            max_tokens=1024   # 限制响应长度\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "    else:\n",
    "        # 有工具：提示模型按函数调用格式返回 tool_calls\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            # 这里仅取第一个工具调用作为最小可运行演示\n",
    "            tool = response.choices[0].message.tool_calls[0]\n",
    "            result = {\n",
    "                \"tool\": tool.function.name,\n",
    "                \"args\": json.loads(tool.function.arguments),\n",
    "            }\n",
    "            # 将 dict 序列化为字符串，便于统一处理与存入记忆\n",
    "            result = json.dumps(result)\n",
    "        else:\n",
    "            # 即使提供了 tools，也可能返回纯文本（例如模型策略判断不调用工具）\n",
    "            result = response.choices[0].message.content\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Goal：目标对象\n",
    "# - priority：目标优先级（便于排序/裁剪）\n",
    "# - name/description：目标名称和详细说明（同时涵盖“要做什么/如何做”）\n",
    "# 使用 @dataclass 装饰器定义 Goal 为一个不可变的数据类（frozen=True），这意味着其实例一旦创建，其属性值就不能被修改，有助于保证数据安全和可靠性。\n",
    "@dataclass(frozen=True)\n",
    "class Goal:\n",
    "    priority: int\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "# Action：动作/工具的抽象\n",
    "# - name：动作名（作为工具名暴露给 LLM）\n",
    "# - function：实际执行的 Python 函数\n",
    "# - description：工具说明，帮助 LLM 选择正确工具\n",
    "# - parameters：JSON Schema（决定 LLM 该如何拼好参数）\n",
    "# - terminal：是否为“终止型”动作（被选中后终止主循环）\n",
    "class Action:\n",
    "    def __init__(self,\n",
    "                 name: str,\n",
    "                 function: Callable,\n",
    "                 description: str,\n",
    "                 parameters: Dict,\n",
    "                 terminal: bool = False):\n",
    "        self.name = name\n",
    "        self.function = function\n",
    "        self.description = description\n",
    "        self.terminal = terminal\n",
    "        self.parameters = parameters\n",
    "\n",
    "    def execute(self, **args) -> Any:\n",
    "        \"\"\"执行该动作所绑定的底层函数，参数通过关键字形式解包传入\"\"\"\n",
    "        # 解包参数并调用底层实现函数\n",
    "        return self.function(**args)\n",
    "\n",
    "\n",
    "# ActionRegistry：动作/工具注册表\n",
    "# - 负责集中管理动作/工具对象，支持按名称检索与批量导出供 AgentLanguage 生成工具Schema\n",
    "class ActionRegistry:\n",
    "    def __init__(self):\n",
    "        self.actions = {}\n",
    "\n",
    "    def register(self, action: Action):\n",
    "        self.actions[action.name] = action\n",
    "\n",
    "    def get_action(self, name: str) -> [Action, None]:\n",
    "        return self.actions.get(name, None)\n",
    "\n",
    "    def get_actions(self) -> List[Action]:\n",
    "        \"\"\"获取所有已注册的动作，按注册顺序返回列表\"\"\"\n",
    "        return list(self.actions.values())\n",
    "\n",
    "\n",
    "# Memory：回合记忆\n",
    "# - items：统一存储“用户/助手/环境”等事件，形成对话历史\n",
    "# - 通过 get_memories 提供最近N条消息给提示构造使用\n",
    "# - 通过 copy_without_system_memories 可过滤掉系统消息（某些场景需要）\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.items = []  # Basic conversation histor\n",
    "\n",
    "    def add_memory(self, memory: dict):\n",
    "        \"\"\"将一条记忆事件追加到工作记忆，用于后续提示词构造与推理\"\"\"\n",
    "        self.items.append(memory)\n",
    "\n",
    "    def get_memories(self, limit: int = None) -> List[Dict]:\n",
    "        \"\"\"获取用于提示词的对话历史；可通过 limit 限制条数以控制上下文长度\"\"\"\n",
    "        return self.items[:limit]\n",
    "\n",
    "    def copy_without_system_memories(self):\n",
    "        \"\"\"返回一份不包含系统类型（type==system）记忆的副本，用于部分提示场景\"\"\"\n",
    "        filtered_items = [m for m in self.items if m[\"type\"] != \"system\"]\n",
    "        memory = Memory()\n",
    "        memory.items = filtered_items\n",
    "        return memory\n",
    "\n",
    "\n",
    "# Environment：环境层（动作的真实执行者）\n",
    "# - execute_action：捕获执行异常，统一返回结构（是否执行成功/错误/traceback/时间戳）\n",
    "# - format_result：为成功结果补充元数据（时间戳），便于记录与日志化\n",
    "class Environment:\n",
    "    def execute_action(self, action: Action, args: dict) -> dict:\n",
    "        \"\"\"执行指定动作并返回标准化结果；捕获异常并提供错误与追踪信息\"\"\"\n",
    "        try:\n",
    "            result = action.execute(**args)\n",
    "            return self.format_result(result)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"tool_executed\": False,\n",
    "                \"error\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "\n",
    "    def format_result(self, result: Any) -> dict:\n",
    "        \"\"\"为执行结果补充元数据（如时间戳）并统一为标准结构\"\"\"\n",
    "        return {\n",
    "            \"tool_executed\": True,\n",
    "            \"result\": result,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        }\n",
    "\n",
    "\n",
    "# AgentLanguage：语言适配层\n",
    "# - 负责把（Goals/Actions/Memory）格式化为 LLM 需要的 Prompt\n",
    "# - 负责从 LLM 的原始输出中解析出“要调用的工具与参数”\n",
    "class AgentLanguage:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def construct_prompt(self,\n",
    "                         actions: List[Action],\n",
    "                         environment: Environment,\n",
    "                         goals: List[Goal],\n",
    "                         memory: Memory) -> Prompt:\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "\n",
    "    def parse_response(self, response: str) -> dict:\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "\n",
    "\n",
    "# AgentFunctionCallingActionLanguage：基于“函数调用”范式的语言适配实现\n",
    "# - 将 Goals 拼接为 system 消息\n",
    "# - 将 Memory 规范化映射为 user/assistant 消息\n",
    "# - 将 Actions 转换为符合 OpenAI 函数调用的 tools Schema\n",
    "class AgentFunctionCallingActionLanguage(AgentLanguage):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def format_goals(self, goals: List[Goal]) -> List:\n",
    "        # 把所有目标拼接为一个 system 消息，便于集中表达“要做什么/如何做”\n",
    "        sep = \"\\n-------------------\\n\"\n",
    "        goal_instructions = \"\\n\\n\".join([f\"{goal.name}:{sep}{goal.description}{sep}\" for goal in goals])\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": goal_instructions}\n",
    "        ]\n",
    "\n",
    "    def format_memory(self, memory: Memory) -> List:\n",
    "        \"\"\"将 Memory 转换为对话消息格式，供 LLM 上下文使用\"\"\"\n",
    "        # 记忆格式化策略：\n",
    "        # - environment 的输出也作为 assistant 角色加入（让模型能“看到”工具执行结果）\n",
    "        # - user/assistant 原样映射\n",
    "        items = memory.get_memories()\n",
    "        mapped_items = []\n",
    "        for item in items:\n",
    "\n",
    "            content = item.get(\"content\", None)\n",
    "            if not content:\n",
    "                content = json.dumps(item, indent=4)\n",
    "\n",
    "            if item[\"type\"] == \"assistant\":\n",
    "                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
    "            elif item[\"type\"] == \"environment\":\n",
    "                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
    "            else:\n",
    "                mapped_items.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "        return mapped_items\n",
    "\n",
    "    def format_actions(self, actions: List[Action]) -> [List,List]:\n",
    "        \"\"\"将已注册的动作转换为 OpenAI 函数调用所需的 tools Schema\"\"\"\n",
    "\n",
    "        # 将注册的 Action 转为 OpenAI 函数调用工具的 Schema 数组\n",
    "        tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": action.name,\n",
    "                    # 描述过长可能无效，限制到 1024 字符\n",
    "                    \"description\": action.description[:1024],\n",
    "                    \"parameters\": action.parameters,\n",
    "                },\n",
    "            } for action in actions\n",
    "        ]\n",
    "\n",
    "        return tools\n",
    "\n",
    "    def construct_prompt(self,\n",
    "                         actions: List[Action],\n",
    "                         environment: Environment,\n",
    "                         goals: List[Goal],\n",
    "                         memory: Memory) -> Prompt:\n",
    "\n",
    "        # 构造最终 Prompt：Goals（system）+ Memory（历史消息）+ Tools（函数Schema）\n",
    "        prompt = []\n",
    "        prompt += self.format_goals(goals)\n",
    "        prompt += self.format_memory(memory)\n",
    "\n",
    "        tools = self.format_actions(actions)\n",
    "\n",
    "        return Prompt(messages=prompt, tools=tools)\n",
    "\n",
    "    def adapt_prompt_after_parsing_error(self,\n",
    "                                         prompt: Prompt,\n",
    "                                         response: str,\n",
    "                                         traceback: str,\n",
    "                                         error: Any,\n",
    "                                         retries_left: int) -> Prompt:\n",
    "        # 解析失败后的“自适应 Prompt”策略（此处保留扩展点，演示版不做修改）\n",
    "        return prompt\n",
    "\n",
    "    def parse_response(self, response: str) -> dict:\n",
    "        \"\"\"将 LLM 的响应解析为结构化格式（优先尝试 JSON 解析，失败则回退为终止工具）\"\"\"\n",
    "\n",
    "        # 期望 LLM 返回 JSON 字符串：{\"tool\": 工具名, \"args\": {...}}\n",
    "        try:\n",
    "            return json.loads(response)\n",
    "\n",
    "        except Exception as e:\n",
    "            # 若无法解析，则将内容作为 message 交给终止工具，友好退出\n",
    "            return {\n",
    "                \"tool\": \"terminate\",\n",
    "                \"args\": {\"message\":response}\n",
    "            }\n",
    "\n",
    "\n",
    "# Agent：智能体主循环\n",
    "# - 维护并协调 G/A/M/E（目标/动作/记忆/环境）\n",
    "# - 统一的 prompt 构造、响应解析、动作执行、记忆更新、终止判断\n",
    "class Agent:\n",
    "    def __init__(self,\n",
    "                 goals: List[Goal],\n",
    "                 agent_language: AgentLanguage,\n",
    "                 action_registry: ActionRegistry,\n",
    "                 generate_response: Callable[[Prompt], str],\n",
    "                 environment: Environment):\n",
    "        \"\"\"\n",
    "        使用核心的 GAME 组件初始化智能体：\n",
    "        - goals：目标与指令集合\n",
    "        - agent_language：语言适配层（提示词构造与解析）\n",
    "        - generate_response：LLM 调用函数\n",
    "        - environment：动作执行环境\n",
    "        \"\"\"\n",
    "        self.goals = goals\n",
    "        self.generate_response = generate_response\n",
    "        self.agent_language = agent_language\n",
    "        self.actions = action_registry\n",
    "        self.environment = environment\n",
    "\n",
    "    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\n",
    "        \"\"\"基于当前目标、记忆与动作集合构造提示词（Prompt）\"\"\"\n",
    "        return self.agent_language.construct_prompt(\n",
    "            actions=actions.get_actions(),\n",
    "            environment=self.environment,\n",
    "            goals=goals,\n",
    "            memory=memory\n",
    "        )\n",
    "\n",
    "    def get_action(self, response):\n",
    "        # 解析 LLM 的返回，得到动作名与参数（invocation）\n",
    "        invocation = self.agent_language.parse_response(response)\n",
    "        action = self.actions.get_action(invocation[\"tool\"])\n",
    "        return action, invocation\n",
    "\n",
    "    def should_terminate(self, response: str) -> bool:\n",
    "        # 若当前选择的动作被标记为 terminal，则结束主循环\n",
    "        action_def, _ = self.get_action(response)\n",
    "        return action_def.terminal\n",
    "\n",
    "    def set_current_task(self, memory: Memory, task: str):\n",
    "        # 将用户输入写入记忆，作为本轮起始任务语境\n",
    "        memory.add_memory({\"type\": \"user\", \"content\": task})\n",
    "\n",
    "    def update_memory(self, memory: Memory, response: str, result: dict):\n",
    "        \"\"\"\n",
    "        使用“决策 + 执行结果”更新记忆：\n",
    "        - 将助手的决策（response）作为 assistant 事件存入\n",
    "        - 将环境执行结果（result）序列化为 JSON，作为 environment 事件存入\n",
    "        \"\"\"\n",
    "        # 统一把“助手的决策（response）”与“环境执行结果（result）”写入记忆\n",
    "        new_memories = [\n",
    "            {\"type\": \"assistant\", \"content\": response},\n",
    "            {\"type\": \"environment\", \"content\": json.dumps(result)}\n",
    "        ]\n",
    "        for m in new_memories:\n",
    "            memory.add_memory(m)\n",
    "\n",
    "    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\n",
    "        # 将 Prompt 发送给 LLM，得到“下一步动作/或文本回复”\n",
    "        response = self.generate_response(full_prompt)\n",
    "        return response\n",
    "\n",
    "    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\n",
    "        \"\"\"\n",
    "        执行该智能体的 GAME 主循环，可设置最大迭代次数：\n",
    "        - 每轮：构造 Prompt -> 让 LLM 决策 -> 解析动作 -> 环境执行 -> 写回记忆 -> 终止判断\n",
    "        \"\"\"\n",
    "        # 初始化记忆并写入用户任务\n",
    "        memory = memory or Memory()\n",
    "        self.set_current_task(memory, user_input)\n",
    "\n",
    "        for _ in range(max_iterations):\n",
    "            # 1) 用当前 Goals/Actions/Memory 构造 Prompt\n",
    "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
    "\n",
    "            print(\"Agent thinking...\")\n",
    "            # 2) 发送给 LLM，得到“将要调用的动作及其参数”或普通文本\n",
    "            response = self.prompt_llm_for_action(prompt)\n",
    "            print(f\"Agent Decision: {response}\")\n",
    "\n",
    "            # 3) 解析动作与参数\n",
    "            action, invocation = self.get_action(response)\n",
    "\n",
    "            # 4) 在环境中真实执行动作\n",
    "            result = self.environment.execute_action(action, invocation[\"args\"])\n",
    "            print(f\"Action Result: {result}\")\n",
    "\n",
    "            # 5) 将“决策 + 结果”写回记忆，形成闭环\n",
    "            self.update_memory(memory, response, result)\n",
    "\n",
    "            # 6) 终止判断：如果动作为终止型，则跳出循环\n",
    "            if self.should_terminate(response):\n",
    "                break\n",
    "\n",
    "        return memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6PC3ncxezoJC",
    "outputId": "4de6df40-b724-42c9-95c9-8525ddb9412f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent thinking...\n",
      "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
      "Action Result: {'tool_executed': True, 'result': ['mcp-demo/client/mcp_client.py', 'mcp-demo/client/mcp_client_deepseek.py', 'mcp-demo/client/mcp_client_langchain_chat.py', 'mcp-demo/server/weather_server.py'], 'timestamp': '2025-11-09T00:16:12+0800'}\n",
      "Agent thinking...\n",
      "Agent Decision: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"mcp-demo/client/mcp_client.py\"}}\n",
      "Action Result: {'tool_executed': True, 'result': '#!/usr/bin/env python3\\n# -*- coding: utf-8 -*-\\n\\n\"\"\"\\nMCP 客户端示例\\n\\n用于与 MCP 服务器交互，调用工具并处理结果\\n\\nAuthor: FlyAIBox\\nDate: 2025.10.11\\n\"\"\"\\n\\nimport asyncio\\nimport json\\nimport os\\nimport subprocess\\nimport sys\\nfrom typing import Dict, Any, List, Optional\\nfrom contextlib import AsyncExitStack\\n\\n# 使用正确的 MCP 导入\\nfrom mcp import ClientSession, StdioServerParameters, Tool\\nfrom mcp.client.stdio import stdio_client\\n\\nclass SimpleClientApp:\\n    \"\"\"简单的 MCP 客户端应用\"\"\"\\n    \\n    def __init__(self, server_command: List[str]):\\n        \"\"\"\\n        初始化 MCP 客户端应用\\n        \\n        参数:\\n            server_command: 启动服务器的命令列表\\n        \"\"\"\\n        self.server_command = server_command\\n        self.server_process = None\\n        self.client = None\\n        self.tool_definitions = []\\n        self.exit_stack = AsyncExitStack()\\n    \\n    async def start(self):\\n        \"\"\"启动 MCP 客户端并连接到服务器\"\"\"\\n        print(\"启动 MCP 服务器进程...\")\\n        \\n        # 配置服务器参数\\n        server_params = StdioServerParameters(\\n            command=self.server_command[0],\\n            args=self.server_command[1:],\\n            env=None  # 使用默认环境变量\\n        )\\n        \\n        # 启动服务器并获取通信流\\n        # 通过 stdio_client 协程，以异步方式建立与服务器进程的标准输入/输出通道\\n        # enter_async_context 可以确保后续资源能自动释放（类似 try...finally）\\n        # stdio_client(server_params) 会启动子进程并返回 (reader, writer) 的元组\\n        read_write = await self.exit_stack.enter_async_context(stdio_client(server_params))\\n        # 解包得到 reader（读取服务器输出）、writer（写入服务器输入），用于后续通信\\n        # read_write 是一个包含两个元素的元组，分别对应于与服务器通信的 “读取” (reader) 和 “写入” (writer) 流对象。\\n        read, write = read_write\\n        \\n        # 创建 MCP 客户端会话\\n        self.client = await self.exit_stack.enter_async_context(ClientSession(read, write))\\n        \\n        # 初始化连接\\n        await self.client.initialize()\\n        \\n        # 获取工具定义\\n        response = await self.client.list_tools()\\n        self.tool_definitions = response.tools\\n        \\n        print(f\"已连接到服务器，可用工具: {len(self.tool_definitions)}\")\\n        \\n        for tool in self.tool_definitions:\\n            print(f\"  - {tool.name}: {tool.description}\")\\n        \\n        print(\"\\\\n使用 \\'help\\' 查看帮助，使用 \\'exit\\' 退出\")\\n    \\n    async def execute_tool(self, tool_name: str, params: Dict[str, Any]) -> Optional[str]:\\n        \"\"\"\\n        执行工具调用\\n        \\n        参数:\\n            tool_name: 要调用的工具名称\\n            params: 工具参数字典\\n            \\n        返回:\\n            工具执行结果或错误消息\\n        \"\"\"\\n        # 检查工具是否存在\\n        # 遍历工具定义列表，查找名称与 tool_name 匹配的工具对象\\n        # next() 用于返回第一个匹配的工具（如未找到则返回 None）\\n        tool_def = next((t for t in self.tool_definitions if t.name == tool_name), None)\\n        if not tool_def:\\n            return f\"错误: 未找到工具 \\'{tool_name}\\'\"\\n        \\n        try:\\n            # 调用工具并等待结果\\n            result = await self.client.call_tool(tool_name, arguments=params)\\n            \\n            # 处理结果\\n            if result and hasattr(result, \\'content\\'):\\n                return result.content\\n            else:\\n                return \"工具执行未返回任何结果\"\\n        \\n        except Exception as e:\\n            return f\"执行过程中出错: {str(e)}\"\\n    \\n    async def stop(self):\\n        \"\"\"停止客户端和服务器\"\"\"\\n        print(\"正在关闭 MCP 客户端...\")\\n        await self.exit_stack.aclose()\\n        print(\"已关闭\")\\n    \\n    def print_help(self):\\n        \"\"\"打印帮助信息\"\"\"\\n        print(\"\\\\n可用命令:\")\\n        print(\"  help - 显示此帮助信息\")\\n        print(\"  list - 列出可用工具\")\\n        print(\"  call <工具名> <参数JSON> - 调用工具\")\\n        print(\"  exit - 退出程序\")\\n        print(\"\\\\n示例:\")\\n        print(\"  call get_weather_warning {\\\\\"location\\\\\": \\\\\"101010100\\\\\"}\")\\n        print(\"  call get_daily_forecast 116.41,39.92\")\\n        print(\"  call get_daily_forecast 101010100 7\")\\n    \\n    def print_tools(self):\\n        \"\"\"打印工具列表和描述\"\"\"\\n        print(\"\\\\n可用工具:\")\\n        for tool in self.tool_definitions:\\n            print(f\"  {tool.name} - {tool.description}\")\\n            if hasattr(tool, \\'inputSchema\\') and tool.inputSchema:\\n                print(\"    参数:\")\\n                props = tool.inputSchema.get(\\'properties\\', {})\\n                for param_name, param_info in props.items():\\n                    param_type = param_info.get(\\'type\\', \\'unknown\\')\\n                    param_desc = param_info.get(\\'description\\', \\'\\')\\n                    print(f\"      {param_name} ({param_type}): {param_desc}\")\\n\\nasync def main():\\n    \"\"\"主函数\"\"\"\\n    # 确定服务器路径\\n    server_path = os.path.join(\\n        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\\n        \"server\",\\n        \"weather_server.py\"\\n    )\\n    \\n    # 检查服务器文件是否存在\\n    if not os.path.exists(server_path):\\n        print(f\"错误: 服务器文件不存在: {server_path}\")\\n        return\\n    \\n    # 创建客户端应用\\n    app = SimpleClientApp([\"python\", server_path])\\n    \\n    try:\\n        # 启动客户端\\n        await app.start()\\n        \\n        # 主循环\\n        while True:\\n            try:\\n                # 获取用户输入\\n                command = input(\"\\\\n> \").strip()\\n                \\n                # 处理命令\\n                if command == \"exit\":\\n                    break\\n                \\n                elif command == \"help\":\\n                    app.print_help()\\n                \\n                elif command == \"list\":\\n                    app.print_tools()\\n                \\n                elif command.startswith(\"call \"):\\n                    # 解析命令\\n                    parts = command[5:].strip().split(\" \", 1)\\n                    if len(parts) < 1:\\n                        print(\"错误: 缺少工具名\")\\n                        continue\\n                    \\n                    tool_name = parts[0]\\n                    \\n                    # 解析参数\\n                    params = {}\\n                    if len(parts) > 1:\\n                        params_str = parts[1]\\n                        \\n                        # 尝试解析为 JSON\\n                        try:\\n                            params = json.loads(params_str)\\n                        except json.JSONDecodeError:\\n                            # 如果不是 JSON，尝试简单解析\\n                            try:\\n                                # 对于 get_weather_warning，只需要一个位置参数\\n                                if tool_name == \"get_weather_warning\":\\n                                    params = {\"location\": params_str}\\n                                \\n                                # 对于 get_daily_forecast，需要位置和可选的天数\\n                                elif tool_name == \"get_daily_forecast\":\\n                                    forecast_args = params_str.split()\\n                                    if len(forecast_args) < 1:\\n                                        print(\"错误: get_daily_forecast 至少需要一个位置参数\")\\n                                        continue\\n                                    \\n                                    params = {\"location\": forecast_args[0]}\\n                                    \\n                                    # 如果提供了天数参数\\n                                    if len(forecast_args) > 1:\\n                                        try:\\n                                            params[\"days\"] = int(forecast_args[1])\\n                                        except ValueError:\\n                                            print(\"错误: days 参数必须是整数\")\\n                                            continue\\n                                else:\\n                                    print(f\"错误: 无法解析 {tool_name} 的参数\")\\n                                    continue\\n                            \\n                            except Exception as e:\\n                                print(f\"参数解析错误: {str(e)}\")\\n                                continue\\n                    \\n                    # 执行工具调用\\n                    print(\"正在调用工具...\")\\n                    result = await app.execute_tool(tool_name, params)\\n                    print(\"\\\\n结果:\")\\n                    print(result)\\n                \\n                else:\\n                    print(f\"未知命令: {command}\")\\n                    print(\"使用 \\'help\\' 查看可用命令\")\\n            \\n            except KeyboardInterrupt:\\n                print(\"\\\\n操作被中断\")\\n                break\\n            \\n            except Exception as e:\\n                print(f\"错误: {str(e)}\")\\n    \\n    except Exception as e:\\n        print(f\"客户端错误: {str(e)}\")\\n    \\n    finally:\\n        # 停止客户端\\n        await app.stop()\\n\\nif __name__ == \"__main__\":\\n    try:\\n        asyncio.run(main())\\n    except KeyboardInterrupt:\\n        print(\"\\\\n程序被中断\")\\n        sys.exit(0) ', 'timestamp': '2025-11-09T00:16:14+0800'}\n",
      "Agent thinking...\n",
      "Agent Decision: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"mcp-demo/client/mcp_client_deepseek.py\"}}\n",
      "Action Result: {'tool_executed': True, 'result': '\"\"\"\\n# 集成 DeepSeek 的 MCP 客户端\\n#参考官方案例：https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-chatbot/mcp_simple_chatbot/main.py\\n\\n\\n# 本模块实现了一个模型上下文协议（MCP）客户端，该客户端使用 DeepSeek 的 API\\n# 来处理查询并与 MCP 工具进行交互。它演示了如何：\\n# 1. 连接到 MCP 服务器\\n# 2. 使用 DeepSeek 的 API 来处理查询\\n# 3. 处理工具调用和响应\\n# 4. 维护一个交互式聊天循环\\n\\n# 所需环境变量：\\n# DEEPSEEK_API_KEY：DeepSeek API 密钥 (格式：sk-xxxx...)\\n# DEEPSEEK_BASE_URL：DeepSeek API 基础 URL (https://api.deepseek.com)\\n# DEEPSEEK_MODEL：DeepSeek 模型名称 (例如 deepseek-chat)\\n\\nAuthor: FlyAIBox\\nDate: 2025.10.11\\n\"\"\"\\n\\nimport json\\nimport asyncio\\nimport logging\\nimport os\\nfrom typing import Optional, Dict, Any, List, Tuple\\nfrom contextlib import AsyncExitStack\\n\\nfrom openai import OpenAI\\nfrom dotenv import load_dotenv\\n\\nfrom mcp import ClientSession, StdioServerParameters\\nfrom mcp.client.stdio import stdio_client\\n\\n# 配置日志记录\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\\n)\\nlogger = logging.getLogger(__name__)\\n\\nload_dotenv()\\n\\n\\nclass Configuration:\\n    \"\"\"配置管理类，负责管理和验证环境变量\"\"\"\\n    \\n    def __init__(self) -> None:\\n        \"\"\"初始化配置并加载环境变量\"\"\"\\n        self.load_env()\\n        self._validate_env()\\n        \\n    @staticmethod\\n    def load_env() -> None:\\n        \"\"\"从.env文件加载环境变量\"\"\"\\n        load_dotenv()\\n        \\n    def _validate_env(self) -> None:\\n        \"\"\"验证必需的环境变量是否存在\"\"\"\\n        required_vars = [\"DEEPSEEK_API_KEY\"]\\n        missing_vars = [var for var in required_vars if not os.getenv(var)]\\n        if missing_vars:\\n            raise ValueError(f\"缺少必需的环境变量: {\\', \\'.join(missing_vars)}\")\\n    \\n    @property\\n    def api_key(self) -> str:\\n        \"\"\"获取 DeepSeek API 密钥\"\"\"\\n        return os.getenv(\"DEEPSEEK_API_KEY\", \"\")\\n    \\n    @property\\n    def base_url(self) -> str:\\n        \"\"\"获取 DeepSeek API 基础 URL\"\"\"\\n        return os.getenv(\"DEEPSEEK_BASE_URL\", \"https://api.deepseek.com\")\\n    \\n    @property\\n    def model(self) -> str:\\n        \"\"\"获取 DeepSeek 模型名称\"\"\"\\n        return os.getenv(\"DEEPSEEK_MODEL\", \"deepseek-chat\")\\n\\nclass Tool:\\n    \"\"\"MCP 工具类，表示一个具有属性的工具\"\"\"\\n    \\n    def __init__(self, name: str, description: str, input_schema: Dict[str, Any]) -> None:\\n        \"\"\"\\n        初始化工具\\n        \\n        Args:\\n            name: 工具名称\\n            description: 工具描述\\n            input_schema: 输入参数模式\\n        \"\"\"\\n        self.name = name\\n        self.description = description\\n        self.input_schema = input_schema\\n        \\n    def to_openai_format(self) -> Dict[str, Any]:\\n        \"\"\"将工具转换为 OpenAI API 格式\"\"\"\\n        return {\\n            \"type\": \"function\",\\n            \"function\": {\\n                \"name\": self.name,\\n                \"description\": self.description,\\n                \"parameters\": self.input_schema\\n            }\\n        }\\n\\nclass MCPServer:\\n    \"\"\"MCP 服务器管理类，处理服务器连接和工具执行\"\"\"\\n    \\n    def __init__(self, server_path: str) -> None:\\n        \"\"\"\\n        初始化服务器管理器\\n        \\n        Args:\\n            server_path: 服务器脚本路径\\n        \"\"\"\\n        self.server_path = server_path\\n        self.session: Optional[ClientSession] = None\\n        self.exit_stack = AsyncExitStack()\\n        self._cleanup_lock = asyncio.Lock()\\n        \\n    async def initialize(self) -> None:\\n        \"\"\"初始化服务器连接，包含重试机制\"\"\"\\n        max_retries = 3\\n        retry_delay = 1.0\\n        \\n        for attempt in range(max_retries):\\n            try:\\n                if not os.path.exists(self.server_path):\\n                    raise FileNotFoundError(f\"找不到服务器文件: {self.server_path}\")\\n                \\n                server_params = StdioServerParameters(\\n                    command=\\'python\\',\\n                    args=[self.server_path],\\n                    env=None\\n                )\\n                \\n                stdio_transport = await self.exit_stack.enter_async_context(\\n                    stdio_client(server_params)\\n                )\\n                stdio, write = stdio_transport\\n                \\n                self.session = await self.exit_stack.enter_async_context(\\n                    ClientSession(stdio, write)\\n                )\\n                await self.session.initialize()\\n                logger.info(\"成功连接到 MCP 服务器\")\\n                break\\n                \\n            except Exception as e:\\n                logger.error(f\"第 {attempt + 1}/{max_retries} 次尝试失败: {str(e)}\")\\n                if attempt < max_retries - 1:\\n                    await asyncio.sleep(retry_delay)\\n                else:\\n                    raise\\n                    \\n    async def list_tools(self) -> List[Tool]:\\n        \"\"\"获取服务器提供的可用工具列表\"\"\"\\n        if not self.session:\\n            raise RuntimeError(\"服务器未初始化\")\\n            \\n        response = await self.session.list_tools()\\n        return [\\n            Tool(tool.name, tool.description, tool.inputSchema)\\n            for tool in response.tools\\n        ]\\n        \\n    async def execute_tool(\\n        self,\\n        tool_name: str,\\n        arguments: Dict[str, Any],\\n        retries: int = 2,\\n        delay: float = 1.0\\n    ) -> Any:\\n        \"\"\"\\n        执行工具，包含重试机制\\n        \\n        Args:\\n            tool_name: 工具名称\\n            arguments: 工具参数\\n            retries: 重试次数\\n            delay: 重试延迟时间（秒）\\n            \\n        Returns:\\n            工具执行结果\\n        \"\"\"\\n        if not self.session:\\n            raise RuntimeError(\"服务器未初始化\")\\n            \\n        for attempt in range(retries):\\n            try:\\n                logger.info(f\"执行工具 {tool_name}，参数: {arguments}\")\\n                result = await self.session.call_tool(tool_name, arguments)\\n                return result\\n                \\n            except Exception as e:\\n                logger.error(f\"工具执行失败 (第 {attempt + 1}/{retries} 次尝试): {str(e)}\")\\n                if attempt < retries - 1:\\n                    await asyncio.sleep(delay)\\n                else:\\n                    raise\\n                    \\n    async def cleanup(self) -> None:\\n        \"\"\"清理服务器资源\"\"\"\\n        async with self._cleanup_lock:\\n            try:\\n                await self.exit_stack.aclose()\\n                self.session = None\\n                logger.info(\"服务器资源清理完成\")\\n            except Exception as e:\\n                logger.error(f\"清理过程中出错: {str(e)}\")\\n\\nclass MCPClient:\\n    \"\"\"MCP 客户端实现，集成了 DeepSeek API\"\"\"\\n    \\n    def __init__(self, config: Configuration) -> None:\\n        \"\"\"\\n        初始化 MCP 客户端\\n        \\n        Args:\\n            config: 配置对象\\n        \"\"\"\\n        self.config = config\\n        self.server: Optional[MCPServer] = None\\n        self.client = OpenAI(\\n            api_key=config.api_key,\\n            base_url=config.base_url\\n        )\\n        \\n    async def initialize(self) -> None:\\n        \"\"\"初始化客户端并连接到服务器\"\"\"\\n        server_path = os.path.join(\\n            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\\n            \"server\",\\n            \"weather_server.py\"\\n        )\\n        self.server = MCPServer(server_path)\\n        await self.server.initialize()\\n        \\n\\n    async def process_query(self, query: str) -> str:\\n        \"\"\"\\n        处理用户查询，集成工具调用，支持多轮工具交互\\n\\n        Args:\\n            query: 用户查询字符串\\n\\n        Returns:\\n            处理后的响应结果\\n        \"\"\"\\n        if not self.server:\\n            raise RuntimeError(\"客户端未初始化\")\\n\\n        system_prompt = (\\n            \"You are a helpful assistant specializing in weather information.\\\\n\"\\n            \"You have access to the MCP Weather Server tool with the following functions:\\\\n\"\\n            \"- get_weather_warning(city_id=None, latitude=None, longitude=None): Retrieves weather disaster warnings for a specified city ID or coordinates.\\\\n\"\\n            \"- get_daily_forecast(city_id=None, latitude=None, longitude=None): Retrieves the multi-day weather forecast for a specified city ID or coordinates.\\\\n\"\\n            \"\\\\n\"\\n            \"Core Instructions:\\\\n\"\\n            \"1.  **Carefully analyze the user\\'s request**: Understand all components of the user\\'s query. Determine if the user needs weather warning information, weather forecast information, or both.\\\\n\"\\n            \"2.  **Identify Information Needs**:\\\\n\"\\n            \"    * If the user only asks for warnings (e.g., \\\\\"Are there any warnings in Beijing?\\\\\"), only use `get_weather_warning`.\\\\n\"\\n            \"    * If the user only asks for the forecast (e.g., \\\\\"What\\'s the weather like in Beijing tomorrow?\\\\\"), only use `get_daily_forecast`.\\\\n\"\\n            \"    * **If the user\\'s question includes multiple aspects**, such as asking about **warning status** and also asking **if it\\'s suitable for a certain activity** (which implies a query about future weather, like \\\\\"Have there been high temperature warnings in Beijing in the last week? Is it suitable for outdoor activities?\\\\\"), you need to **call both tools sequentially**.\\\\n\"\\n            \"3.  **Call Tools as Needed**:\\\\n\"\\n            \"    * **Prioritize getting warning information**: If warning information is needed, first call `get_weather_warning`.\\\\n\"\\n            \"    * **Get the weather forecast**: If the user mentions a specific time period (e.g., \\\\\"weekend\\\\\", \\\\\"next three days\\\\\", \\\\\"next week\\\\\") or asks about activity suitability (which typically concerns the next few days), call `get_daily_forecast` to get the forecast for the corresponding period. For vague phrases like \\\\\"last week\\\\\" or \\\\\"recently\\\\\", interpret it as asking about *current* conditions and the *upcoming* few days (covered by the forecast). For questions like \\\\\"Is it suitable for outdoor activities?\\\\\", you should get the forecast for at least the next 2-3 days (e.g., today, tomorrow, the day after tomorrow, or the upcoming weekend) to support your judgment.\\\\n\"\\n            \"    * **Ensure tool call order**: When multiple tools need to be called, they should be called in a logical sequence. For example, first get the warning, then get the forecast. Wait for one tool to finish executing before deciding whether to call the next tool or generate a response.\\\\n\"\\n            \"4.  **Information Integration and Response**:\\\\n\"\\n            \"    * After obtaining all necessary information (warning, forecast), you **must synthesize and analyze this information**.\\\\n\"\\n            \"    * **Completely answer the user\\'s question**: Ensure you answer all parts of the user\\'s query.\\\\n\"\\n            \"    * **Provide advice**: If the user asks about activity suitability, based on the retrieved warning status and forecast information (temperature, weather condition - clear/rainy, wind strength, etc.), provide a clear, data-supported recommendation (e.g., \\\\\"Currently there are no high temperature warnings, but it\\'s expected to rain this weekend, so it\\'s not very suitable for outdoor activities,\\\\\" or \\\\\"It will be sunny for the next few days with no warnings, suitable for outdoor activities.\\\\\").\\\\n\"\\n            \"5.  **Tool Usage Details**:\\\\n\"\\n            \"    * When using the tools, retain the full context of the user\\'s original question.\\\\n\"\\n            \"    * Unless explicitly requested by the user, do not insert specific times of day (e.g., \\\\\"3 PM\\\\\") into the search query or your response.\\\\n\"\\n            \"    * When city information is needed, if the user provides a city name (e.g., \\\\\"Beijing\\\\\"), use the corresponding `city_id` (e.g., Beijing\\'s city_id might be \\'101010100\\').\\\\n\"\\n        )\\n\\n        messages = [\\n            {\"role\": \"system\", \"content\": system_prompt},\\n            {\"role\": \"user\", \"content\": query}\\n        ]\\n\\n        tools = await self.server.list_tools()\\n        available_tools = [tool.to_openai_format() for tool in tools]\\n\\n        max_tool_turns = 5 # 设置一个最大工具调用回合数，防止无限循环\\n\\n        for _ in range(max_tool_turns):\\n            try:\\n                response = self.client.chat.completions.create(\\n                    model=self.config.model,\\n                    messages=messages,\\n                    tools=available_tools,\\n                    tool_choice=\"auto\" # 允许模型决定是否调用工具\\n                )\\n\\n                content = response.choices[0].message\\n                finish_reason = response.choices[0].finish_reason\\n\\n                if finish_reason == \"stop\":\\n                    # 模型生成了最终的文本回复\\n                    messages.append({\"role\": \"assistant\", \"content\": content.content})\\n                    return content.content\\n                elif finish_reason == \"tool_calls\":\\n                    # 模型预测了工具调用\\n                    messages.append(content.model_dump()) # 将 assistant tool_calls 消息添加到历史\\n\\n                    tool_outputs = []\\n                    for tool_call in content.tool_calls:\\n                        tool_name = tool_call.function.name\\n                        tool_args = json.loads(tool_call.function.arguments)\\n\\n                        try:\\n                            # 执行工具\\n                            result = await self.server.execute_tool(tool_name, tool_args)\\n                            logger.info(f\"工具执行完成: {tool_name}\")\\n                            # 将工具结果添加到 tool_outputs 列表\\n                            tool_outputs.append({\\n                                \"tool_call_id\": tool_call.id,\\n                                # MCP tool results have \\'content\\' which is a list of Parts\\n                                # Assuming text content is the first part\\n                                \"output\": result.content[0].text\\n                            })\\n                        except Exception as tool_exec_error:\\n                            logger.error(f\"工具执行失败 {tool_name}: {str(tool_exec_error)}\")\\n                            # 如果工具执行失败，也需要向模型反馈失败信息\\n                            tool_outputs.append({\\n                                \"tool_call_id\": tool_call.id,\\n                                \"output\": f\"Error executing tool {tool_name}: {str(tool_exec_error)}\"\\n                            })\\n\\n                    # 将所有工具结果作为 tool 消息添加到历史\\n                    # 注意：OpenAI API 要求每个 tool_call_id 对应一个 tool 消息\\n                    # 如果模型预测了多个工具调用，这里需要为每个结果添加 tool 消息\\n                    for output in tool_outputs:\\n                        messages.append({\\n                            \"role\": \"tool\",\\n                            \"content\": output[\"output\"],\\n                            \"tool_call_id\": output[\"tool_call_id\"],\\n                        })\\n\\n                    # 继续下一轮循环，将完整的消息历史发送给模型，让它根据工具结果继续\\n                    logger.info(f\"工具调用回合完成，继续与模型交互...\")\\n                    continue # 继续循环，发送包含工具结果的消息历史给模型\\n                else:\\n                    # 发生未知情况或错误\\n                    logger.error(f\"模型返回意外的 finish_reason: {finish_reason}\")\\n                    return f\"处理你的查询时遇到意外情况 ({finish_reason})\"\\n\\n            except Exception as e:\\n                logger.error(f\"处理查询时出错: {str(e)}\")\\n                return f\"处理你的查询时发生错误: {str(e)}\"\\n\\n        # 如果循环达到最大次数仍未得到最终回复\\n        logger.warning(\"达到最大工具调用回合数，未能获得最终回复\")\\n        return \"抱歉，处理你的请求时出现问题，未能获得最终结果。\"\\n\\n    async def chat_loop(self) -> None:\\n        \"\"\"运行交互式聊天循环\"\"\"\\n        logger.info(\"开始聊天会话...\")\\n        \\n        while True:\\n            try:\\n                query = input(\"\\\\n请输入你的问题 (输入 \\'quit\\' 或 \\'exit\\' 退出): \").strip()\\n                \\n                if query.lower() in [\\'quit\\', \\'exit\\']:\\n                    logger.info(\"结束聊天会话...\")\\n                    break\\n                    \\n                response = await self.process_query(query)\\n                print(f\"\\\\n助手: {response}\")\\n                \\n            except KeyboardInterrupt:\\n                logger.info(\"\\\\n收到键盘中断，结束会话...\")\\n                break\\n            except Exception as e:\\n                logger.error(f\"聊天循环中出错: {str(e)}\")\\n                print(f\"\\\\n发生错误: {str(e)}\")\\n                \\n    async def cleanup(self) -> None:\\n        \"\"\"清理客户端资源\"\"\"\\n        if self.server:\\n            await self.server.cleanup()\\n\\nasync def main() -> None:\\n    \"\"\"主程序入口\"\"\"\\n    try:\\n        config = Configuration()\\n        client = MCPClient(config)\\n        \\n        await client.initialize()\\n        await client.chat_loop()\\n        \\n    except Exception as e:\\n        logger.error(f\"致命错误: {str(e)}\")\\n        raise\\n        \\n    finally:\\n        if \\'client\\' in locals():\\n            await client.cleanup()\\n\\nif __name__ == \"__main__\":\\n    try:\\n        asyncio.run(main())\\n    except KeyboardInterrupt:\\n        logger.info(\"程序被用户终止\")\\n    except Exception as e:\\n        logger.error(f\"程序因错误终止: {str(e)}\")\\n        raise\\n', 'timestamp': '2025-11-09T00:16:16+0800'}\n",
      "Agent thinking...\n",
      "Agent Decision: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"mcp-demo/client/mcp_client_langchain_chat.py\"}}\n",
      "Action Result: {'tool_executed': True, 'result': '\"\"\"\\n# 集成 DeepSeek 的 MCP 客户端[LangChain版]\\n#更详细的使用方法请参考：https://github.com/langchain-ai/langchain-mcp-adapters\\n\\n# 所需环境变量：\\n# DEEPSEEK_API_KEY：DeepSeek API 密钥 (格式：sk-xxxx...)\\n# DEEPSEEK_BASE_URL：DeepSeek API 基础 URL (https://api.deepseek.com)\\n# DEEPSEEK_MODEL：DeepSeek 模型名称 (例如 deepseek-chat)\\n\\nAuthor: FlyAIBox\\nDate: 2025.10.11\\n\"\"\"\\nimport asyncio\\nimport logging\\nimport os\\nfrom contextlib import AsyncExitStack\\nfrom typing import List, Optional\\n\\nfrom dotenv import load_dotenv\\nfrom langchain_core.messages import SystemMessage\\nfrom langchain_core.tools import BaseTool\\nfrom langchain_mcp_adapters.tools import load_mcp_tools\\nfrom langchain_openai import ChatOpenAI\\nfrom langgraph.prebuilt import create_react_agent\\nfrom mcp import ClientSession, StdioServerParameters\\nfrom mcp.client.stdio import stdio_client\\n\\n# 配置日志\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\\n)\\nlogger = logging.getLogger(__name__)\\n\\n\\n# 优先从环境变量加载API密钥等信息\\nload_dotenv()\\n\\nclass Configuration:\\n    \"\"\"配置管理类，负责管理和验证环境变量\"\"\"\\n    \\n    def __init__(self) -> None:\\n        \"\"\"初始化配置并加载环境变量\"\"\"\\n        self.load_env()\\n        self._validate_env()\\n        \\n    @staticmethod\\n    def load_env() -> None:\\n        \"\"\"从.env文件加载环境变量\"\"\"\\n        load_dotenv()\\n        \\n    def _validate_env(self) -> None:\\n        \"\"\"验证必需的环境变量是否存在\"\"\"\\n        required_vars = [\"DEEPSEEK_API_KEY\"]\\n        missing_vars = [var for var in required_vars if not os.getenv(var)]\\n        if missing_vars:\\n            raise ValueError(f\"缺少必需的环境变量: {\\', \\'.join(missing_vars)}\")\\n    \\n    @property\\n    def api_key(self) -> str:\\n        \"\"\"获取 DeepSeek API 密钥\"\"\"\\n        return os.getenv(\"DEEPSEEK_API_KEY\", \"\")\\n    \\n    @property\\n    def base_url(self) -> str:\\n        \"\"\"获取 DeepSeek API 基础 URL\"\"\"\\n        return os.getenv(\"DEEPSEEK_BASE_URL\", \"https://api.deepseek.com\")\\n    \\n    @property\\n    def model(self) -> str:\\n        \"\"\"获取 DeepSeek 模型名称\"\"\"\\n        return os.getenv(\"DEEPSEEK_MODEL\", \"deepseek-chat\")\\n\\nclass MCPServer:\\n    \"\"\"MCP 服务器管理类，处理服务器连接和工具执行\"\"\"\\n    def __init__(self, server_path: str) -> None:\\n        \"\"\"\\n        初始化服务器管理器\\n        \\n        Args:\\n            server_path: 服务器脚本路径\\n        \"\"\"\\n        self.server_path = server_path\\n        self.session: Optional[ClientSession] = None\\n        self.exit_stack = AsyncExitStack()\\n        self._cleanup_lock = asyncio.Lock()\\n\\n\\n    async def initialize(self) -> None:\\n        \"\"\"初始化服务器连接，包含重试机制\"\"\"\\n        max_retries = 3\\n        retry_delay = 1.0\\n        \\n        for attempt in range(max_retries):\\n            try:\\n                if not os.path.exists(self.server_path):\\n                    raise FileNotFoundError(f\"找不到服务器文件: {self.server_path}\")\\n                \\n                server_params = StdioServerParameters(\\n                    command=\\'python\\',\\n                    args=[self.server_path],\\n                    env=None\\n                )\\n                \\n                stdio_transport = await self.exit_stack.enter_async_context(\\n                    stdio_client(server_params)\\n                )\\n                stdio, write = stdio_transport\\n                \\n                self.session = await self.exit_stack.enter_async_context(\\n                    ClientSession(stdio, write)\\n                )\\n                await self.session.initialize()\\n                logger.info(\"成功连接到 MCP 服务器\")\\n                break\\n                \\n            except Exception as e:\\n                logger.error(f\"第 {attempt + 1}/{max_retries} 次尝试失败: {str(e)}\")\\n                if attempt < max_retries - 1:\\n                    await asyncio.sleep(retry_delay)\\n                else:\\n                    raise\\n\\n    async def list_tools(self) -> List[BaseTool]:\\n        \"\"\"获取服务器提供的可用工具列表\"\"\"\\n        if not self.session:\\n            raise RuntimeError(\"服务器未初始化\")\\n        # LangChain方式获取可用工具列表\\n\\n        # 自动化的工具处理：\\n        # LangChain版本使用load_mcp_tools函数\\n        # 自动适配MCP工具，省去了工具格式转换的工作\\n        tools = await load_mcp_tools(self.session)\\n        logger.info(f\"成功加载工具: {[tool.name for tool in tools]}\")\\n        return tools\\n\\n\\n    async def cleanup(self) -> None:\\n        \"\"\"清理服务器资源\"\"\"\\n        async with self._cleanup_lock:\\n            try:\\n                await self.exit_stack.aclose()\\n                self.session = None\\n                logger.info(\"服务器资源清理完成\")\\n            except Exception as e:\\n                logger.error(f\"清理过程中出错: {str(e)}\")\\n    \\n\\nclass MCPClient:\\n    \"\"\"MCP 客户端实现，集成了 DeepSeek API\"\"\"\\n    \\n    def __init__(self, config: Configuration) -> None:\\n        \"\"\"\\n        初始化 MCP 客户端\\n        \\n        Args:\\n            config: 配置对象\\n        \"\"\"\\n        self.config = config\\n        self.server: Optional[MCPServer] = None\\n        self.llm_client = ChatOpenAI(\\n            api_key=config.api_key,\\n            base_url=config.base_url,\\n            model=config.model\\n        )\\n        \\n    async def initialize(self) -> None:\\n        \"\"\"初始化客户端并连接到服务器\"\"\"\\n        server_path = os.path.join(\\n            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\\n            \"server\",\\n            \"weather_server.py\"\\n        )\\n        self.server = MCPServer(server_path)\\n        await self.server.initialize()\\n        \\n\\n    async def process_query(self, query: str):\\n        \"\"\"\\n        处理用户查询，集成工具调用，支持多轮工具交互\\n\\n        Args:\\n            query: 用户查询字符串\\n\\n        Returns:\\n            处理后的响应结果\\n        \"\"\"\\n        if not self.server:\\n            raise RuntimeError(\"客户端未初始化\")\\n\\n        # 创建提示模板\\n        prompt = SystemMessage(content=\"\"\"\"You are a helpful assistant specializing in weather information.\\\\n\\n                            You have access to the MCP Weather Server tool with the following functions:\\\\n\\n                            - get_weather_warning(city_id=None, latitude=None, longitude=None): Retrieves weather disaster warnings for a specified city ID or coordinates.\\\\n\\n                            - get_daily_forecast(city_id=None, latitude=None, longitude=None): Retrieves the multi-day weather forecast for a specified city ID or coordinates.\\\\n\\n                            \\\\n\\n                            Core Instructions:\\\\n\\n                            1.  **Carefully analyze the user\\'s request**: Understand all components of the user\\'s query. Determine if the user needs weather warning information, weather forecast information, or both.\\\\n\\n                            2.  **Identify Information Needs**:\\\\n\\n                                * If the user only asks for warnings (e.g., \\\\Are there any warnings in Beijing?\\\\), only use `get_weather_warning`.\\\\n\\n                                * If the user only asks for the forecast (e.g., \\\\What\\'s the weather like in Beijing tomorrow?\\\\), only use `get_daily_forecast`.\\\\n\\n                                * **If the user\\'s question includes multiple aspects**, such as asking about **warning status** and also asking **if it\\'s suitable for a certain activity** (which implies a query about future weather, like \\\\Have there been high temperature warnings in Beijing in the last week? Is it suitable for outdoor activities?\\\\), you need to **call both tools sequentially**.\\\\n\\n                            3.  **Call Tools as Needed**:\\\\n\\n                                * **Prioritize getting warning information**: If warning information is needed, first call `get_weather_warning`.\\\\n\\n                                * **Get the weather forecast**: If the user mentions a specific time period (e.g., \\\\weekend\\\\, \\\\next three days\\\\, \\\\next week\\\\) or asks about activity suitability (which typically concerns the next few days), call `get_daily_forecast` to get the forecast for the corresponding period. For vague phrases like \\\\last week\\\\ or \\\\recently\\\\, interpret it as asking about *current* conditions and the *upcoming* few days (covered by the forecast). For questions like \\\\Is it suitable for outdoor activities?\\\\, you should get the forecast for at least the next 2-3 days (e.g., today, tomorrow, the day after tomorrow, or the upcoming weekend) to support your judgment.\\\\n\\n                                * **Ensure tool call order**: When multiple tools need to be called, they should be called in a logical sequence. For example, first get the warning, then get the forecast. Wait for one tool to finish executing before deciding whether to call the next tool or generate a response.\\\\n\\n                            4.  **Information Integration and Response**:\\\\n\\n                                * After obtaining all necessary information (warning, forecast), you **must synthesize and analyze this information**.\\\\n\\n                                * **Completely answer the user\\'s question**: Ensure you answer all parts of the user\\'s query.\\\\n\\n                                * **Provide advice**: If the user asks about activity suitability, based on the retrieved warning status and forecast information (temperature, weather condition - clear/rainy, wind strength, etc.), provide a clear, data-supported recommendation (e.g., \\\\Currently there are no high temperature warnings, but it\\'s expected to rain this weekend, so it\\'s not very suitable for outdoor activities,\\\\ or \\\\It will be sunny for the next few days with no warnings, suitable for outdoor activities.\\\\).\\\\n\\n                            5.  **Tool Usage Details**:\\\\n\\n                                * When using the tools, retain the full context of the user\\'s original question.\\\\n\\n                                * Unless explicitly requested by the user, do not insert specific times of day (e.g., \\\\3 PM\\\\) into the search query or your response.\\\\n\\n                                * When city information is needed, if the user provides a city name (e.g., \\\\Beijing\\\\), use the corresponding `city_id` (e.g., Beijing\\'s city_id might be \\'101010100\\').\\\\n                      \\n                            \"\"\")\\n        \\n        \\n        ## 列举工具\\n        tools = await self.server.list_tools()\\n\\n        ## 更简洁的代理创建流程：\\n        # LangChain版本使用create_react_agent函数\\n        # 直接创建代理，并指定模型、工具和提示\\n        # 简化了代码复杂度\\n        \\n        # LangChain方式创建和运行agent\\n        logger.info(\"正在创建agent...\")\\n        agent = create_react_agent(\\n            model=self.llm_client,  \\n            tools=tools,\\n            prompt=prompt\\n        )\\n        logger.info(\"Agent创建成功\")\\n\\n        # 发送查询\\n        logger.info(\"正在发送天气查询...\")\\n        agent_response = await agent.ainvoke({\\n            \"messages\": query\\n        })\\n\\n        # 打印响应\\n        logger.info(\"\\\\nAgent Response:\")\\n        print(agent_response)\\n\\n        # 遍历消息列表并打印 ToolMessage 的 content\\n        if \\'messages\\' in agent_response:\\n            print(\"\\\\n--- Tool Message Contents ---\")\\n            for message in agent_response[\\'messages\\']:\\n                print(f\"\\\\nTool: {message.name}\") # 可以选择打印工具名称\\n                print(f\"Content:\\\\n{message.content}\")\\n                print(\"-\" * 20) # 分隔不同 ToolMessage 的内容\\n             \\n\\n    async def chat_loop(self) -> None:\\n        \"\"\"运行交互式聊天循环\"\"\"\\n        logger.info(\"开始聊天会话...\")\\n        \\n        while True:\\n            try:\\n                query = input(\"\\\\n请输入你的问题 (输入 \\'quit\\' 或 \\'exit\\' 退出): \").strip()\\n                \\n                if query.lower() in [\\'quit\\', \\'exit\\']:\\n                    logger.info(\"结束聊天会话...\")\\n                    break\\n                    \\n                await self.process_query(query)\\n                \\n                \\n            except KeyboardInterrupt:\\n                logger.info(\"\\\\n收到键盘中断，结束会话...\")\\n                break\\n            except Exception as e:\\n                logger.error(f\"聊天循环中出错: {str(e)}\")\\n                print(f\"\\\\n发生错误: {str(e)}\")\\n                \\n    async def cleanup(self) -> None:\\n        \"\"\"清理客户端资源\"\"\"\\n        if self.server:\\n            await self.server.cleanup()\\n\\nasync def main() -> None:\\n    \"\"\"主程序入口\"\"\"\\n    try:\\n        config = Configuration()\\n        client = MCPClient(config)\\n        \\n        await client.initialize()\\n        await client.chat_loop()\\n        \\n    except Exception as e:\\n        logger.error(f\"致命错误: {str(e)}\")\\n        raise\\n        \\n    finally:\\n        if \\'client\\' in locals():\\n            await client.cleanup()\\n\\nif __name__ == \"__main__\":\\n    try:\\n        asyncio.run(main())\\n    except KeyboardInterrupt:\\n        logger.info(\"程序被用户终止\")\\n    except Exception as e:\\n        logger.error(f\"程序因错误终止: {str(e)}\")\\n        raise\\n\\n', 'timestamp': '2025-11-09T00:16:18+0800'}\n",
      "Agent thinking...\n",
      "Agent Decision: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"mcp-demo/server/weather_server.py\"}}\n",
      "Action Result: {'tool_executed': True, 'result': '#!/usr/bin/env python3\\n# -*- coding: utf-8 -*-\\n\\n\"\"\"\\nMCP 服务器可以提供三种主要类型的功能：\\n\\n资源：客户端可以读取的类似文件的数据（例如 API 响应或文件内容）\\n工具：可由 LLM 调用的函数（经用户批准）\\n提示：预先编写的模板，帮助用户完成特定任务\\n\\n######################################\\n\\nMCP 天气服务器\\n\\n提供两个工具：\\n1. get_weather_warning: 获取指定城市ID或经纬度的天气灾害预警\\n2. get_daily_forecast: 获取指定城市ID或经纬度的天气预报\\n\\nAuthor: FlyAIBox\\nDate: 2025.10.11\\n\"\"\"\\n\\nfrom typing import Any, Dict, List, Optional, Union\\nimport asyncio\\nimport httpx\\nimport os\\nfrom urllib.parse import urljoin\\nfrom mcp.server.fastmcp import FastMCP\\nfrom dotenv import load_dotenv\\nfrom pathlib import Path\\n\\n# 加载 .env 文件中的环境变量\\ndotenv_path = Path(__file__).resolve().parents[1] / \\'.env\\'\\nload_dotenv(dotenv_path)\\n\\n# 初始化 FastMCP 服务器\\nmcp = FastMCP(\"weather\",  # 服务器名称\\n              debug=True,  # 启用调试模式，会输出详细日志\\n              host=\"0.0.0.0\") # 监听所有网络接口，允许远程连接\\n\\n# 从环境变量中读取常量\\nQWEATHER_API_BASE = os.getenv(\"QWEATHER_API_BASE\")\\nQWEATHER_API_KEY = os.getenv(\"QWEATHER_API_KEY\")\\n\\ndef _normalize_base_url(raw_base: Optional[str]) -> str:\\n    \"\"\"\\n    确保基础 URL 包含协议并以单个斜杠结尾，兼容 .env 中未写协议的情况\\n    \"\"\"\\n    if not raw_base:\\n        raise RuntimeError(\"未配置 QWEATHER_API_BASE 环境变量\")\\n\\n    base = raw_base.strip()\\n    if not base.startswith((\"http://\", \"https://\")):\\n        base = f\"https://{base.lstrip(\\'/\\')}\"\\n\\n    # urljoin 要求目录风格以斜杠结尾，避免 \\'v7/weather/7d\\' 被覆盖\\n    if not base.endswith(\"/\"):\\n        base = f\"{base}/\"\\n\\n    return base\\n\\ntry:\\n    _QWEATHER_BASE_URL = _normalize_base_url(QWEATHER_API_BASE)\\nexcept RuntimeError as err:\\n    print(f\"[配置错误] {err}\")\\n    _QWEATHER_BASE_URL = None\\n\\nasync def make_qweather_request(endpoint: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:\\n    \"\"\"\\n    向和风天气 API 发送请求\\n    \\n    参数:\\n        endpoint: API 端点路径（不包含基础 URL）\\n        params: API 请求的参数\\n        \\n    返回:\\n        成功时返回 JSON 响应，失败时返回 None\\n    \"\"\"\\n    if not _QWEATHER_BASE_URL:\\n        print(\"QWEATHER_API_BASE 未正确配置，已跳过请求。\")\\n        return None\\n\\n    if not QWEATHER_API_KEY:\\n        print(\"QWEATHER_API_KEY 未设置，已跳过请求。\")\\n        return None\\n\\n    safe_endpoint = endpoint.lstrip(\"/\")\\n    url = urljoin(_QWEATHER_BASE_URL, safe_endpoint)\\n\\n    # 使用 Header 方式认证（和风天气的新版本API）\\n    headers = {\\n        \"X-QW-Api-Key\": QWEATHER_API_KEY\\n    }\\n    \\n    async with httpx.AsyncClient() as client:\\n        try:\\n            print(f\"请求 URL: {url}\")\\n            print(f\"请求参数: {params}\")\\n            response = await client.get(url, params=params, headers=headers, timeout=30.0)\\n            print(f\"响应状态码: {response.status_code}\")\\n            response.raise_for_status()\\n            result = response.json()\\n            print(f\"响应内容: {result}\")\\n            return result\\n        except httpx.HTTPStatusError as e:\\n            print(f\"HTTP 状态错误: {e.response.status_code} - {e.response.text}\")\\n            return None\\n        except Exception as e:\\n            print(f\"API 请求错误: {type(e).__name__}: {e}\")\\n            return None\\n\\ndef format_warning(warning: Dict[str, Any]) -> str:\\n    \"\"\"\\n    将天气预警数据格式化为可读字符串\\n    \\n    参数:\\n        warning: 天气预警数据对象\\n        \\n    返回:\\n        格式化后的预警信息\\n    \"\"\"\\n    return f\"\"\"\\n预警ID: {warning.get(\\'id\\', \\'未知\\')}\\n标题: {warning.get(\\'title\\', \\'未知\\')}\\n发布时间: {warning.get(\\'pubTime\\', \\'未知\\')}\\n开始时间: {warning.get(\\'startTime\\', \\'未知\\')}\\n结束时间: {warning.get(\\'endTime\\', \\'未知\\')}\\n预警类型: {warning.get(\\'typeName\\', \\'未知\\')}\\n预警等级: {warning.get(\\'severity\\', \\'未知\\')} ({warning.get(\\'severityColor\\', \\'未知\\')})\\n发布单位: {warning.get(\\'sender\\', \\'未知\\')}\\n状态: {warning.get(\\'status\\', \\'未知\\')}\\n详细信息: {warning.get(\\'text\\', \\'无详细信息\\')}\\n\"\"\"\\n\\n@mcp.tool()\\nasync def get_weather_warning(location: Union[str, int]) -> str:\\n    \"\"\"\\n    获取指定位置的天气灾害预警\\n    \\n    参数:\\n        location: 城市ID或经纬度坐标（经度,纬度）\\n                例如：\\'101010100\\'（北京）或 \\'116.41,39.92\\'\\n                也可以直接传入数字ID，如 101010100\\n        \\n    返回:\\n        格式化的预警信息字符串\\n    \"\"\"\\n    # 确保 location 为字符串类型\\n    location = str(location)\\n    \\n    params = {\\n        \"location\": location,\\n        \"lang\": \"zh\"\\n    }\\n    \\n    data = await make_qweather_request(\"v7/warning/now\", params)\\n    \\n    if not data:\\n        return \"无法获取预警信息或API请求失败。\"\\n    \\n    if data.get(\"code\") != \"200\":\\n        return f\"API 返回错误: {data.get(\\'code\\')}\"\\n    \\n    warnings = data.get(\"warning\", [])\\n    \\n    if not warnings:\\n        return f\"当前位置 {location} 没有活动预警。\"\\n    \\n    formatted_warnings = [format_warning(warning) for warning in warnings]\\n    return \"\\\\n---\\\\n\".join(formatted_warnings)\\n\\ndef format_daily_forecast(daily: Dict[str, Any]) -> str:\\n    \"\"\"\\n    将天气预报数据格式化为可读字符串\\n    \\n    参数:\\n        daily: 天气预报数据对象\\n        \\n    返回:\\n        格式化后的预报信息\\n    \"\"\"\\n    return f\"\"\"\\n日期: {daily.get(\\'fxDate\\', \\'未知\\')}\\n日出: {daily.get(\\'sunrise\\', \\'未知\\')}  日落: {daily.get(\\'sunset\\', \\'未知\\')}\\n最高温度: {daily.get(\\'tempMax\\', \\'未知\\')}°C  最低温度: {daily.get(\\'tempMin\\', \\'未知\\')}°C\\n白天天气: {daily.get(\\'textDay\\', \\'未知\\')}  夜间天气: {daily.get(\\'textNight\\', \\'未知\\')}\\n白天风向: {daily.get(\\'windDirDay\\', \\'未知\\')} {daily.get(\\'windScaleDay\\', \\'未知\\')}级 ({daily.get(\\'windSpeedDay\\', \\'未知\\')}km/h)\\n夜间风向: {daily.get(\\'windDirNight\\', \\'未知\\')} {daily.get(\\'windScaleNight\\', \\'未知\\')}级 ({daily.get(\\'windSpeedNight\\', \\'未知\\')}km/h)\\n相对湿度: {daily.get(\\'humidity\\', \\'未知\\')}%\\n降水量: {daily.get(\\'precip\\', \\'未知\\')}mm\\n紫外线指数: {daily.get(\\'uvIndex\\', \\'未知\\')}\\n能见度: {daily.get(\\'vis\\', \\'未知\\')}km\\n\"\"\"\\n\\n@mcp.tool()\\nasync def get_daily_forecast(location: Union[str, int], days: int = 3) -> str:\\n    \"\"\"\\n    获取指定位置的天气预报\\n    \\n    参数:\\n        location: 城市ID或经纬度坐标（经度,纬度）\\n                例如：\\'101010100\\'（北京）或 \\'116.41,39.92\\'\\n                也可以直接传入数字ID，如 101010100\\n        days: 预报天数，可选值为 3、7、10、15、30，默认为 3\\n        \\n    返回:\\n        格式化的天气预报字符串\\n    \"\"\"\\n    # 确保 location 为字符串类型\\n    location = str(location)\\n    \\n    # 确保 days 参数有效\\n    valid_days = [3, 7, 10, 15, 30]\\n    if days not in valid_days:\\n        days = 3  # 默认使用3天预报\\n    \\n    params = {\\n        \"location\": location,\\n        \"lang\": \"zh\"\\n    }\\n    # 和风天气API文档 https://dev.qweather.com/docs/api/weather/weather-daily-forecast/\\n    endpoint = f\"v7/weather/{days}d\"\\n    data = await make_qweather_request(endpoint, params)\\n    \\n    if not data:\\n        return \"无法获取天气预报或API请求失败。\"\\n    \\n    if data.get(\"code\") != \"200\":\\n        return f\"API 返回错误: {data.get(\\'code\\')}\"\\n    \\n    daily_forecasts = data.get(\"daily\", [])\\n    \\n    if not daily_forecasts:\\n        return f\"无法获取 {location} 的天气预报数据。\"\\n    \\n    formatted_forecasts = [format_daily_forecast(daily) for daily in daily_forecasts]\\n    return \"\\\\n---\\\\n\".join(formatted_forecasts)\\n\\nif __name__ == \"__main__\":\\n    print(\"正在启动 MCP 天气服务器...\")\\n    print(\"提供工具: get_weather_warning, get_daily_forecast\")\\n    print(\"请确保环境变量 QWEATHER_API_KEY 已设置\")\\n    print(\"使用 Ctrl+C 停止服务器\")\\n    \\n    # 初始化并运行服务器\\n    mcp.run(transport=\\'stdio\\') \\n', 'timestamp': '2025-11-09T00:16:19+0800'}\n",
      "Agent thinking...\n",
      "Agent Decision: {\"tool\": \"terminate\", \"args\": {\"message\": \"# MCP Weather Client and Server Project\\n\\n## Overview:\\n\\nThis project consists of a client-server application designed to interact with the Model Context Protocol (MCP), a protocol that facilitates standardized communication among various AI models and tools. The MCP Weather application provides weather-related functions by utilizing external APIs and integrates these capabilities into a client-server model.\\n\\n## Project Structure:\\n\\n- **Client:** \\n  - The client contains multiple implementations demonstrating different integrations:\\n    - `mcp_client.py`: A simple client application showcasing MCP client-server interactions, calling tools, and handling responses.\\n    - `mcp_client_deepseek.py`: Integration with DeepSeek API in the MCP client, handling user queries and processing them via DeepSeek.\\n    - `mcp_client_langchain_chat.py`: Integration with DeepSeek API using a LangChain version, illustrating streamlined tool handling and agent creation.\\n\\n- **Server:**\\n  - `weather_server.py`: An MCP server providing weather information services. It includes tools for retrieving weather warnings and forecasts from a weather API and formats the responses for client use.\\n\\n## Key Features:\\n\\n- **MCP Client-Server Communication**: Demonstrates how the client communicates with the server using standard input/output, with asynchronous handling for efficiency.\\n- **Tool Integration**: Features tools that gather weather data from external APIs and make it easily accessible through standardized MCP protocols.\\n- **Flexible Implementations**: Shows different ways to integrate with AI functionalities - starting from a basic model to more sophisticated methods using third-party libraries like DeepSeek and LangChain.\\n\\n## Getting Started:\\n\\n### Prerequisites:\\n- Install Python 3.7 or later.\\n- Install necessary packages using pip (e.g., `asyncio`, `httpx`, `openai`, `langchain_openai`, `dotenv`, and others mentioned in client implementations).\\n\\n### Environment Configuration:\\n- Create a `.env` file at the root of the project to store essential environment variables.\\n- **Sample `.env` Configuration**:\\n  ```\\n  DEEPSEEK_API_KEY=your_deepseek_api_key\\n  DEEPSEEK_BASE_URL=https://api.deepseek.com\\n  DEEPSEEK_MODEL=deepseek-chat\\n  QWEATHER_API_BASE=your_qweather_api_base\\n  QWEATHER_API_KEY=your_qweather_api_key\\n  ```\\n\\n### Running the Server:\\n1. Navigate to the server directory (e.g., `mcp-demo/server`).\\n2. Execute `python weather_server.py` to start the server.\\n\\n### Running the Client:\\n1. Choose a client implementation from the `mcp-demo/client` directory.\\n2. Execute the script using Python (e.g., `python mcp_client.py`).\\n3. Follow the command prompts to interact with the MCP server.\\n\\n## Usage:\\n- The client applications prompt for user input commands to perform various operations, including fetching weather warnings and forecasts.\\n- Server responses are processed and displayed to the user through formatted text, providing valuable weather insights.\\n\\n## Future Improvements:\\n\\n- Expand tool capabilities to include more advanced MCP features and integrate additional APIs.\\n- Enhance error handling and logging to provide deeper insights into application behavior.\\n\\n---\\nThis project offers a well-structured demonstration of how MCP can be utilized for tool integration and provides practical examples for weather-based applications using modern AI frameworks and protocols.\"}}\n",
      "Action Result: {'tool_executed': True, 'result': '# MCP Weather Client and Server Project\\n\\n## Overview:\\n\\nThis project consists of a client-server application designed to interact with the Model Context Protocol (MCP), a protocol that facilitates standardized communication among various AI models and tools. The MCP Weather application provides weather-related functions by utilizing external APIs and integrates these capabilities into a client-server model.\\n\\n## Project Structure:\\n\\n- **Client:** \\n  - The client contains multiple implementations demonstrating different integrations:\\n    - `mcp_client.py`: A simple client application showcasing MCP client-server interactions, calling tools, and handling responses.\\n    - `mcp_client_deepseek.py`: Integration with DeepSeek API in the MCP client, handling user queries and processing them via DeepSeek.\\n    - `mcp_client_langchain_chat.py`: Integration with DeepSeek API using a LangChain version, illustrating streamlined tool handling and agent creation.\\n\\n- **Server:**\\n  - `weather_server.py`: An MCP server providing weather information services. It includes tools for retrieving weather warnings and forecasts from a weather API and formats the responses for client use.\\n\\n## Key Features:\\n\\n- **MCP Client-Server Communication**: Demonstrates how the client communicates with the server using standard input/output, with asynchronous handling for efficiency.\\n- **Tool Integration**: Features tools that gather weather data from external APIs and make it easily accessible through standardized MCP protocols.\\n- **Flexible Implementations**: Shows different ways to integrate with AI functionalities - starting from a basic model to more sophisticated methods using third-party libraries like DeepSeek and LangChain.\\n\\n## Getting Started:\\n\\n### Prerequisites:\\n- Install Python 3.7 or later.\\n- Install necessary packages using pip (e.g., `asyncio`, `httpx`, `openai`, `langchain_openai`, `dotenv`, and others mentioned in client implementations).\\n\\n### Environment Configuration:\\n- Create a `.env` file at the root of the project to store essential environment variables.\\n- **Sample `.env` Configuration**:\\n  ```\\n  DEEPSEEK_API_KEY=your_deepseek_api_key\\n  DEEPSEEK_BASE_URL=https://api.deepseek.com\\n  DEEPSEEK_MODEL=deepseek-chat\\n  QWEATHER_API_BASE=your_qweather_api_base\\n  QWEATHER_API_KEY=your_qweather_api_key\\n  ```\\n\\n### Running the Server:\\n1. Navigate to the server directory (e.g., `mcp-demo/server`).\\n2. Execute `python weather_server.py` to start the server.\\n\\n### Running the Client:\\n1. Choose a client implementation from the `mcp-demo/client` directory.\\n2. Execute the script using Python (e.g., `python mcp_client.py`).\\n3. Follow the command prompts to interact with the MCP server.\\n\\n## Usage:\\n- The client applications prompt for user input commands to perform various operations, including fetching weather warnings and forecasts.\\n- Server responses are processed and displayed to the user through formatted text, providing valuable weather insights.\\n\\n## Future Improvements:\\n\\n- Expand tool capabilities to include more advanced MCP features and integrate additional APIs.\\n- Enhance error handling and logging to provide deeper insights into application behavior.\\n\\n---\\nThis project offers a well-structured demonstration of how MCP can be utilized for tool integration and provides practical examples for weather-based applications using modern AI frameworks and protocols.\\nTerminating...', 'timestamp': '2025-11-09T00:16:27+0800'}\n",
      "[{'type': 'user', 'content': 'Write a README for this project.'}, {'type': 'assistant', 'content': '{\"tool\": \"list_project_files\", \"args\": {}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": [\"mcp-demo/client/mcp_client.py\", \"mcp-demo/client/mcp_client_deepseek.py\", \"mcp-demo/client/mcp_client_langchain_chat.py\", \"mcp-demo/server/weather_server.py\"], \"timestamp\": \"2025-11-09T00:16:12+0800\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"read_project_file\", \"args\": {\"name\": \"mcp-demo/client/mcp_client.py\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"#!/usr/bin/env python3\\\\n# -*- coding: utf-8 -*-\\\\n\\\\n\\\\\"\\\\\"\\\\\"\\\\nMCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\u793a\\\\u4f8b\\\\n\\\\n\\\\u7528\\\\u4e8e\\\\u4e0e MCP \\\\u670d\\\\u52a1\\\\u5668\\\\u4ea4\\\\u4e92\\\\uff0c\\\\u8c03\\\\u7528\\\\u5de5\\\\u5177\\\\u5e76\\\\u5904\\\\u7406\\\\u7ed3\\\\u679c\\\\n\\\\nAuthor: FlyAIBox\\\\nDate: 2025.10.11\\\\n\\\\\"\\\\\"\\\\\"\\\\n\\\\nimport asyncio\\\\nimport json\\\\nimport os\\\\nimport subprocess\\\\nimport sys\\\\nfrom typing import Dict, Any, List, Optional\\\\nfrom contextlib import AsyncExitStack\\\\n\\\\n# \\\\u4f7f\\\\u7528\\\\u6b63\\\\u786e\\\\u7684 MCP \\\\u5bfc\\\\u5165\\\\nfrom mcp import ClientSession, StdioServerParameters, Tool\\\\nfrom mcp.client.stdio import stdio_client\\\\n\\\\nclass SimpleClientApp:\\\\n    \\\\\"\\\\\"\\\\\"\\\\u7b80\\\\u5355\\\\u7684 MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\u5e94\\\\u7528\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self, server_command: List[str]):\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u521d\\\\u59cb\\\\u5316 MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\u5e94\\\\u7528\\\\n        \\\\n        \\\\u53c2\\\\u6570:\\\\n            server_command: \\\\u542f\\\\u52a8\\\\u670d\\\\u52a1\\\\u5668\\\\u7684\\\\u547d\\\\u4ee4\\\\u5217\\\\u8868\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.server_command = server_command\\\\n        self.server_process = None\\\\n        self.client = None\\\\n        self.tool_definitions = []\\\\n        self.exit_stack = AsyncExitStack()\\\\n    \\\\n    async def start(self):\\\\n        \\\\\"\\\\\"\\\\\"\\\\u542f\\\\u52a8 MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\u5e76\\\\u8fde\\\\u63a5\\\\u5230\\\\u670d\\\\u52a1\\\\u5668\\\\\"\\\\\"\\\\\"\\\\n        print(\\\\\"\\\\u542f\\\\u52a8 MCP \\\\u670d\\\\u52a1\\\\u5668\\\\u8fdb\\\\u7a0b...\\\\\")\\\\n        \\\\n        # \\\\u914d\\\\u7f6e\\\\u670d\\\\u52a1\\\\u5668\\\\u53c2\\\\u6570\\\\n        server_params = StdioServerParameters(\\\\n            command=self.server_command[0],\\\\n            args=self.server_command[1:],\\\\n            env=None  # \\\\u4f7f\\\\u7528\\\\u9ed8\\\\u8ba4\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\n        )\\\\n        \\\\n        # \\\\u542f\\\\u52a8\\\\u670d\\\\u52a1\\\\u5668\\\\u5e76\\\\u83b7\\\\u53d6\\\\u901a\\\\u4fe1\\\\u6d41\\\\n        # \\\\u901a\\\\u8fc7 stdio_client \\\\u534f\\\\u7a0b\\\\uff0c\\\\u4ee5\\\\u5f02\\\\u6b65\\\\u65b9\\\\u5f0f\\\\u5efa\\\\u7acb\\\\u4e0e\\\\u670d\\\\u52a1\\\\u5668\\\\u8fdb\\\\u7a0b\\\\u7684\\\\u6807\\\\u51c6\\\\u8f93\\\\u5165/\\\\u8f93\\\\u51fa\\\\u901a\\\\u9053\\\\n        # enter_async_context \\\\u53ef\\\\u4ee5\\\\u786e\\\\u4fdd\\\\u540e\\\\u7eed\\\\u8d44\\\\u6e90\\\\u80fd\\\\u81ea\\\\u52a8\\\\u91ca\\\\u653e\\\\uff08\\\\u7c7b\\\\u4f3c try...finally\\\\uff09\\\\n        # stdio_client(server_params) \\\\u4f1a\\\\u542f\\\\u52a8\\\\u5b50\\\\u8fdb\\\\u7a0b\\\\u5e76\\\\u8fd4\\\\u56de (reader, writer) \\\\u7684\\\\u5143\\\\u7ec4\\\\n        read_write = await self.exit_stack.enter_async_context(stdio_client(server_params))\\\\n        # \\\\u89e3\\\\u5305\\\\u5f97\\\\u5230 reader\\\\uff08\\\\u8bfb\\\\u53d6\\\\u670d\\\\u52a1\\\\u5668\\\\u8f93\\\\u51fa\\\\uff09\\\\u3001writer\\\\uff08\\\\u5199\\\\u5165\\\\u670d\\\\u52a1\\\\u5668\\\\u8f93\\\\u5165\\\\uff09\\\\uff0c\\\\u7528\\\\u4e8e\\\\u540e\\\\u7eed\\\\u901a\\\\u4fe1\\\\n        # read_write \\\\u662f\\\\u4e00\\\\u4e2a\\\\u5305\\\\u542b\\\\u4e24\\\\u4e2a\\\\u5143\\\\u7d20\\\\u7684\\\\u5143\\\\u7ec4\\\\uff0c\\\\u5206\\\\u522b\\\\u5bf9\\\\u5e94\\\\u4e8e\\\\u4e0e\\\\u670d\\\\u52a1\\\\u5668\\\\u901a\\\\u4fe1\\\\u7684 \\\\u201c\\\\u8bfb\\\\u53d6\\\\u201d (reader) \\\\u548c \\\\u201c\\\\u5199\\\\u5165\\\\u201d (writer) \\\\u6d41\\\\u5bf9\\\\u8c61\\\\u3002\\\\n        read, write = read_write\\\\n        \\\\n        # \\\\u521b\\\\u5efa MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\u4f1a\\\\u8bdd\\\\n        self.client = await self.exit_stack.enter_async_context(ClientSession(read, write))\\\\n        \\\\n        # \\\\u521d\\\\u59cb\\\\u5316\\\\u8fde\\\\u63a5\\\\n        await self.client.initialize()\\\\n        \\\\n        # \\\\u83b7\\\\u53d6\\\\u5de5\\\\u5177\\\\u5b9a\\\\u4e49\\\\n        response = await self.client.list_tools()\\\\n        self.tool_definitions = response.tools\\\\n        \\\\n        print(f\\\\\"\\\\u5df2\\\\u8fde\\\\u63a5\\\\u5230\\\\u670d\\\\u52a1\\\\u5668\\\\uff0c\\\\u53ef\\\\u7528\\\\u5de5\\\\u5177: {len(self.tool_definitions)}\\\\\")\\\\n        \\\\n        for tool in self.tool_definitions:\\\\n            print(f\\\\\"  - {tool.name}: {tool.description}\\\\\")\\\\n        \\\\n        print(\\\\\"\\\\\\\\n\\\\u4f7f\\\\u7528 \\'help\\' \\\\u67e5\\\\u770b\\\\u5e2e\\\\u52a9\\\\uff0c\\\\u4f7f\\\\u7528 \\'exit\\' \\\\u9000\\\\u51fa\\\\\")\\\\n    \\\\n    async def execute_tool(self, tool_name: str, params: Dict[str, Any]) -> Optional[str]:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u6267\\\\u884c\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\n        \\\\n        \\\\u53c2\\\\u6570:\\\\n            tool_name: \\\\u8981\\\\u8c03\\\\u7528\\\\u7684\\\\u5de5\\\\u5177\\\\u540d\\\\u79f0\\\\n            params: \\\\u5de5\\\\u5177\\\\u53c2\\\\u6570\\\\u5b57\\\\u5178\\\\n            \\\\n        \\\\u8fd4\\\\u56de:\\\\n            \\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u7ed3\\\\u679c\\\\u6216\\\\u9519\\\\u8bef\\\\u6d88\\\\u606f\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        # \\\\u68c0\\\\u67e5\\\\u5de5\\\\u5177\\\\u662f\\\\u5426\\\\u5b58\\\\u5728\\\\n        # \\\\u904d\\\\u5386\\\\u5de5\\\\u5177\\\\u5b9a\\\\u4e49\\\\u5217\\\\u8868\\\\uff0c\\\\u67e5\\\\u627e\\\\u540d\\\\u79f0\\\\u4e0e tool_name \\\\u5339\\\\u914d\\\\u7684\\\\u5de5\\\\u5177\\\\u5bf9\\\\u8c61\\\\n        # next() \\\\u7528\\\\u4e8e\\\\u8fd4\\\\u56de\\\\u7b2c\\\\u4e00\\\\u4e2a\\\\u5339\\\\u914d\\\\u7684\\\\u5de5\\\\u5177\\\\uff08\\\\u5982\\\\u672a\\\\u627e\\\\u5230\\\\u5219\\\\u8fd4\\\\u56de None\\\\uff09\\\\n        tool_def = next((t for t in self.tool_definitions if t.name == tool_name), None)\\\\n        if not tool_def:\\\\n            return f\\\\\"\\\\u9519\\\\u8bef: \\\\u672a\\\\u627e\\\\u5230\\\\u5de5\\\\u5177 \\'{tool_name}\\'\\\\\"\\\\n        \\\\n        try:\\\\n            # \\\\u8c03\\\\u7528\\\\u5de5\\\\u5177\\\\u5e76\\\\u7b49\\\\u5f85\\\\u7ed3\\\\u679c\\\\n            result = await self.client.call_tool(tool_name, arguments=params)\\\\n            \\\\n            # \\\\u5904\\\\u7406\\\\u7ed3\\\\u679c\\\\n            if result and hasattr(result, \\'content\\'):\\\\n                return result.content\\\\n            else:\\\\n                return \\\\\"\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u672a\\\\u8fd4\\\\u56de\\\\u4efb\\\\u4f55\\\\u7ed3\\\\u679c\\\\\"\\\\n        \\\\n        except Exception as e:\\\\n            return f\\\\\"\\\\u6267\\\\u884c\\\\u8fc7\\\\u7a0b\\\\u4e2d\\\\u51fa\\\\u9519: {str(e)}\\\\\"\\\\n    \\\\n    async def stop(self):\\\\n        \\\\\"\\\\\"\\\\\"\\\\u505c\\\\u6b62\\\\u5ba2\\\\u6237\\\\u7aef\\\\u548c\\\\u670d\\\\u52a1\\\\u5668\\\\\"\\\\\"\\\\\"\\\\n        print(\\\\\"\\\\u6b63\\\\u5728\\\\u5173\\\\u95ed MCP \\\\u5ba2\\\\u6237\\\\u7aef...\\\\\")\\\\n        await self.exit_stack.aclose()\\\\n        print(\\\\\"\\\\u5df2\\\\u5173\\\\u95ed\\\\\")\\\\n    \\\\n    def print_help(self):\\\\n        \\\\\"\\\\\"\\\\\"\\\\u6253\\\\u5370\\\\u5e2e\\\\u52a9\\\\u4fe1\\\\u606f\\\\\"\\\\\"\\\\\"\\\\n        print(\\\\\"\\\\\\\\n\\\\u53ef\\\\u7528\\\\u547d\\\\u4ee4:\\\\\")\\\\n        print(\\\\\"  help - \\\\u663e\\\\u793a\\\\u6b64\\\\u5e2e\\\\u52a9\\\\u4fe1\\\\u606f\\\\\")\\\\n        print(\\\\\"  list - \\\\u5217\\\\u51fa\\\\u53ef\\\\u7528\\\\u5de5\\\\u5177\\\\\")\\\\n        print(\\\\\"  call <\\\\u5de5\\\\u5177\\\\u540d> <\\\\u53c2\\\\u6570JSON> - \\\\u8c03\\\\u7528\\\\u5de5\\\\u5177\\\\\")\\\\n        print(\\\\\"  exit - \\\\u9000\\\\u51fa\\\\u7a0b\\\\u5e8f\\\\\")\\\\n        print(\\\\\"\\\\\\\\n\\\\u793a\\\\u4f8b:\\\\\")\\\\n        print(\\\\\"  call get_weather_warning {\\\\\\\\\\\\\"location\\\\\\\\\\\\\": \\\\\\\\\\\\\"101010100\\\\\\\\\\\\\"}\\\\\")\\\\n        print(\\\\\"  call get_daily_forecast 116.41,39.92\\\\\")\\\\n        print(\\\\\"  call get_daily_forecast 101010100 7\\\\\")\\\\n    \\\\n    def print_tools(self):\\\\n        \\\\\"\\\\\"\\\\\"\\\\u6253\\\\u5370\\\\u5de5\\\\u5177\\\\u5217\\\\u8868\\\\u548c\\\\u63cf\\\\u8ff0\\\\\"\\\\\"\\\\\"\\\\n        print(\\\\\"\\\\\\\\n\\\\u53ef\\\\u7528\\\\u5de5\\\\u5177:\\\\\")\\\\n        for tool in self.tool_definitions:\\\\n            print(f\\\\\"  {tool.name} - {tool.description}\\\\\")\\\\n            if hasattr(tool, \\'inputSchema\\') and tool.inputSchema:\\\\n                print(\\\\\"    \\\\u53c2\\\\u6570:\\\\\")\\\\n                props = tool.inputSchema.get(\\'properties\\', {})\\\\n                for param_name, param_info in props.items():\\\\n                    param_type = param_info.get(\\'type\\', \\'unknown\\')\\\\n                    param_desc = param_info.get(\\'description\\', \\'\\')\\\\n                    print(f\\\\\"      {param_name} ({param_type}): {param_desc}\\\\\")\\\\n\\\\nasync def main():\\\\n    \\\\\"\\\\\"\\\\\"\\\\u4e3b\\\\u51fd\\\\u6570\\\\\"\\\\\"\\\\\"\\\\n    # \\\\u786e\\\\u5b9a\\\\u670d\\\\u52a1\\\\u5668\\\\u8def\\\\u5f84\\\\n    server_path = os.path.join(\\\\n        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\\\\n        \\\\\"server\\\\\",\\\\n        \\\\\"weather_server.py\\\\\"\\\\n    )\\\\n    \\\\n    # \\\\u68c0\\\\u67e5\\\\u670d\\\\u52a1\\\\u5668\\\\u6587\\\\u4ef6\\\\u662f\\\\u5426\\\\u5b58\\\\u5728\\\\n    if not os.path.exists(server_path):\\\\n        print(f\\\\\"\\\\u9519\\\\u8bef: \\\\u670d\\\\u52a1\\\\u5668\\\\u6587\\\\u4ef6\\\\u4e0d\\\\u5b58\\\\u5728: {server_path}\\\\\")\\\\n        return\\\\n    \\\\n    # \\\\u521b\\\\u5efa\\\\u5ba2\\\\u6237\\\\u7aef\\\\u5e94\\\\u7528\\\\n    app = SimpleClientApp([\\\\\"python\\\\\", server_path])\\\\n    \\\\n    try:\\\\n        # \\\\u542f\\\\u52a8\\\\u5ba2\\\\u6237\\\\u7aef\\\\n        await app.start()\\\\n        \\\\n        # \\\\u4e3b\\\\u5faa\\\\u73af\\\\n        while True:\\\\n            try:\\\\n                # \\\\u83b7\\\\u53d6\\\\u7528\\\\u6237\\\\u8f93\\\\u5165\\\\n                command = input(\\\\\"\\\\\\\\n> \\\\\").strip()\\\\n                \\\\n                # \\\\u5904\\\\u7406\\\\u547d\\\\u4ee4\\\\n                if command == \\\\\"exit\\\\\":\\\\n                    break\\\\n                \\\\n                elif command == \\\\\"help\\\\\":\\\\n                    app.print_help()\\\\n                \\\\n                elif command == \\\\\"list\\\\\":\\\\n                    app.print_tools()\\\\n                \\\\n                elif command.startswith(\\\\\"call \\\\\"):\\\\n                    # \\\\u89e3\\\\u6790\\\\u547d\\\\u4ee4\\\\n                    parts = command[5:].strip().split(\\\\\" \\\\\", 1)\\\\n                    if len(parts) < 1:\\\\n                        print(\\\\\"\\\\u9519\\\\u8bef: \\\\u7f3a\\\\u5c11\\\\u5de5\\\\u5177\\\\u540d\\\\\")\\\\n                        continue\\\\n                    \\\\n                    tool_name = parts[0]\\\\n                    \\\\n                    # \\\\u89e3\\\\u6790\\\\u53c2\\\\u6570\\\\n                    params = {}\\\\n                    if len(parts) > 1:\\\\n                        params_str = parts[1]\\\\n                        \\\\n                        # \\\\u5c1d\\\\u8bd5\\\\u89e3\\\\u6790\\\\u4e3a JSON\\\\n                        try:\\\\n                            params = json.loads(params_str)\\\\n                        except json.JSONDecodeError:\\\\n                            # \\\\u5982\\\\u679c\\\\u4e0d\\\\u662f JSON\\\\uff0c\\\\u5c1d\\\\u8bd5\\\\u7b80\\\\u5355\\\\u89e3\\\\u6790\\\\n                            try:\\\\n                                # \\\\u5bf9\\\\u4e8e get_weather_warning\\\\uff0c\\\\u53ea\\\\u9700\\\\u8981\\\\u4e00\\\\u4e2a\\\\u4f4d\\\\u7f6e\\\\u53c2\\\\u6570\\\\n                                if tool_name == \\\\\"get_weather_warning\\\\\":\\\\n                                    params = {\\\\\"location\\\\\": params_str}\\\\n                                \\\\n                                # \\\\u5bf9\\\\u4e8e get_daily_forecast\\\\uff0c\\\\u9700\\\\u8981\\\\u4f4d\\\\u7f6e\\\\u548c\\\\u53ef\\\\u9009\\\\u7684\\\\u5929\\\\u6570\\\\n                                elif tool_name == \\\\\"get_daily_forecast\\\\\":\\\\n                                    forecast_args = params_str.split()\\\\n                                    if len(forecast_args) < 1:\\\\n                                        print(\\\\\"\\\\u9519\\\\u8bef: get_daily_forecast \\\\u81f3\\\\u5c11\\\\u9700\\\\u8981\\\\u4e00\\\\u4e2a\\\\u4f4d\\\\u7f6e\\\\u53c2\\\\u6570\\\\\")\\\\n                                        continue\\\\n                                    \\\\n                                    params = {\\\\\"location\\\\\": forecast_args[0]}\\\\n                                    \\\\n                                    # \\\\u5982\\\\u679c\\\\u63d0\\\\u4f9b\\\\u4e86\\\\u5929\\\\u6570\\\\u53c2\\\\u6570\\\\n                                    if len(forecast_args) > 1:\\\\n                                        try:\\\\n                                            params[\\\\\"days\\\\\"] = int(forecast_args[1])\\\\n                                        except ValueError:\\\\n                                            print(\\\\\"\\\\u9519\\\\u8bef: days \\\\u53c2\\\\u6570\\\\u5fc5\\\\u987b\\\\u662f\\\\u6574\\\\u6570\\\\\")\\\\n                                            continue\\\\n                                else:\\\\n                                    print(f\\\\\"\\\\u9519\\\\u8bef: \\\\u65e0\\\\u6cd5\\\\u89e3\\\\u6790 {tool_name} \\\\u7684\\\\u53c2\\\\u6570\\\\\")\\\\n                                    continue\\\\n                            \\\\n                            except Exception as e:\\\\n                                print(f\\\\\"\\\\u53c2\\\\u6570\\\\u89e3\\\\u6790\\\\u9519\\\\u8bef: {str(e)}\\\\\")\\\\n                                continue\\\\n                    \\\\n                    # \\\\u6267\\\\u884c\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\n                    print(\\\\\"\\\\u6b63\\\\u5728\\\\u8c03\\\\u7528\\\\u5de5\\\\u5177...\\\\\")\\\\n                    result = await app.execute_tool(tool_name, params)\\\\n                    print(\\\\\"\\\\\\\\n\\\\u7ed3\\\\u679c:\\\\\")\\\\n                    print(result)\\\\n                \\\\n                else:\\\\n                    print(f\\\\\"\\\\u672a\\\\u77e5\\\\u547d\\\\u4ee4: {command}\\\\\")\\\\n                    print(\\\\\"\\\\u4f7f\\\\u7528 \\'help\\' \\\\u67e5\\\\u770b\\\\u53ef\\\\u7528\\\\u547d\\\\u4ee4\\\\\")\\\\n            \\\\n            except KeyboardInterrupt:\\\\n                print(\\\\\"\\\\\\\\n\\\\u64cd\\\\u4f5c\\\\u88ab\\\\u4e2d\\\\u65ad\\\\\")\\\\n                break\\\\n            \\\\n            except Exception as e:\\\\n                print(f\\\\\"\\\\u9519\\\\u8bef: {str(e)}\\\\\")\\\\n    \\\\n    except Exception as e:\\\\n        print(f\\\\\"\\\\u5ba2\\\\u6237\\\\u7aef\\\\u9519\\\\u8bef: {str(e)}\\\\\")\\\\n    \\\\n    finally:\\\\n        # \\\\u505c\\\\u6b62\\\\u5ba2\\\\u6237\\\\u7aef\\\\n        await app.stop()\\\\n\\\\nif __name__ == \\\\\"__main__\\\\\":\\\\n    try:\\\\n        asyncio.run(main())\\\\n    except KeyboardInterrupt:\\\\n        print(\\\\\"\\\\\\\\n\\\\u7a0b\\\\u5e8f\\\\u88ab\\\\u4e2d\\\\u65ad\\\\\")\\\\n        sys.exit(0) \", \"timestamp\": \"2025-11-09T00:16:14+0800\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"read_project_file\", \"args\": {\"name\": \"mcp-demo/client/mcp_client_deepseek.py\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"\\\\\"\\\\\"\\\\\"\\\\n# \\\\u96c6\\\\u6210 DeepSeek \\\\u7684 MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\n#\\\\u53c2\\\\u8003\\\\u5b98\\\\u65b9\\\\u6848\\\\u4f8b\\\\uff1ahttps://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-chatbot/mcp_simple_chatbot/main.py\\\\n\\\\n\\\\n# \\\\u672c\\\\u6a21\\\\u5757\\\\u5b9e\\\\u73b0\\\\u4e86\\\\u4e00\\\\u4e2a\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u5ba2\\\\u6237\\\\u7aef\\\\uff0c\\\\u8be5\\\\u5ba2\\\\u6237\\\\u7aef\\\\u4f7f\\\\u7528 DeepSeek \\\\u7684 API\\\\n# \\\\u6765\\\\u5904\\\\u7406\\\\u67e5\\\\u8be2\\\\u5e76\\\\u4e0e MCP \\\\u5de5\\\\u5177\\\\u8fdb\\\\u884c\\\\u4ea4\\\\u4e92\\\\u3002\\\\u5b83\\\\u6f14\\\\u793a\\\\u4e86\\\\u5982\\\\u4f55\\\\uff1a\\\\n# 1. \\\\u8fde\\\\u63a5\\\\u5230 MCP \\\\u670d\\\\u52a1\\\\u5668\\\\n# 2. \\\\u4f7f\\\\u7528 DeepSeek \\\\u7684 API \\\\u6765\\\\u5904\\\\u7406\\\\u67e5\\\\u8be2\\\\n# 3. \\\\u5904\\\\u7406\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\u548c\\\\u54cd\\\\u5e94\\\\n# 4. \\\\u7ef4\\\\u62a4\\\\u4e00\\\\u4e2a\\\\u4ea4\\\\u4e92\\\\u5f0f\\\\u804a\\\\u5929\\\\u5faa\\\\u73af\\\\n\\\\n# \\\\u6240\\\\u9700\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\uff1a\\\\n# DEEPSEEK_API_KEY\\\\uff1aDeepSeek API \\\\u5bc6\\\\u94a5 (\\\\u683c\\\\u5f0f\\\\uff1ask-xxxx...)\\\\n# DEEPSEEK_BASE_URL\\\\uff1aDeepSeek API \\\\u57fa\\\\u7840 URL (https://api.deepseek.com)\\\\n# DEEPSEEK_MODEL\\\\uff1aDeepSeek \\\\u6a21\\\\u578b\\\\u540d\\\\u79f0 (\\\\u4f8b\\\\u5982 deepseek-chat)\\\\n\\\\nAuthor: FlyAIBox\\\\nDate: 2025.10.11\\\\n\\\\\"\\\\\"\\\\\"\\\\n\\\\nimport json\\\\nimport asyncio\\\\nimport logging\\\\nimport os\\\\nfrom typing import Optional, Dict, Any, List, Tuple\\\\nfrom contextlib import AsyncExitStack\\\\n\\\\nfrom openai import OpenAI\\\\nfrom dotenv import load_dotenv\\\\n\\\\nfrom mcp import ClientSession, StdioServerParameters\\\\nfrom mcp.client.stdio import stdio_client\\\\n\\\\n# \\\\u914d\\\\u7f6e\\\\u65e5\\\\u5fd7\\\\u8bb0\\\\u5f55\\\\nlogging.basicConfig(\\\\n    level=logging.INFO,\\\\n    format=\\\\\"%(asctime)s - %(levelname)s - %(message)s\\\\\"\\\\n)\\\\nlogger = logging.getLogger(__name__)\\\\n\\\\nload_dotenv()\\\\n\\\\n\\\\nclass Configuration:\\\\n    \\\\\"\\\\\"\\\\\"\\\\u914d\\\\u7f6e\\\\u7ba1\\\\u7406\\\\u7c7b\\\\uff0c\\\\u8d1f\\\\u8d23\\\\u7ba1\\\\u7406\\\\u548c\\\\u9a8c\\\\u8bc1\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u521d\\\\u59cb\\\\u5316\\\\u914d\\\\u7f6e\\\\u5e76\\\\u52a0\\\\u8f7d\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\"\\\\\"\\\\\"\\\\n        self.load_env()\\\\n        self._validate_env()\\\\n        \\\\n    @staticmethod\\\\n    def load_env() -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u4ece.env\\\\u6587\\\\u4ef6\\\\u52a0\\\\u8f7d\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\"\\\\\"\\\\\"\\\\n        load_dotenv()\\\\n        \\\\n    def _validate_env(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u9a8c\\\\u8bc1\\\\u5fc5\\\\u9700\\\\u7684\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\u662f\\\\u5426\\\\u5b58\\\\u5728\\\\\"\\\\\"\\\\\"\\\\n        required_vars = [\\\\\"DEEPSEEK_API_KEY\\\\\"]\\\\n        missing_vars = [var for var in required_vars if not os.getenv(var)]\\\\n        if missing_vars:\\\\n            raise ValueError(f\\\\\"\\\\u7f3a\\\\u5c11\\\\u5fc5\\\\u9700\\\\u7684\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf: {\\', \\'.join(missing_vars)}\\\\\")\\\\n    \\\\n    @property\\\\n    def api_key(self) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6 DeepSeek API \\\\u5bc6\\\\u94a5\\\\\"\\\\\"\\\\\"\\\\n        return os.getenv(\\\\\"DEEPSEEK_API_KEY\\\\\", \\\\\"\\\\\")\\\\n    \\\\n    @property\\\\n    def base_url(self) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6 DeepSeek API \\\\u57fa\\\\u7840 URL\\\\\"\\\\\"\\\\\"\\\\n        return os.getenv(\\\\\"DEEPSEEK_BASE_URL\\\\\", \\\\\"https://api.deepseek.com\\\\\")\\\\n    \\\\n    @property\\\\n    def model(self) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6 DeepSeek \\\\u6a21\\\\u578b\\\\u540d\\\\u79f0\\\\\"\\\\\"\\\\\"\\\\n        return os.getenv(\\\\\"DEEPSEEK_MODEL\\\\\", \\\\\"deepseek-chat\\\\\")\\\\n\\\\nclass Tool:\\\\n    \\\\\"\\\\\"\\\\\"MCP \\\\u5de5\\\\u5177\\\\u7c7b\\\\uff0c\\\\u8868\\\\u793a\\\\u4e00\\\\u4e2a\\\\u5177\\\\u6709\\\\u5c5e\\\\u6027\\\\u7684\\\\u5de5\\\\u5177\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self, name: str, description: str, input_schema: Dict[str, Any]) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u521d\\\\u59cb\\\\u5316\\\\u5de5\\\\u5177\\\\n        \\\\n        Args:\\\\n            name: \\\\u5de5\\\\u5177\\\\u540d\\\\u79f0\\\\n            description: \\\\u5de5\\\\u5177\\\\u63cf\\\\u8ff0\\\\n            input_schema: \\\\u8f93\\\\u5165\\\\u53c2\\\\u6570\\\\u6a21\\\\u5f0f\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.name = name\\\\n        self.description = description\\\\n        self.input_schema = input_schema\\\\n        \\\\n    def to_openai_format(self) -> Dict[str, Any]:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u5c06\\\\u5de5\\\\u5177\\\\u8f6c\\\\u6362\\\\u4e3a OpenAI API \\\\u683c\\\\u5f0f\\\\\"\\\\\"\\\\\"\\\\n        return {\\\\n            \\\\\"type\\\\\": \\\\\"function\\\\\",\\\\n            \\\\\"function\\\\\": {\\\\n                \\\\\"name\\\\\": self.name,\\\\n                \\\\\"description\\\\\": self.description,\\\\n                \\\\\"parameters\\\\\": self.input_schema\\\\n            }\\\\n        }\\\\n\\\\nclass MCPServer:\\\\n    \\\\\"\\\\\"\\\\\"MCP \\\\u670d\\\\u52a1\\\\u5668\\\\u7ba1\\\\u7406\\\\u7c7b\\\\uff0c\\\\u5904\\\\u7406\\\\u670d\\\\u52a1\\\\u5668\\\\u8fde\\\\u63a5\\\\u548c\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self, server_path: str) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u521d\\\\u59cb\\\\u5316\\\\u670d\\\\u52a1\\\\u5668\\\\u7ba1\\\\u7406\\\\u5668\\\\n        \\\\n        Args:\\\\n            server_path: \\\\u670d\\\\u52a1\\\\u5668\\\\u811a\\\\u672c\\\\u8def\\\\u5f84\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.server_path = server_path\\\\n        self.session: Optional[ClientSession] = None\\\\n        self.exit_stack = AsyncExitStack()\\\\n        self._cleanup_lock = asyncio.Lock()\\\\n        \\\\n    async def initialize(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u521d\\\\u59cb\\\\u5316\\\\u670d\\\\u52a1\\\\u5668\\\\u8fde\\\\u63a5\\\\uff0c\\\\u5305\\\\u542b\\\\u91cd\\\\u8bd5\\\\u673a\\\\u5236\\\\\"\\\\\"\\\\\"\\\\n        max_retries = 3\\\\n        retry_delay = 1.0\\\\n        \\\\n        for attempt in range(max_retries):\\\\n            try:\\\\n                if not os.path.exists(self.server_path):\\\\n                    raise FileNotFoundError(f\\\\\"\\\\u627e\\\\u4e0d\\\\u5230\\\\u670d\\\\u52a1\\\\u5668\\\\u6587\\\\u4ef6: {self.server_path}\\\\\")\\\\n                \\\\n                server_params = StdioServerParameters(\\\\n                    command=\\'python\\',\\\\n                    args=[self.server_path],\\\\n                    env=None\\\\n                )\\\\n                \\\\n                stdio_transport = await self.exit_stack.enter_async_context(\\\\n                    stdio_client(server_params)\\\\n                )\\\\n                stdio, write = stdio_transport\\\\n                \\\\n                self.session = await self.exit_stack.enter_async_context(\\\\n                    ClientSession(stdio, write)\\\\n                )\\\\n                await self.session.initialize()\\\\n                logger.info(\\\\\"\\\\u6210\\\\u529f\\\\u8fde\\\\u63a5\\\\u5230 MCP \\\\u670d\\\\u52a1\\\\u5668\\\\\")\\\\n                break\\\\n                \\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u7b2c {attempt + 1}/{max_retries} \\\\u6b21\\\\u5c1d\\\\u8bd5\\\\u5931\\\\u8d25: {str(e)}\\\\\")\\\\n                if attempt < max_retries - 1:\\\\n                    await asyncio.sleep(retry_delay)\\\\n                else:\\\\n                    raise\\\\n                    \\\\n    async def list_tools(self) -> List[Tool]:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6\\\\u670d\\\\u52a1\\\\u5668\\\\u63d0\\\\u4f9b\\\\u7684\\\\u53ef\\\\u7528\\\\u5de5\\\\u5177\\\\u5217\\\\u8868\\\\\"\\\\\"\\\\\"\\\\n        if not self.session:\\\\n            raise RuntimeError(\\\\\"\\\\u670d\\\\u52a1\\\\u5668\\\\u672a\\\\u521d\\\\u59cb\\\\u5316\\\\\")\\\\n            \\\\n        response = await self.session.list_tools()\\\\n        return [\\\\n            Tool(tool.name, tool.description, tool.inputSchema)\\\\n            for tool in response.tools\\\\n        ]\\\\n        \\\\n    async def execute_tool(\\\\n        self,\\\\n        tool_name: str,\\\\n        arguments: Dict[str, Any],\\\\n        retries: int = 2,\\\\n        delay: float = 1.0\\\\n    ) -> Any:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u6267\\\\u884c\\\\u5de5\\\\u5177\\\\uff0c\\\\u5305\\\\u542b\\\\u91cd\\\\u8bd5\\\\u673a\\\\u5236\\\\n        \\\\n        Args:\\\\n            tool_name: \\\\u5de5\\\\u5177\\\\u540d\\\\u79f0\\\\n            arguments: \\\\u5de5\\\\u5177\\\\u53c2\\\\u6570\\\\n            retries: \\\\u91cd\\\\u8bd5\\\\u6b21\\\\u6570\\\\n            delay: \\\\u91cd\\\\u8bd5\\\\u5ef6\\\\u8fdf\\\\u65f6\\\\u95f4\\\\uff08\\\\u79d2\\\\uff09\\\\n            \\\\n        Returns:\\\\n            \\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u7ed3\\\\u679c\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        if not self.session:\\\\n            raise RuntimeError(\\\\\"\\\\u670d\\\\u52a1\\\\u5668\\\\u672a\\\\u521d\\\\u59cb\\\\u5316\\\\\")\\\\n            \\\\n        for attempt in range(retries):\\\\n            try:\\\\n                logger.info(f\\\\\"\\\\u6267\\\\u884c\\\\u5de5\\\\u5177 {tool_name}\\\\uff0c\\\\u53c2\\\\u6570: {arguments}\\\\\")\\\\n                result = await self.session.call_tool(tool_name, arguments)\\\\n                return result\\\\n                \\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u5931\\\\u8d25 (\\\\u7b2c {attempt + 1}/{retries} \\\\u6b21\\\\u5c1d\\\\u8bd5): {str(e)}\\\\\")\\\\n                if attempt < retries - 1:\\\\n                    await asyncio.sleep(delay)\\\\n                else:\\\\n                    raise\\\\n                    \\\\n    async def cleanup(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u6e05\\\\u7406\\\\u670d\\\\u52a1\\\\u5668\\\\u8d44\\\\u6e90\\\\\"\\\\\"\\\\\"\\\\n        async with self._cleanup_lock:\\\\n            try:\\\\n                await self.exit_stack.aclose()\\\\n                self.session = None\\\\n                logger.info(\\\\\"\\\\u670d\\\\u52a1\\\\u5668\\\\u8d44\\\\u6e90\\\\u6e05\\\\u7406\\\\u5b8c\\\\u6210\\\\\")\\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u6e05\\\\u7406\\\\u8fc7\\\\u7a0b\\\\u4e2d\\\\u51fa\\\\u9519: {str(e)}\\\\\")\\\\n\\\\nclass MCPClient:\\\\n    \\\\\"\\\\\"\\\\\"MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\u5b9e\\\\u73b0\\\\uff0c\\\\u96c6\\\\u6210\\\\u4e86 DeepSeek API\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self, config: Configuration) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u521d\\\\u59cb\\\\u5316 MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\n        \\\\n        Args:\\\\n            config: \\\\u914d\\\\u7f6e\\\\u5bf9\\\\u8c61\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.config = config\\\\n        self.server: Optional[MCPServer] = None\\\\n        self.client = OpenAI(\\\\n            api_key=config.api_key,\\\\n            base_url=config.base_url\\\\n        )\\\\n        \\\\n    async def initialize(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u521d\\\\u59cb\\\\u5316\\\\u5ba2\\\\u6237\\\\u7aef\\\\u5e76\\\\u8fde\\\\u63a5\\\\u5230\\\\u670d\\\\u52a1\\\\u5668\\\\\"\\\\\"\\\\\"\\\\n        server_path = os.path.join(\\\\n            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\\\\n            \\\\\"server\\\\\",\\\\n            \\\\\"weather_server.py\\\\\"\\\\n        )\\\\n        self.server = MCPServer(server_path)\\\\n        await self.server.initialize()\\\\n        \\\\n\\\\n    async def process_query(self, query: str) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u5904\\\\u7406\\\\u7528\\\\u6237\\\\u67e5\\\\u8be2\\\\uff0c\\\\u96c6\\\\u6210\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\uff0c\\\\u652f\\\\u6301\\\\u591a\\\\u8f6e\\\\u5de5\\\\u5177\\\\u4ea4\\\\u4e92\\\\n\\\\n        Args:\\\\n            query: \\\\u7528\\\\u6237\\\\u67e5\\\\u8be2\\\\u5b57\\\\u7b26\\\\u4e32\\\\n\\\\n        Returns:\\\\n            \\\\u5904\\\\u7406\\\\u540e\\\\u7684\\\\u54cd\\\\u5e94\\\\u7ed3\\\\u679c\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        if not self.server:\\\\n            raise RuntimeError(\\\\\"\\\\u5ba2\\\\u6237\\\\u7aef\\\\u672a\\\\u521d\\\\u59cb\\\\u5316\\\\\")\\\\n\\\\n        system_prompt = (\\\\n            \\\\\"You are a helpful assistant specializing in weather information.\\\\\\\\n\\\\\"\\\\n            \\\\\"You have access to the MCP Weather Server tool with the following functions:\\\\\\\\n\\\\\"\\\\n            \\\\\"- get_weather_warning(city_id=None, latitude=None, longitude=None): Retrieves weather disaster warnings for a specified city ID or coordinates.\\\\\\\\n\\\\\"\\\\n            \\\\\"- get_daily_forecast(city_id=None, latitude=None, longitude=None): Retrieves the multi-day weather forecast for a specified city ID or coordinates.\\\\\\\\n\\\\\"\\\\n            \\\\\"\\\\\\\\n\\\\\"\\\\n            \\\\\"Core Instructions:\\\\\\\\n\\\\\"\\\\n            \\\\\"1.  **Carefully analyze the user\\'s request**: Understand all components of the user\\'s query. Determine if the user needs weather warning information, weather forecast information, or both.\\\\\\\\n\\\\\"\\\\n            \\\\\"2.  **Identify Information Needs**:\\\\\\\\n\\\\\"\\\\n            \\\\\"    * If the user only asks for warnings (e.g., \\\\\\\\\\\\\"Are there any warnings in Beijing?\\\\\\\\\\\\\"), only use `get_weather_warning`.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * If the user only asks for the forecast (e.g., \\\\\\\\\\\\\"What\\'s the weather like in Beijing tomorrow?\\\\\\\\\\\\\"), only use `get_daily_forecast`.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **If the user\\'s question includes multiple aspects**, such as asking about **warning status** and also asking **if it\\'s suitable for a certain activity** (which implies a query about future weather, like \\\\\\\\\\\\\"Have there been high temperature warnings in Beijing in the last week? Is it suitable for outdoor activities?\\\\\\\\\\\\\"), you need to **call both tools sequentially**.\\\\\\\\n\\\\\"\\\\n            \\\\\"3.  **Call Tools as Needed**:\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **Prioritize getting warning information**: If warning information is needed, first call `get_weather_warning`.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **Get the weather forecast**: If the user mentions a specific time period (e.g., \\\\\\\\\\\\\"weekend\\\\\\\\\\\\\", \\\\\\\\\\\\\"next three days\\\\\\\\\\\\\", \\\\\\\\\\\\\"next week\\\\\\\\\\\\\") or asks about activity suitability (which typically concerns the next few days), call `get_daily_forecast` to get the forecast for the corresponding period. For vague phrases like \\\\\\\\\\\\\"last week\\\\\\\\\\\\\" or \\\\\\\\\\\\\"recently\\\\\\\\\\\\\", interpret it as asking about *current* conditions and the *upcoming* few days (covered by the forecast). For questions like \\\\\\\\\\\\\"Is it suitable for outdoor activities?\\\\\\\\\\\\\", you should get the forecast for at least the next 2-3 days (e.g., today, tomorrow, the day after tomorrow, or the upcoming weekend) to support your judgment.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **Ensure tool call order**: When multiple tools need to be called, they should be called in a logical sequence. For example, first get the warning, then get the forecast. Wait for one tool to finish executing before deciding whether to call the next tool or generate a response.\\\\\\\\n\\\\\"\\\\n            \\\\\"4.  **Information Integration and Response**:\\\\\\\\n\\\\\"\\\\n            \\\\\"    * After obtaining all necessary information (warning, forecast), you **must synthesize and analyze this information**.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **Completely answer the user\\'s question**: Ensure you answer all parts of the user\\'s query.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **Provide advice**: If the user asks about activity suitability, based on the retrieved warning status and forecast information (temperature, weather condition - clear/rainy, wind strength, etc.), provide a clear, data-supported recommendation (e.g., \\\\\\\\\\\\\"Currently there are no high temperature warnings, but it\\'s expected to rain this weekend, so it\\'s not very suitable for outdoor activities,\\\\\\\\\\\\\" or \\\\\\\\\\\\\"It will be sunny for the next few days with no warnings, suitable for outdoor activities.\\\\\\\\\\\\\").\\\\\\\\n\\\\\"\\\\n            \\\\\"5.  **Tool Usage Details**:\\\\\\\\n\\\\\"\\\\n            \\\\\"    * When using the tools, retain the full context of the user\\'s original question.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * Unless explicitly requested by the user, do not insert specific times of day (e.g., \\\\\\\\\\\\\"3 PM\\\\\\\\\\\\\") into the search query or your response.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * When city information is needed, if the user provides a city name (e.g., \\\\\\\\\\\\\"Beijing\\\\\\\\\\\\\"), use the corresponding `city_id` (e.g., Beijing\\'s city_id might be \\'101010100\\').\\\\\\\\n\\\\\"\\\\n        )\\\\n\\\\n        messages = [\\\\n            {\\\\\"role\\\\\": \\\\\"system\\\\\", \\\\\"content\\\\\": system_prompt},\\\\n            {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": query}\\\\n        ]\\\\n\\\\n        tools = await self.server.list_tools()\\\\n        available_tools = [tool.to_openai_format() for tool in tools]\\\\n\\\\n        max_tool_turns = 5 # \\\\u8bbe\\\\u7f6e\\\\u4e00\\\\u4e2a\\\\u6700\\\\u5927\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\u56de\\\\u5408\\\\u6570\\\\uff0c\\\\u9632\\\\u6b62\\\\u65e0\\\\u9650\\\\u5faa\\\\u73af\\\\n\\\\n        for _ in range(max_tool_turns):\\\\n            try:\\\\n                response = self.client.chat.completions.create(\\\\n                    model=self.config.model,\\\\n                    messages=messages,\\\\n                    tools=available_tools,\\\\n                    tool_choice=\\\\\"auto\\\\\" # \\\\u5141\\\\u8bb8\\\\u6a21\\\\u578b\\\\u51b3\\\\u5b9a\\\\u662f\\\\u5426\\\\u8c03\\\\u7528\\\\u5de5\\\\u5177\\\\n                )\\\\n\\\\n                content = response.choices[0].message\\\\n                finish_reason = response.choices[0].finish_reason\\\\n\\\\n                if finish_reason == \\\\\"stop\\\\\":\\\\n                    # \\\\u6a21\\\\u578b\\\\u751f\\\\u6210\\\\u4e86\\\\u6700\\\\u7ec8\\\\u7684\\\\u6587\\\\u672c\\\\u56de\\\\u590d\\\\n                    messages.append({\\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": content.content})\\\\n                    return content.content\\\\n                elif finish_reason == \\\\\"tool_calls\\\\\":\\\\n                    # \\\\u6a21\\\\u578b\\\\u9884\\\\u6d4b\\\\u4e86\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\n                    messages.append(content.model_dump()) # \\\\u5c06 assistant tool_calls \\\\u6d88\\\\u606f\\\\u6dfb\\\\u52a0\\\\u5230\\\\u5386\\\\u53f2\\\\n\\\\n                    tool_outputs = []\\\\n                    for tool_call in content.tool_calls:\\\\n                        tool_name = tool_call.function.name\\\\n                        tool_args = json.loads(tool_call.function.arguments)\\\\n\\\\n                        try:\\\\n                            # \\\\u6267\\\\u884c\\\\u5de5\\\\u5177\\\\n                            result = await self.server.execute_tool(tool_name, tool_args)\\\\n                            logger.info(f\\\\\"\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u5b8c\\\\u6210: {tool_name}\\\\\")\\\\n                            # \\\\u5c06\\\\u5de5\\\\u5177\\\\u7ed3\\\\u679c\\\\u6dfb\\\\u52a0\\\\u5230 tool_outputs \\\\u5217\\\\u8868\\\\n                            tool_outputs.append({\\\\n                                \\\\\"tool_call_id\\\\\": tool_call.id,\\\\n                                # MCP tool results have \\'content\\' which is a list of Parts\\\\n                                # Assuming text content is the first part\\\\n                                \\\\\"output\\\\\": result.content[0].text\\\\n                            })\\\\n                        except Exception as tool_exec_error:\\\\n                            logger.error(f\\\\\"\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u5931\\\\u8d25 {tool_name}: {str(tool_exec_error)}\\\\\")\\\\n                            # \\\\u5982\\\\u679c\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u5931\\\\u8d25\\\\uff0c\\\\u4e5f\\\\u9700\\\\u8981\\\\u5411\\\\u6a21\\\\u578b\\\\u53cd\\\\u9988\\\\u5931\\\\u8d25\\\\u4fe1\\\\u606f\\\\n                            tool_outputs.append({\\\\n                                \\\\\"tool_call_id\\\\\": tool_call.id,\\\\n                                \\\\\"output\\\\\": f\\\\\"Error executing tool {tool_name}: {str(tool_exec_error)}\\\\\"\\\\n                            })\\\\n\\\\n                    # \\\\u5c06\\\\u6240\\\\u6709\\\\u5de5\\\\u5177\\\\u7ed3\\\\u679c\\\\u4f5c\\\\u4e3a tool \\\\u6d88\\\\u606f\\\\u6dfb\\\\u52a0\\\\u5230\\\\u5386\\\\u53f2\\\\n                    # \\\\u6ce8\\\\u610f\\\\uff1aOpenAI API \\\\u8981\\\\u6c42\\\\u6bcf\\\\u4e2a tool_call_id \\\\u5bf9\\\\u5e94\\\\u4e00\\\\u4e2a tool \\\\u6d88\\\\u606f\\\\n                    # \\\\u5982\\\\u679c\\\\u6a21\\\\u578b\\\\u9884\\\\u6d4b\\\\u4e86\\\\u591a\\\\u4e2a\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\uff0c\\\\u8fd9\\\\u91cc\\\\u9700\\\\u8981\\\\u4e3a\\\\u6bcf\\\\u4e2a\\\\u7ed3\\\\u679c\\\\u6dfb\\\\u52a0 tool \\\\u6d88\\\\u606f\\\\n                    for output in tool_outputs:\\\\n                        messages.append({\\\\n                            \\\\\"role\\\\\": \\\\\"tool\\\\\",\\\\n                            \\\\\"content\\\\\": output[\\\\\"output\\\\\"],\\\\n                            \\\\\"tool_call_id\\\\\": output[\\\\\"tool_call_id\\\\\"],\\\\n                        })\\\\n\\\\n                    # \\\\u7ee7\\\\u7eed\\\\u4e0b\\\\u4e00\\\\u8f6e\\\\u5faa\\\\u73af\\\\uff0c\\\\u5c06\\\\u5b8c\\\\u6574\\\\u7684\\\\u6d88\\\\u606f\\\\u5386\\\\u53f2\\\\u53d1\\\\u9001\\\\u7ed9\\\\u6a21\\\\u578b\\\\uff0c\\\\u8ba9\\\\u5b83\\\\u6839\\\\u636e\\\\u5de5\\\\u5177\\\\u7ed3\\\\u679c\\\\u7ee7\\\\u7eed\\\\n                    logger.info(f\\\\\"\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\u56de\\\\u5408\\\\u5b8c\\\\u6210\\\\uff0c\\\\u7ee7\\\\u7eed\\\\u4e0e\\\\u6a21\\\\u578b\\\\u4ea4\\\\u4e92...\\\\\")\\\\n                    continue # \\\\u7ee7\\\\u7eed\\\\u5faa\\\\u73af\\\\uff0c\\\\u53d1\\\\u9001\\\\u5305\\\\u542b\\\\u5de5\\\\u5177\\\\u7ed3\\\\u679c\\\\u7684\\\\u6d88\\\\u606f\\\\u5386\\\\u53f2\\\\u7ed9\\\\u6a21\\\\u578b\\\\n                else:\\\\n                    # \\\\u53d1\\\\u751f\\\\u672a\\\\u77e5\\\\u60c5\\\\u51b5\\\\u6216\\\\u9519\\\\u8bef\\\\n                    logger.error(f\\\\\"\\\\u6a21\\\\u578b\\\\u8fd4\\\\u56de\\\\u610f\\\\u5916\\\\u7684 finish_reason: {finish_reason}\\\\\")\\\\n                    return f\\\\\"\\\\u5904\\\\u7406\\\\u4f60\\\\u7684\\\\u67e5\\\\u8be2\\\\u65f6\\\\u9047\\\\u5230\\\\u610f\\\\u5916\\\\u60c5\\\\u51b5 ({finish_reason})\\\\\"\\\\n\\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u5904\\\\u7406\\\\u67e5\\\\u8be2\\\\u65f6\\\\u51fa\\\\u9519: {str(e)}\\\\\")\\\\n                return f\\\\\"\\\\u5904\\\\u7406\\\\u4f60\\\\u7684\\\\u67e5\\\\u8be2\\\\u65f6\\\\u53d1\\\\u751f\\\\u9519\\\\u8bef: {str(e)}\\\\\"\\\\n\\\\n        # \\\\u5982\\\\u679c\\\\u5faa\\\\u73af\\\\u8fbe\\\\u5230\\\\u6700\\\\u5927\\\\u6b21\\\\u6570\\\\u4ecd\\\\u672a\\\\u5f97\\\\u5230\\\\u6700\\\\u7ec8\\\\u56de\\\\u590d\\\\n        logger.warning(\\\\\"\\\\u8fbe\\\\u5230\\\\u6700\\\\u5927\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\u56de\\\\u5408\\\\u6570\\\\uff0c\\\\u672a\\\\u80fd\\\\u83b7\\\\u5f97\\\\u6700\\\\u7ec8\\\\u56de\\\\u590d\\\\\")\\\\n        return \\\\\"\\\\u62b1\\\\u6b49\\\\uff0c\\\\u5904\\\\u7406\\\\u4f60\\\\u7684\\\\u8bf7\\\\u6c42\\\\u65f6\\\\u51fa\\\\u73b0\\\\u95ee\\\\u9898\\\\uff0c\\\\u672a\\\\u80fd\\\\u83b7\\\\u5f97\\\\u6700\\\\u7ec8\\\\u7ed3\\\\u679c\\\\u3002\\\\\"\\\\n\\\\n    async def chat_loop(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u8fd0\\\\u884c\\\\u4ea4\\\\u4e92\\\\u5f0f\\\\u804a\\\\u5929\\\\u5faa\\\\u73af\\\\\"\\\\\"\\\\\"\\\\n        logger.info(\\\\\"\\\\u5f00\\\\u59cb\\\\u804a\\\\u5929\\\\u4f1a\\\\u8bdd...\\\\\")\\\\n        \\\\n        while True:\\\\n            try:\\\\n                query = input(\\\\\"\\\\\\\\n\\\\u8bf7\\\\u8f93\\\\u5165\\\\u4f60\\\\u7684\\\\u95ee\\\\u9898 (\\\\u8f93\\\\u5165 \\'quit\\' \\\\u6216 \\'exit\\' \\\\u9000\\\\u51fa): \\\\\").strip()\\\\n                \\\\n                if query.lower() in [\\'quit\\', \\'exit\\']:\\\\n                    logger.info(\\\\\"\\\\u7ed3\\\\u675f\\\\u804a\\\\u5929\\\\u4f1a\\\\u8bdd...\\\\\")\\\\n                    break\\\\n                    \\\\n                response = await self.process_query(query)\\\\n                print(f\\\\\"\\\\\\\\n\\\\u52a9\\\\u624b: {response}\\\\\")\\\\n                \\\\n            except KeyboardInterrupt:\\\\n                logger.info(\\\\\"\\\\\\\\n\\\\u6536\\\\u5230\\\\u952e\\\\u76d8\\\\u4e2d\\\\u65ad\\\\uff0c\\\\u7ed3\\\\u675f\\\\u4f1a\\\\u8bdd...\\\\\")\\\\n                break\\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u804a\\\\u5929\\\\u5faa\\\\u73af\\\\u4e2d\\\\u51fa\\\\u9519: {str(e)}\\\\\")\\\\n                print(f\\\\\"\\\\\\\\n\\\\u53d1\\\\u751f\\\\u9519\\\\u8bef: {str(e)}\\\\\")\\\\n                \\\\n    async def cleanup(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u6e05\\\\u7406\\\\u5ba2\\\\u6237\\\\u7aef\\\\u8d44\\\\u6e90\\\\\"\\\\\"\\\\\"\\\\n        if self.server:\\\\n            await self.server.cleanup()\\\\n\\\\nasync def main() -> None:\\\\n    \\\\\"\\\\\"\\\\\"\\\\u4e3b\\\\u7a0b\\\\u5e8f\\\\u5165\\\\u53e3\\\\\"\\\\\"\\\\\"\\\\n    try:\\\\n        config = Configuration()\\\\n        client = MCPClient(config)\\\\n        \\\\n        await client.initialize()\\\\n        await client.chat_loop()\\\\n        \\\\n    except Exception as e:\\\\n        logger.error(f\\\\\"\\\\u81f4\\\\u547d\\\\u9519\\\\u8bef: {str(e)}\\\\\")\\\\n        raise\\\\n        \\\\n    finally:\\\\n        if \\'client\\' in locals():\\\\n            await client.cleanup()\\\\n\\\\nif __name__ == \\\\\"__main__\\\\\":\\\\n    try:\\\\n        asyncio.run(main())\\\\n    except KeyboardInterrupt:\\\\n        logger.info(\\\\\"\\\\u7a0b\\\\u5e8f\\\\u88ab\\\\u7528\\\\u6237\\\\u7ec8\\\\u6b62\\\\\")\\\\n    except Exception as e:\\\\n        logger.error(f\\\\\"\\\\u7a0b\\\\u5e8f\\\\u56e0\\\\u9519\\\\u8bef\\\\u7ec8\\\\u6b62: {str(e)}\\\\\")\\\\n        raise\\\\n\", \"timestamp\": \"2025-11-09T00:16:16+0800\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"read_project_file\", \"args\": {\"name\": \"mcp-demo/client/mcp_client_langchain_chat.py\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"\\\\\"\\\\\"\\\\\"\\\\n# \\\\u96c6\\\\u6210 DeepSeek \\\\u7684 MCP \\\\u5ba2\\\\u6237\\\\u7aef[LangChain\\\\u7248]\\\\n#\\\\u66f4\\\\u8be6\\\\u7ec6\\\\u7684\\\\u4f7f\\\\u7528\\\\u65b9\\\\u6cd5\\\\u8bf7\\\\u53c2\\\\u8003\\\\uff1ahttps://github.com/langchain-ai/langchain-mcp-adapters\\\\n\\\\n# \\\\u6240\\\\u9700\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\uff1a\\\\n# DEEPSEEK_API_KEY\\\\uff1aDeepSeek API \\\\u5bc6\\\\u94a5 (\\\\u683c\\\\u5f0f\\\\uff1ask-xxxx...)\\\\n# DEEPSEEK_BASE_URL\\\\uff1aDeepSeek API \\\\u57fa\\\\u7840 URL (https://api.deepseek.com)\\\\n# DEEPSEEK_MODEL\\\\uff1aDeepSeek \\\\u6a21\\\\u578b\\\\u540d\\\\u79f0 (\\\\u4f8b\\\\u5982 deepseek-chat)\\\\n\\\\nAuthor: FlyAIBox\\\\nDate: 2025.10.11\\\\n\\\\\"\\\\\"\\\\\"\\\\nimport asyncio\\\\nimport logging\\\\nimport os\\\\nfrom contextlib import AsyncExitStack\\\\nfrom typing import List, Optional\\\\n\\\\nfrom dotenv import load_dotenv\\\\nfrom langchain_core.messages import SystemMessage\\\\nfrom langchain_core.tools import BaseTool\\\\nfrom langchain_mcp_adapters.tools import load_mcp_tools\\\\nfrom langchain_openai import ChatOpenAI\\\\nfrom langgraph.prebuilt import create_react_agent\\\\nfrom mcp import ClientSession, StdioServerParameters\\\\nfrom mcp.client.stdio import stdio_client\\\\n\\\\n# \\\\u914d\\\\u7f6e\\\\u65e5\\\\u5fd7\\\\nlogging.basicConfig(\\\\n    level=logging.INFO,\\\\n    format=\\\\\"%(asctime)s - %(levelname)s - %(message)s\\\\\"\\\\n)\\\\nlogger = logging.getLogger(__name__)\\\\n\\\\n\\\\n# \\\\u4f18\\\\u5148\\\\u4ece\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\u52a0\\\\u8f7dAPI\\\\u5bc6\\\\u94a5\\\\u7b49\\\\u4fe1\\\\u606f\\\\nload_dotenv()\\\\n\\\\nclass Configuration:\\\\n    \\\\\"\\\\\"\\\\\"\\\\u914d\\\\u7f6e\\\\u7ba1\\\\u7406\\\\u7c7b\\\\uff0c\\\\u8d1f\\\\u8d23\\\\u7ba1\\\\u7406\\\\u548c\\\\u9a8c\\\\u8bc1\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u521d\\\\u59cb\\\\u5316\\\\u914d\\\\u7f6e\\\\u5e76\\\\u52a0\\\\u8f7d\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\"\\\\\"\\\\\"\\\\n        self.load_env()\\\\n        self._validate_env()\\\\n        \\\\n    @staticmethod\\\\n    def load_env() -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u4ece.env\\\\u6587\\\\u4ef6\\\\u52a0\\\\u8f7d\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\"\\\\\"\\\\\"\\\\n        load_dotenv()\\\\n        \\\\n    def _validate_env(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u9a8c\\\\u8bc1\\\\u5fc5\\\\u9700\\\\u7684\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\u662f\\\\u5426\\\\u5b58\\\\u5728\\\\\"\\\\\"\\\\\"\\\\n        required_vars = [\\\\\"DEEPSEEK_API_KEY\\\\\"]\\\\n        missing_vars = [var for var in required_vars if not os.getenv(var)]\\\\n        if missing_vars:\\\\n            raise ValueError(f\\\\\"\\\\u7f3a\\\\u5c11\\\\u5fc5\\\\u9700\\\\u7684\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf: {\\', \\'.join(missing_vars)}\\\\\")\\\\n    \\\\n    @property\\\\n    def api_key(self) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6 DeepSeek API \\\\u5bc6\\\\u94a5\\\\\"\\\\\"\\\\\"\\\\n        return os.getenv(\\\\\"DEEPSEEK_API_KEY\\\\\", \\\\\"\\\\\")\\\\n    \\\\n    @property\\\\n    def base_url(self) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6 DeepSeek API \\\\u57fa\\\\u7840 URL\\\\\"\\\\\"\\\\\"\\\\n        return os.getenv(\\\\\"DEEPSEEK_BASE_URL\\\\\", \\\\\"https://api.deepseek.com\\\\\")\\\\n    \\\\n    @property\\\\n    def model(self) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6 DeepSeek \\\\u6a21\\\\u578b\\\\u540d\\\\u79f0\\\\\"\\\\\"\\\\\"\\\\n        return os.getenv(\\\\\"DEEPSEEK_MODEL\\\\\", \\\\\"deepseek-chat\\\\\")\\\\n\\\\nclass MCPServer:\\\\n    \\\\\"\\\\\"\\\\\"MCP \\\\u670d\\\\u52a1\\\\u5668\\\\u7ba1\\\\u7406\\\\u7c7b\\\\uff0c\\\\u5904\\\\u7406\\\\u670d\\\\u52a1\\\\u5668\\\\u8fde\\\\u63a5\\\\u548c\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\\"\\\\\"\\\\\"\\\\n    def __init__(self, server_path: str) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u521d\\\\u59cb\\\\u5316\\\\u670d\\\\u52a1\\\\u5668\\\\u7ba1\\\\u7406\\\\u5668\\\\n        \\\\n        Args:\\\\n            server_path: \\\\u670d\\\\u52a1\\\\u5668\\\\u811a\\\\u672c\\\\u8def\\\\u5f84\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.server_path = server_path\\\\n        self.session: Optional[ClientSession] = None\\\\n        self.exit_stack = AsyncExitStack()\\\\n        self._cleanup_lock = asyncio.Lock()\\\\n\\\\n\\\\n    async def initialize(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u521d\\\\u59cb\\\\u5316\\\\u670d\\\\u52a1\\\\u5668\\\\u8fde\\\\u63a5\\\\uff0c\\\\u5305\\\\u542b\\\\u91cd\\\\u8bd5\\\\u673a\\\\u5236\\\\\"\\\\\"\\\\\"\\\\n        max_retries = 3\\\\n        retry_delay = 1.0\\\\n        \\\\n        for attempt in range(max_retries):\\\\n            try:\\\\n                if not os.path.exists(self.server_path):\\\\n                    raise FileNotFoundError(f\\\\\"\\\\u627e\\\\u4e0d\\\\u5230\\\\u670d\\\\u52a1\\\\u5668\\\\u6587\\\\u4ef6: {self.server_path}\\\\\")\\\\n                \\\\n                server_params = StdioServerParameters(\\\\n                    command=\\'python\\',\\\\n                    args=[self.server_path],\\\\n                    env=None\\\\n                )\\\\n                \\\\n                stdio_transport = await self.exit_stack.enter_async_context(\\\\n                    stdio_client(server_params)\\\\n                )\\\\n                stdio, write = stdio_transport\\\\n                \\\\n                self.session = await self.exit_stack.enter_async_context(\\\\n                    ClientSession(stdio, write)\\\\n                )\\\\n                await self.session.initialize()\\\\n                logger.info(\\\\\"\\\\u6210\\\\u529f\\\\u8fde\\\\u63a5\\\\u5230 MCP \\\\u670d\\\\u52a1\\\\u5668\\\\\")\\\\n                break\\\\n                \\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u7b2c {attempt + 1}/{max_retries} \\\\u6b21\\\\u5c1d\\\\u8bd5\\\\u5931\\\\u8d25: {str(e)}\\\\\")\\\\n                if attempt < max_retries - 1:\\\\n                    await asyncio.sleep(retry_delay)\\\\n                else:\\\\n                    raise\\\\n\\\\n    async def list_tools(self) -> List[BaseTool]:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6\\\\u670d\\\\u52a1\\\\u5668\\\\u63d0\\\\u4f9b\\\\u7684\\\\u53ef\\\\u7528\\\\u5de5\\\\u5177\\\\u5217\\\\u8868\\\\\"\\\\\"\\\\\"\\\\n        if not self.session:\\\\n            raise RuntimeError(\\\\\"\\\\u670d\\\\u52a1\\\\u5668\\\\u672a\\\\u521d\\\\u59cb\\\\u5316\\\\\")\\\\n        # LangChain\\\\u65b9\\\\u5f0f\\\\u83b7\\\\u53d6\\\\u53ef\\\\u7528\\\\u5de5\\\\u5177\\\\u5217\\\\u8868\\\\n\\\\n        # \\\\u81ea\\\\u52a8\\\\u5316\\\\u7684\\\\u5de5\\\\u5177\\\\u5904\\\\u7406\\\\uff1a\\\\n        # LangChain\\\\u7248\\\\u672c\\\\u4f7f\\\\u7528load_mcp_tools\\\\u51fd\\\\u6570\\\\n        # \\\\u81ea\\\\u52a8\\\\u9002\\\\u914dMCP\\\\u5de5\\\\u5177\\\\uff0c\\\\u7701\\\\u53bb\\\\u4e86\\\\u5de5\\\\u5177\\\\u683c\\\\u5f0f\\\\u8f6c\\\\u6362\\\\u7684\\\\u5de5\\\\u4f5c\\\\n        tools = await load_mcp_tools(self.session)\\\\n        logger.info(f\\\\\"\\\\u6210\\\\u529f\\\\u52a0\\\\u8f7d\\\\u5de5\\\\u5177: {[tool.name for tool in tools]}\\\\\")\\\\n        return tools\\\\n\\\\n\\\\n    async def cleanup(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u6e05\\\\u7406\\\\u670d\\\\u52a1\\\\u5668\\\\u8d44\\\\u6e90\\\\\"\\\\\"\\\\\"\\\\n        async with self._cleanup_lock:\\\\n            try:\\\\n                await self.exit_stack.aclose()\\\\n                self.session = None\\\\n                logger.info(\\\\\"\\\\u670d\\\\u52a1\\\\u5668\\\\u8d44\\\\u6e90\\\\u6e05\\\\u7406\\\\u5b8c\\\\u6210\\\\\")\\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u6e05\\\\u7406\\\\u8fc7\\\\u7a0b\\\\u4e2d\\\\u51fa\\\\u9519: {str(e)}\\\\\")\\\\n    \\\\n\\\\nclass MCPClient:\\\\n    \\\\\"\\\\\"\\\\\"MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\u5b9e\\\\u73b0\\\\uff0c\\\\u96c6\\\\u6210\\\\u4e86 DeepSeek API\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self, config: Configuration) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u521d\\\\u59cb\\\\u5316 MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\n        \\\\n        Args:\\\\n            config: \\\\u914d\\\\u7f6e\\\\u5bf9\\\\u8c61\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.config = config\\\\n        self.server: Optional[MCPServer] = None\\\\n        self.llm_client = ChatOpenAI(\\\\n            api_key=config.api_key,\\\\n            base_url=config.base_url,\\\\n            model=config.model\\\\n        )\\\\n        \\\\n    async def initialize(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u521d\\\\u59cb\\\\u5316\\\\u5ba2\\\\u6237\\\\u7aef\\\\u5e76\\\\u8fde\\\\u63a5\\\\u5230\\\\u670d\\\\u52a1\\\\u5668\\\\\"\\\\\"\\\\\"\\\\n        server_path = os.path.join(\\\\n            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\\\\n            \\\\\"server\\\\\",\\\\n            \\\\\"weather_server.py\\\\\"\\\\n        )\\\\n        self.server = MCPServer(server_path)\\\\n        await self.server.initialize()\\\\n        \\\\n\\\\n    async def process_query(self, query: str):\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u5904\\\\u7406\\\\u7528\\\\u6237\\\\u67e5\\\\u8be2\\\\uff0c\\\\u96c6\\\\u6210\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\uff0c\\\\u652f\\\\u6301\\\\u591a\\\\u8f6e\\\\u5de5\\\\u5177\\\\u4ea4\\\\u4e92\\\\n\\\\n        Args:\\\\n            query: \\\\u7528\\\\u6237\\\\u67e5\\\\u8be2\\\\u5b57\\\\u7b26\\\\u4e32\\\\n\\\\n        Returns:\\\\n            \\\\u5904\\\\u7406\\\\u540e\\\\u7684\\\\u54cd\\\\u5e94\\\\u7ed3\\\\u679c\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        if not self.server:\\\\n            raise RuntimeError(\\\\\"\\\\u5ba2\\\\u6237\\\\u7aef\\\\u672a\\\\u521d\\\\u59cb\\\\u5316\\\\\")\\\\n\\\\n        # \\\\u521b\\\\u5efa\\\\u63d0\\\\u793a\\\\u6a21\\\\u677f\\\\n        prompt = SystemMessage(content=\\\\\"\\\\\"\\\\\"\\\\\"You are a helpful assistant specializing in weather information.\\\\\\\\n\\\\n                            You have access to the MCP Weather Server tool with the following functions:\\\\\\\\n\\\\n                            - get_weather_warning(city_id=None, latitude=None, longitude=None): Retrieves weather disaster warnings for a specified city ID or coordinates.\\\\\\\\n\\\\n                            - get_daily_forecast(city_id=None, latitude=None, longitude=None): Retrieves the multi-day weather forecast for a specified city ID or coordinates.\\\\\\\\n\\\\n                            \\\\\\\\n\\\\n                            Core Instructions:\\\\\\\\n\\\\n                            1.  **Carefully analyze the user\\'s request**: Understand all components of the user\\'s query. Determine if the user needs weather warning information, weather forecast information, or both.\\\\\\\\n\\\\n                            2.  **Identify Information Needs**:\\\\\\\\n\\\\n                                * If the user only asks for warnings (e.g., \\\\\\\\Are there any warnings in Beijing?\\\\\\\\), only use `get_weather_warning`.\\\\\\\\n\\\\n                                * If the user only asks for the forecast (e.g., \\\\\\\\What\\'s the weather like in Beijing tomorrow?\\\\\\\\), only use `get_daily_forecast`.\\\\\\\\n\\\\n                                * **If the user\\'s question includes multiple aspects**, such as asking about **warning status** and also asking **if it\\'s suitable for a certain activity** (which implies a query about future weather, like \\\\\\\\Have there been high temperature warnings in Beijing in the last week? Is it suitable for outdoor activities?\\\\\\\\), you need to **call both tools sequentially**.\\\\\\\\n\\\\n                            3.  **Call Tools as Needed**:\\\\\\\\n\\\\n                                * **Prioritize getting warning information**: If warning information is needed, first call `get_weather_warning`.\\\\\\\\n\\\\n                                * **Get the weather forecast**: If the user mentions a specific time period (e.g., \\\\\\\\weekend\\\\\\\\, \\\\\\\\next three days\\\\\\\\, \\\\\\\\next week\\\\\\\\) or asks about activity suitability (which typically concerns the next few days), call `get_daily_forecast` to get the forecast for the corresponding period. For vague phrases like \\\\\\\\last week\\\\\\\\ or \\\\\\\\recently\\\\\\\\, interpret it as asking about *current* conditions and the *upcoming* few days (covered by the forecast). For questions like \\\\\\\\Is it suitable for outdoor activities?\\\\\\\\, you should get the forecast for at least the next 2-3 days (e.g., today, tomorrow, the day after tomorrow, or the upcoming weekend) to support your judgment.\\\\\\\\n\\\\n                                * **Ensure tool call order**: When multiple tools need to be called, they should be called in a logical sequence. For example, first get the warning, then get the forecast. Wait for one tool to finish executing before deciding whether to call the next tool or generate a response.\\\\\\\\n\\\\n                            4.  **Information Integration and Response**:\\\\\\\\n\\\\n                                * After obtaining all necessary information (warning, forecast), you **must synthesize and analyze this information**.\\\\\\\\n\\\\n                                * **Completely answer the user\\'s question**: Ensure you answer all parts of the user\\'s query.\\\\\\\\n\\\\n                                * **Provide advice**: If the user asks about activity suitability, based on the retrieved warning status and forecast information (temperature, weather condition - clear/rainy, wind strength, etc.), provide a clear, data-supported recommendation (e.g., \\\\\\\\Currently there are no high temperature warnings, but it\\'s expected to rain this weekend, so it\\'s not very suitable for outdoor activities,\\\\\\\\ or \\\\\\\\It will be sunny for the next few days with no warnings, suitable for outdoor activities.\\\\\\\\).\\\\\\\\n\\\\n                            5.  **Tool Usage Details**:\\\\\\\\n\\\\n                                * When using the tools, retain the full context of the user\\'s original question.\\\\\\\\n\\\\n                                * Unless explicitly requested by the user, do not insert specific times of day (e.g., \\\\\\\\3 PM\\\\\\\\) into the search query or your response.\\\\\\\\n\\\\n                                * When city information is needed, if the user provides a city name (e.g., \\\\\\\\Beijing\\\\\\\\), use the corresponding `city_id` (e.g., Beijing\\'s city_id might be \\'101010100\\').\\\\\\\\n                      \\\\n                            \\\\\"\\\\\"\\\\\")\\\\n        \\\\n        \\\\n        ## \\\\u5217\\\\u4e3e\\\\u5de5\\\\u5177\\\\n        tools = await self.server.list_tools()\\\\n\\\\n        ## \\\\u66f4\\\\u7b80\\\\u6d01\\\\u7684\\\\u4ee3\\\\u7406\\\\u521b\\\\u5efa\\\\u6d41\\\\u7a0b\\\\uff1a\\\\n        # LangChain\\\\u7248\\\\u672c\\\\u4f7f\\\\u7528create_react_agent\\\\u51fd\\\\u6570\\\\n        # \\\\u76f4\\\\u63a5\\\\u521b\\\\u5efa\\\\u4ee3\\\\u7406\\\\uff0c\\\\u5e76\\\\u6307\\\\u5b9a\\\\u6a21\\\\u578b\\\\u3001\\\\u5de5\\\\u5177\\\\u548c\\\\u63d0\\\\u793a\\\\n        # \\\\u7b80\\\\u5316\\\\u4e86\\\\u4ee3\\\\u7801\\\\u590d\\\\u6742\\\\u5ea6\\\\n        \\\\n        # LangChain\\\\u65b9\\\\u5f0f\\\\u521b\\\\u5efa\\\\u548c\\\\u8fd0\\\\u884cagent\\\\n        logger.info(\\\\\"\\\\u6b63\\\\u5728\\\\u521b\\\\u5efaagent...\\\\\")\\\\n        agent = create_react_agent(\\\\n            model=self.llm_client,  \\\\n            tools=tools,\\\\n            prompt=prompt\\\\n        )\\\\n        logger.info(\\\\\"Agent\\\\u521b\\\\u5efa\\\\u6210\\\\u529f\\\\\")\\\\n\\\\n        # \\\\u53d1\\\\u9001\\\\u67e5\\\\u8be2\\\\n        logger.info(\\\\\"\\\\u6b63\\\\u5728\\\\u53d1\\\\u9001\\\\u5929\\\\u6c14\\\\u67e5\\\\u8be2...\\\\\")\\\\n        agent_response = await agent.ainvoke({\\\\n            \\\\\"messages\\\\\": query\\\\n        })\\\\n\\\\n        # \\\\u6253\\\\u5370\\\\u54cd\\\\u5e94\\\\n        logger.info(\\\\\"\\\\\\\\nAgent Response:\\\\\")\\\\n        print(agent_response)\\\\n\\\\n        # \\\\u904d\\\\u5386\\\\u6d88\\\\u606f\\\\u5217\\\\u8868\\\\u5e76\\\\u6253\\\\u5370 ToolMessage \\\\u7684 content\\\\n        if \\'messages\\' in agent_response:\\\\n            print(\\\\\"\\\\\\\\n--- Tool Message Contents ---\\\\\")\\\\n            for message in agent_response[\\'messages\\']:\\\\n                print(f\\\\\"\\\\\\\\nTool: {message.name}\\\\\") # \\\\u53ef\\\\u4ee5\\\\u9009\\\\u62e9\\\\u6253\\\\u5370\\\\u5de5\\\\u5177\\\\u540d\\\\u79f0\\\\n                print(f\\\\\"Content:\\\\\\\\n{message.content}\\\\\")\\\\n                print(\\\\\"-\\\\\" * 20) # \\\\u5206\\\\u9694\\\\u4e0d\\\\u540c ToolMessage \\\\u7684\\\\u5185\\\\u5bb9\\\\n             \\\\n\\\\n    async def chat_loop(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u8fd0\\\\u884c\\\\u4ea4\\\\u4e92\\\\u5f0f\\\\u804a\\\\u5929\\\\u5faa\\\\u73af\\\\\"\\\\\"\\\\\"\\\\n        logger.info(\\\\\"\\\\u5f00\\\\u59cb\\\\u804a\\\\u5929\\\\u4f1a\\\\u8bdd...\\\\\")\\\\n        \\\\n        while True:\\\\n            try:\\\\n                query = input(\\\\\"\\\\\\\\n\\\\u8bf7\\\\u8f93\\\\u5165\\\\u4f60\\\\u7684\\\\u95ee\\\\u9898 (\\\\u8f93\\\\u5165 \\'quit\\' \\\\u6216 \\'exit\\' \\\\u9000\\\\u51fa): \\\\\").strip()\\\\n                \\\\n                if query.lower() in [\\'quit\\', \\'exit\\']:\\\\n                    logger.info(\\\\\"\\\\u7ed3\\\\u675f\\\\u804a\\\\u5929\\\\u4f1a\\\\u8bdd...\\\\\")\\\\n                    break\\\\n                    \\\\n                await self.process_query(query)\\\\n                \\\\n                \\\\n            except KeyboardInterrupt:\\\\n                logger.info(\\\\\"\\\\\\\\n\\\\u6536\\\\u5230\\\\u952e\\\\u76d8\\\\u4e2d\\\\u65ad\\\\uff0c\\\\u7ed3\\\\u675f\\\\u4f1a\\\\u8bdd...\\\\\")\\\\n                break\\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u804a\\\\u5929\\\\u5faa\\\\u73af\\\\u4e2d\\\\u51fa\\\\u9519: {str(e)}\\\\\")\\\\n                print(f\\\\\"\\\\\\\\n\\\\u53d1\\\\u751f\\\\u9519\\\\u8bef: {str(e)}\\\\\")\\\\n                \\\\n    async def cleanup(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u6e05\\\\u7406\\\\u5ba2\\\\u6237\\\\u7aef\\\\u8d44\\\\u6e90\\\\\"\\\\\"\\\\\"\\\\n        if self.server:\\\\n            await self.server.cleanup()\\\\n\\\\nasync def main() -> None:\\\\n    \\\\\"\\\\\"\\\\\"\\\\u4e3b\\\\u7a0b\\\\u5e8f\\\\u5165\\\\u53e3\\\\\"\\\\\"\\\\\"\\\\n    try:\\\\n        config = Configuration()\\\\n        client = MCPClient(config)\\\\n        \\\\n        await client.initialize()\\\\n        await client.chat_loop()\\\\n        \\\\n    except Exception as e:\\\\n        logger.error(f\\\\\"\\\\u81f4\\\\u547d\\\\u9519\\\\u8bef: {str(e)}\\\\\")\\\\n        raise\\\\n        \\\\n    finally:\\\\n        if \\'client\\' in locals():\\\\n            await client.cleanup()\\\\n\\\\nif __name__ == \\\\\"__main__\\\\\":\\\\n    try:\\\\n        asyncio.run(main())\\\\n    except KeyboardInterrupt:\\\\n        logger.info(\\\\\"\\\\u7a0b\\\\u5e8f\\\\u88ab\\\\u7528\\\\u6237\\\\u7ec8\\\\u6b62\\\\\")\\\\n    except Exception as e:\\\\n        logger.error(f\\\\\"\\\\u7a0b\\\\u5e8f\\\\u56e0\\\\u9519\\\\u8bef\\\\u7ec8\\\\u6b62: {str(e)}\\\\\")\\\\n        raise\\\\n\\\\n\", \"timestamp\": \"2025-11-09T00:16:18+0800\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"read_project_file\", \"args\": {\"name\": \"mcp-demo/server/weather_server.py\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"#!/usr/bin/env python3\\\\n# -*- coding: utf-8 -*-\\\\n\\\\n\\\\\"\\\\\"\\\\\"\\\\nMCP \\\\u670d\\\\u52a1\\\\u5668\\\\u53ef\\\\u4ee5\\\\u63d0\\\\u4f9b\\\\u4e09\\\\u79cd\\\\u4e3b\\\\u8981\\\\u7c7b\\\\u578b\\\\u7684\\\\u529f\\\\u80fd\\\\uff1a\\\\n\\\\n\\\\u8d44\\\\u6e90\\\\uff1a\\\\u5ba2\\\\u6237\\\\u7aef\\\\u53ef\\\\u4ee5\\\\u8bfb\\\\u53d6\\\\u7684\\\\u7c7b\\\\u4f3c\\\\u6587\\\\u4ef6\\\\u7684\\\\u6570\\\\u636e\\\\uff08\\\\u4f8b\\\\u5982 API \\\\u54cd\\\\u5e94\\\\u6216\\\\u6587\\\\u4ef6\\\\u5185\\\\u5bb9\\\\uff09\\\\n\\\\u5de5\\\\u5177\\\\uff1a\\\\u53ef\\\\u7531 LLM \\\\u8c03\\\\u7528\\\\u7684\\\\u51fd\\\\u6570\\\\uff08\\\\u7ecf\\\\u7528\\\\u6237\\\\u6279\\\\u51c6\\\\uff09\\\\n\\\\u63d0\\\\u793a\\\\uff1a\\\\u9884\\\\u5148\\\\u7f16\\\\u5199\\\\u7684\\\\u6a21\\\\u677f\\\\uff0c\\\\u5e2e\\\\u52a9\\\\u7528\\\\u6237\\\\u5b8c\\\\u6210\\\\u7279\\\\u5b9a\\\\u4efb\\\\u52a1\\\\n\\\\n######################################\\\\n\\\\nMCP \\\\u5929\\\\u6c14\\\\u670d\\\\u52a1\\\\u5668\\\\n\\\\n\\\\u63d0\\\\u4f9b\\\\u4e24\\\\u4e2a\\\\u5de5\\\\u5177\\\\uff1a\\\\n1. get_weather_warning: \\\\u83b7\\\\u53d6\\\\u6307\\\\u5b9a\\\\u57ce\\\\u5e02ID\\\\u6216\\\\u7ecf\\\\u7eac\\\\u5ea6\\\\u7684\\\\u5929\\\\u6c14\\\\u707e\\\\u5bb3\\\\u9884\\\\u8b66\\\\n2. get_daily_forecast: \\\\u83b7\\\\u53d6\\\\u6307\\\\u5b9a\\\\u57ce\\\\u5e02ID\\\\u6216\\\\u7ecf\\\\u7eac\\\\u5ea6\\\\u7684\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\n\\\\nAuthor: FlyAIBox\\\\nDate: 2025.10.11\\\\n\\\\\"\\\\\"\\\\\"\\\\n\\\\nfrom typing import Any, Dict, List, Optional, Union\\\\nimport asyncio\\\\nimport httpx\\\\nimport os\\\\nfrom urllib.parse import urljoin\\\\nfrom mcp.server.fastmcp import FastMCP\\\\nfrom dotenv import load_dotenv\\\\nfrom pathlib import Path\\\\n\\\\n# \\\\u52a0\\\\u8f7d .env \\\\u6587\\\\u4ef6\\\\u4e2d\\\\u7684\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\ndotenv_path = Path(__file__).resolve().parents[1] / \\'.env\\'\\\\nload_dotenv(dotenv_path)\\\\n\\\\n# \\\\u521d\\\\u59cb\\\\u5316 FastMCP \\\\u670d\\\\u52a1\\\\u5668\\\\nmcp = FastMCP(\\\\\"weather\\\\\",  # \\\\u670d\\\\u52a1\\\\u5668\\\\u540d\\\\u79f0\\\\n              debug=True,  # \\\\u542f\\\\u7528\\\\u8c03\\\\u8bd5\\\\u6a21\\\\u5f0f\\\\uff0c\\\\u4f1a\\\\u8f93\\\\u51fa\\\\u8be6\\\\u7ec6\\\\u65e5\\\\u5fd7\\\\n              host=\\\\\"0.0.0.0\\\\\") # \\\\u76d1\\\\u542c\\\\u6240\\\\u6709\\\\u7f51\\\\u7edc\\\\u63a5\\\\u53e3\\\\uff0c\\\\u5141\\\\u8bb8\\\\u8fdc\\\\u7a0b\\\\u8fde\\\\u63a5\\\\n\\\\n# \\\\u4ece\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\u4e2d\\\\u8bfb\\\\u53d6\\\\u5e38\\\\u91cf\\\\nQWEATHER_API_BASE = os.getenv(\\\\\"QWEATHER_API_BASE\\\\\")\\\\nQWEATHER_API_KEY = os.getenv(\\\\\"QWEATHER_API_KEY\\\\\")\\\\n\\\\ndef _normalize_base_url(raw_base: Optional[str]) -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u786e\\\\u4fdd\\\\u57fa\\\\u7840 URL \\\\u5305\\\\u542b\\\\u534f\\\\u8bae\\\\u5e76\\\\u4ee5\\\\u5355\\\\u4e2a\\\\u659c\\\\u6760\\\\u7ed3\\\\u5c3e\\\\uff0c\\\\u517c\\\\u5bb9 .env \\\\u4e2d\\\\u672a\\\\u5199\\\\u534f\\\\u8bae\\\\u7684\\\\u60c5\\\\u51b5\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    if not raw_base:\\\\n        raise RuntimeError(\\\\\"\\\\u672a\\\\u914d\\\\u7f6e QWEATHER_API_BASE \\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\")\\\\n\\\\n    base = raw_base.strip()\\\\n    if not base.startswith((\\\\\"http://\\\\\", \\\\\"https://\\\\\")):\\\\n        base = f\\\\\"https://{base.lstrip(\\'/\\')}\\\\\"\\\\n\\\\n    # urljoin \\\\u8981\\\\u6c42\\\\u76ee\\\\u5f55\\\\u98ce\\\\u683c\\\\u4ee5\\\\u659c\\\\u6760\\\\u7ed3\\\\u5c3e\\\\uff0c\\\\u907f\\\\u514d \\'v7/weather/7d\\' \\\\u88ab\\\\u8986\\\\u76d6\\\\n    if not base.endswith(\\\\\"/\\\\\"):\\\\n        base = f\\\\\"{base}/\\\\\"\\\\n\\\\n    return base\\\\n\\\\ntry:\\\\n    _QWEATHER_BASE_URL = _normalize_base_url(QWEATHER_API_BASE)\\\\nexcept RuntimeError as err:\\\\n    print(f\\\\\"[\\\\u914d\\\\u7f6e\\\\u9519\\\\u8bef] {err}\\\\\")\\\\n    _QWEATHER_BASE_URL = None\\\\n\\\\nasync def make_qweather_request(endpoint: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u5411\\\\u548c\\\\u98ce\\\\u5929\\\\u6c14 API \\\\u53d1\\\\u9001\\\\u8bf7\\\\u6c42\\\\n    \\\\n    \\\\u53c2\\\\u6570:\\\\n        endpoint: API \\\\u7aef\\\\u70b9\\\\u8def\\\\u5f84\\\\uff08\\\\u4e0d\\\\u5305\\\\u542b\\\\u57fa\\\\u7840 URL\\\\uff09\\\\n        params: API \\\\u8bf7\\\\u6c42\\\\u7684\\\\u53c2\\\\u6570\\\\n        \\\\n    \\\\u8fd4\\\\u56de:\\\\n        \\\\u6210\\\\u529f\\\\u65f6\\\\u8fd4\\\\u56de JSON \\\\u54cd\\\\u5e94\\\\uff0c\\\\u5931\\\\u8d25\\\\u65f6\\\\u8fd4\\\\u56de None\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    if not _QWEATHER_BASE_URL:\\\\n        print(\\\\\"QWEATHER_API_BASE \\\\u672a\\\\u6b63\\\\u786e\\\\u914d\\\\u7f6e\\\\uff0c\\\\u5df2\\\\u8df3\\\\u8fc7\\\\u8bf7\\\\u6c42\\\\u3002\\\\\")\\\\n        return None\\\\n\\\\n    if not QWEATHER_API_KEY:\\\\n        print(\\\\\"QWEATHER_API_KEY \\\\u672a\\\\u8bbe\\\\u7f6e\\\\uff0c\\\\u5df2\\\\u8df3\\\\u8fc7\\\\u8bf7\\\\u6c42\\\\u3002\\\\\")\\\\n        return None\\\\n\\\\n    safe_endpoint = endpoint.lstrip(\\\\\"/\\\\\")\\\\n    url = urljoin(_QWEATHER_BASE_URL, safe_endpoint)\\\\n\\\\n    # \\\\u4f7f\\\\u7528 Header \\\\u65b9\\\\u5f0f\\\\u8ba4\\\\u8bc1\\\\uff08\\\\u548c\\\\u98ce\\\\u5929\\\\u6c14\\\\u7684\\\\u65b0\\\\u7248\\\\u672cAPI\\\\uff09\\\\n    headers = {\\\\n        \\\\\"X-QW-Api-Key\\\\\": QWEATHER_API_KEY\\\\n    }\\\\n    \\\\n    async with httpx.AsyncClient() as client:\\\\n        try:\\\\n            print(f\\\\\"\\\\u8bf7\\\\u6c42 URL: {url}\\\\\")\\\\n            print(f\\\\\"\\\\u8bf7\\\\u6c42\\\\u53c2\\\\u6570: {params}\\\\\")\\\\n            response = await client.get(url, params=params, headers=headers, timeout=30.0)\\\\n            print(f\\\\\"\\\\u54cd\\\\u5e94\\\\u72b6\\\\u6001\\\\u7801: {response.status_code}\\\\\")\\\\n            response.raise_for_status()\\\\n            result = response.json()\\\\n            print(f\\\\\"\\\\u54cd\\\\u5e94\\\\u5185\\\\u5bb9: {result}\\\\\")\\\\n            return result\\\\n        except httpx.HTTPStatusError as e:\\\\n            print(f\\\\\"HTTP \\\\u72b6\\\\u6001\\\\u9519\\\\u8bef: {e.response.status_code} - {e.response.text}\\\\\")\\\\n            return None\\\\n        except Exception as e:\\\\n            print(f\\\\\"API \\\\u8bf7\\\\u6c42\\\\u9519\\\\u8bef: {type(e).__name__}: {e}\\\\\")\\\\n            return None\\\\n\\\\ndef format_warning(warning: Dict[str, Any]) -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u5c06\\\\u5929\\\\u6c14\\\\u9884\\\\u8b66\\\\u6570\\\\u636e\\\\u683c\\\\u5f0f\\\\u5316\\\\u4e3a\\\\u53ef\\\\u8bfb\\\\u5b57\\\\u7b26\\\\u4e32\\\\n    \\\\n    \\\\u53c2\\\\u6570:\\\\n        warning: \\\\u5929\\\\u6c14\\\\u9884\\\\u8b66\\\\u6570\\\\u636e\\\\u5bf9\\\\u8c61\\\\n        \\\\n    \\\\u8fd4\\\\u56de:\\\\n        \\\\u683c\\\\u5f0f\\\\u5316\\\\u540e\\\\u7684\\\\u9884\\\\u8b66\\\\u4fe1\\\\u606f\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    return f\\\\\"\\\\\"\\\\\"\\\\n\\\\u9884\\\\u8b66ID: {warning.get(\\'id\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u6807\\\\u9898: {warning.get(\\'title\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u53d1\\\\u5e03\\\\u65f6\\\\u95f4: {warning.get(\\'pubTime\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u5f00\\\\u59cb\\\\u65f6\\\\u95f4: {warning.get(\\'startTime\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u7ed3\\\\u675f\\\\u65f6\\\\u95f4: {warning.get(\\'endTime\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u9884\\\\u8b66\\\\u7c7b\\\\u578b: {warning.get(\\'typeName\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u9884\\\\u8b66\\\\u7b49\\\\u7ea7: {warning.get(\\'severity\\', \\'\\\\u672a\\\\u77e5\\')} ({warning.get(\\'severityColor\\', \\'\\\\u672a\\\\u77e5\\')})\\\\n\\\\u53d1\\\\u5e03\\\\u5355\\\\u4f4d: {warning.get(\\'sender\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u72b6\\\\u6001: {warning.get(\\'status\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u8be6\\\\u7ec6\\\\u4fe1\\\\u606f: {warning.get(\\'text\\', \\'\\\\u65e0\\\\u8be6\\\\u7ec6\\\\u4fe1\\\\u606f\\')}\\\\n\\\\\"\\\\\"\\\\\"\\\\n\\\\n@mcp.tool()\\\\nasync def get_weather_warning(location: Union[str, int]) -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u83b7\\\\u53d6\\\\u6307\\\\u5b9a\\\\u4f4d\\\\u7f6e\\\\u7684\\\\u5929\\\\u6c14\\\\u707e\\\\u5bb3\\\\u9884\\\\u8b66\\\\n    \\\\n    \\\\u53c2\\\\u6570:\\\\n        location: \\\\u57ce\\\\u5e02ID\\\\u6216\\\\u7ecf\\\\u7eac\\\\u5ea6\\\\u5750\\\\u6807\\\\uff08\\\\u7ecf\\\\u5ea6,\\\\u7eac\\\\u5ea6\\\\uff09\\\\n                \\\\u4f8b\\\\u5982\\\\uff1a\\'101010100\\'\\\\uff08\\\\u5317\\\\u4eac\\\\uff09\\\\u6216 \\'116.41,39.92\\'\\\\n                \\\\u4e5f\\\\u53ef\\\\u4ee5\\\\u76f4\\\\u63a5\\\\u4f20\\\\u5165\\\\u6570\\\\u5b57ID\\\\uff0c\\\\u5982 101010100\\\\n        \\\\n    \\\\u8fd4\\\\u56de:\\\\n        \\\\u683c\\\\u5f0f\\\\u5316\\\\u7684\\\\u9884\\\\u8b66\\\\u4fe1\\\\u606f\\\\u5b57\\\\u7b26\\\\u4e32\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    # \\\\u786e\\\\u4fdd location \\\\u4e3a\\\\u5b57\\\\u7b26\\\\u4e32\\\\u7c7b\\\\u578b\\\\n    location = str(location)\\\\n    \\\\n    params = {\\\\n        \\\\\"location\\\\\": location,\\\\n        \\\\\"lang\\\\\": \\\\\"zh\\\\\"\\\\n    }\\\\n    \\\\n    data = await make_qweather_request(\\\\\"v7/warning/now\\\\\", params)\\\\n    \\\\n    if not data:\\\\n        return \\\\\"\\\\u65e0\\\\u6cd5\\\\u83b7\\\\u53d6\\\\u9884\\\\u8b66\\\\u4fe1\\\\u606f\\\\u6216API\\\\u8bf7\\\\u6c42\\\\u5931\\\\u8d25\\\\u3002\\\\\"\\\\n    \\\\n    if data.get(\\\\\"code\\\\\") != \\\\\"200\\\\\":\\\\n        return f\\\\\"API \\\\u8fd4\\\\u56de\\\\u9519\\\\u8bef: {data.get(\\'code\\')}\\\\\"\\\\n    \\\\n    warnings = data.get(\\\\\"warning\\\\\", [])\\\\n    \\\\n    if not warnings:\\\\n        return f\\\\\"\\\\u5f53\\\\u524d\\\\u4f4d\\\\u7f6e {location} \\\\u6ca1\\\\u6709\\\\u6d3b\\\\u52a8\\\\u9884\\\\u8b66\\\\u3002\\\\\"\\\\n    \\\\n    formatted_warnings = [format_warning(warning) for warning in warnings]\\\\n    return \\\\\"\\\\\\\\n---\\\\\\\\n\\\\\".join(formatted_warnings)\\\\n\\\\ndef format_daily_forecast(daily: Dict[str, Any]) -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u5c06\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\u6570\\\\u636e\\\\u683c\\\\u5f0f\\\\u5316\\\\u4e3a\\\\u53ef\\\\u8bfb\\\\u5b57\\\\u7b26\\\\u4e32\\\\n    \\\\n    \\\\u53c2\\\\u6570:\\\\n        daily: \\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\u6570\\\\u636e\\\\u5bf9\\\\u8c61\\\\n        \\\\n    \\\\u8fd4\\\\u56de:\\\\n        \\\\u683c\\\\u5f0f\\\\u5316\\\\u540e\\\\u7684\\\\u9884\\\\u62a5\\\\u4fe1\\\\u606f\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    return f\\\\\"\\\\\"\\\\\"\\\\n\\\\u65e5\\\\u671f: {daily.get(\\'fxDate\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u65e5\\\\u51fa: {daily.get(\\'sunrise\\', \\'\\\\u672a\\\\u77e5\\')}  \\\\u65e5\\\\u843d: {daily.get(\\'sunset\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u6700\\\\u9ad8\\\\u6e29\\\\u5ea6: {daily.get(\\'tempMax\\', \\'\\\\u672a\\\\u77e5\\')}\\\\u00b0C  \\\\u6700\\\\u4f4e\\\\u6e29\\\\u5ea6: {daily.get(\\'tempMin\\', \\'\\\\u672a\\\\u77e5\\')}\\\\u00b0C\\\\n\\\\u767d\\\\u5929\\\\u5929\\\\u6c14: {daily.get(\\'textDay\\', \\'\\\\u672a\\\\u77e5\\')}  \\\\u591c\\\\u95f4\\\\u5929\\\\u6c14: {daily.get(\\'textNight\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u767d\\\\u5929\\\\u98ce\\\\u5411: {daily.get(\\'windDirDay\\', \\'\\\\u672a\\\\u77e5\\')} {daily.get(\\'windScaleDay\\', \\'\\\\u672a\\\\u77e5\\')}\\\\u7ea7 ({daily.get(\\'windSpeedDay\\', \\'\\\\u672a\\\\u77e5\\')}km/h)\\\\n\\\\u591c\\\\u95f4\\\\u98ce\\\\u5411: {daily.get(\\'windDirNight\\', \\'\\\\u672a\\\\u77e5\\')} {daily.get(\\'windScaleNight\\', \\'\\\\u672a\\\\u77e5\\')}\\\\u7ea7 ({daily.get(\\'windSpeedNight\\', \\'\\\\u672a\\\\u77e5\\')}km/h)\\\\n\\\\u76f8\\\\u5bf9\\\\u6e7f\\\\u5ea6: {daily.get(\\'humidity\\', \\'\\\\u672a\\\\u77e5\\')}%\\\\n\\\\u964d\\\\u6c34\\\\u91cf: {daily.get(\\'precip\\', \\'\\\\u672a\\\\u77e5\\')}mm\\\\n\\\\u7d2b\\\\u5916\\\\u7ebf\\\\u6307\\\\u6570: {daily.get(\\'uvIndex\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u80fd\\\\u89c1\\\\u5ea6: {daily.get(\\'vis\\', \\'\\\\u672a\\\\u77e5\\')}km\\\\n\\\\\"\\\\\"\\\\\"\\\\n\\\\n@mcp.tool()\\\\nasync def get_daily_forecast(location: Union[str, int], days: int = 3) -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u83b7\\\\u53d6\\\\u6307\\\\u5b9a\\\\u4f4d\\\\u7f6e\\\\u7684\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\n    \\\\n    \\\\u53c2\\\\u6570:\\\\n        location: \\\\u57ce\\\\u5e02ID\\\\u6216\\\\u7ecf\\\\u7eac\\\\u5ea6\\\\u5750\\\\u6807\\\\uff08\\\\u7ecf\\\\u5ea6,\\\\u7eac\\\\u5ea6\\\\uff09\\\\n                \\\\u4f8b\\\\u5982\\\\uff1a\\'101010100\\'\\\\uff08\\\\u5317\\\\u4eac\\\\uff09\\\\u6216 \\'116.41,39.92\\'\\\\n                \\\\u4e5f\\\\u53ef\\\\u4ee5\\\\u76f4\\\\u63a5\\\\u4f20\\\\u5165\\\\u6570\\\\u5b57ID\\\\uff0c\\\\u5982 101010100\\\\n        days: \\\\u9884\\\\u62a5\\\\u5929\\\\u6570\\\\uff0c\\\\u53ef\\\\u9009\\\\u503c\\\\u4e3a 3\\\\u30017\\\\u300110\\\\u300115\\\\u300130\\\\uff0c\\\\u9ed8\\\\u8ba4\\\\u4e3a 3\\\\n        \\\\n    \\\\u8fd4\\\\u56de:\\\\n        \\\\u683c\\\\u5f0f\\\\u5316\\\\u7684\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\u5b57\\\\u7b26\\\\u4e32\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    # \\\\u786e\\\\u4fdd location \\\\u4e3a\\\\u5b57\\\\u7b26\\\\u4e32\\\\u7c7b\\\\u578b\\\\n    location = str(location)\\\\n    \\\\n    # \\\\u786e\\\\u4fdd days \\\\u53c2\\\\u6570\\\\u6709\\\\u6548\\\\n    valid_days = [3, 7, 10, 15, 30]\\\\n    if days not in valid_days:\\\\n        days = 3  # \\\\u9ed8\\\\u8ba4\\\\u4f7f\\\\u75283\\\\u5929\\\\u9884\\\\u62a5\\\\n    \\\\n    params = {\\\\n        \\\\\"location\\\\\": location,\\\\n        \\\\\"lang\\\\\": \\\\\"zh\\\\\"\\\\n    }\\\\n    # \\\\u548c\\\\u98ce\\\\u5929\\\\u6c14API\\\\u6587\\\\u6863 https://dev.qweather.com/docs/api/weather/weather-daily-forecast/\\\\n    endpoint = f\\\\\"v7/weather/{days}d\\\\\"\\\\n    data = await make_qweather_request(endpoint, params)\\\\n    \\\\n    if not data:\\\\n        return \\\\\"\\\\u65e0\\\\u6cd5\\\\u83b7\\\\u53d6\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\u6216API\\\\u8bf7\\\\u6c42\\\\u5931\\\\u8d25\\\\u3002\\\\\"\\\\n    \\\\n    if data.get(\\\\\"code\\\\\") != \\\\\"200\\\\\":\\\\n        return f\\\\\"API \\\\u8fd4\\\\u56de\\\\u9519\\\\u8bef: {data.get(\\'code\\')}\\\\\"\\\\n    \\\\n    daily_forecasts = data.get(\\\\\"daily\\\\\", [])\\\\n    \\\\n    if not daily_forecasts:\\\\n        return f\\\\\"\\\\u65e0\\\\u6cd5\\\\u83b7\\\\u53d6 {location} \\\\u7684\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\u6570\\\\u636e\\\\u3002\\\\\"\\\\n    \\\\n    formatted_forecasts = [format_daily_forecast(daily) for daily in daily_forecasts]\\\\n    return \\\\\"\\\\\\\\n---\\\\\\\\n\\\\\".join(formatted_forecasts)\\\\n\\\\nif __name__ == \\\\\"__main__\\\\\":\\\\n    print(\\\\\"\\\\u6b63\\\\u5728\\\\u542f\\\\u52a8 MCP \\\\u5929\\\\u6c14\\\\u670d\\\\u52a1\\\\u5668...\\\\\")\\\\n    print(\\\\\"\\\\u63d0\\\\u4f9b\\\\u5de5\\\\u5177: get_weather_warning, get_daily_forecast\\\\\")\\\\n    print(\\\\\"\\\\u8bf7\\\\u786e\\\\u4fdd\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf QWEATHER_API_KEY \\\\u5df2\\\\u8bbe\\\\u7f6e\\\\\")\\\\n    print(\\\\\"\\\\u4f7f\\\\u7528 Ctrl+C \\\\u505c\\\\u6b62\\\\u670d\\\\u52a1\\\\u5668\\\\\")\\\\n    \\\\n    # \\\\u521d\\\\u59cb\\\\u5316\\\\u5e76\\\\u8fd0\\\\u884c\\\\u670d\\\\u52a1\\\\u5668\\\\n    mcp.run(transport=\\'stdio\\') \\\\n\", \"timestamp\": \"2025-11-09T00:16:19+0800\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"terminate\", \"args\": {\"message\": \"# MCP Weather Client and Server Project\\\\n\\\\n## Overview:\\\\n\\\\nThis project consists of a client-server application designed to interact with the Model Context Protocol (MCP), a protocol that facilitates standardized communication among various AI models and tools. The MCP Weather application provides weather-related functions by utilizing external APIs and integrates these capabilities into a client-server model.\\\\n\\\\n## Project Structure:\\\\n\\\\n- **Client:** \\\\n  - The client contains multiple implementations demonstrating different integrations:\\\\n    - `mcp_client.py`: A simple client application showcasing MCP client-server interactions, calling tools, and handling responses.\\\\n    - `mcp_client_deepseek.py`: Integration with DeepSeek API in the MCP client, handling user queries and processing them via DeepSeek.\\\\n    - `mcp_client_langchain_chat.py`: Integration with DeepSeek API using a LangChain version, illustrating streamlined tool handling and agent creation.\\\\n\\\\n- **Server:**\\\\n  - `weather_server.py`: An MCP server providing weather information services. It includes tools for retrieving weather warnings and forecasts from a weather API and formats the responses for client use.\\\\n\\\\n## Key Features:\\\\n\\\\n- **MCP Client-Server Communication**: Demonstrates how the client communicates with the server using standard input/output, with asynchronous handling for efficiency.\\\\n- **Tool Integration**: Features tools that gather weather data from external APIs and make it easily accessible through standardized MCP protocols.\\\\n- **Flexible Implementations**: Shows different ways to integrate with AI functionalities - starting from a basic model to more sophisticated methods using third-party libraries like DeepSeek and LangChain.\\\\n\\\\n## Getting Started:\\\\n\\\\n### Prerequisites:\\\\n- Install Python 3.7 or later.\\\\n- Install necessary packages using pip (e.g., `asyncio`, `httpx`, `openai`, `langchain_openai`, `dotenv`, and others mentioned in client implementations).\\\\n\\\\n### Environment Configuration:\\\\n- Create a `.env` file at the root of the project to store essential environment variables.\\\\n- **Sample `.env` Configuration**:\\\\n  ```\\\\n  DEEPSEEK_API_KEY=your_deepseek_api_key\\\\n  DEEPSEEK_BASE_URL=https://api.deepseek.com\\\\n  DEEPSEEK_MODEL=deepseek-chat\\\\n  QWEATHER_API_BASE=your_qweather_api_base\\\\n  QWEATHER_API_KEY=your_qweather_api_key\\\\n  ```\\\\n\\\\n### Running the Server:\\\\n1. Navigate to the server directory (e.g., `mcp-demo/server`).\\\\n2. Execute `python weather_server.py` to start the server.\\\\n\\\\n### Running the Client:\\\\n1. Choose a client implementation from the `mcp-demo/client` directory.\\\\n2. Execute the script using Python (e.g., `python mcp_client.py`).\\\\n3. Follow the command prompts to interact with the MCP server.\\\\n\\\\n## Usage:\\\\n- The client applications prompt for user input commands to perform various operations, including fetching weather warnings and forecasts.\\\\n- Server responses are processed and displayed to the user through formatted text, providing valuable weather insights.\\\\n\\\\n## Future Improvements:\\\\n\\\\n- Expand tool capabilities to include more advanced MCP features and integrate additional APIs.\\\\n- Enhance error handling and logging to provide deeper insights into application behavior.\\\\n\\\\n---\\\\nThis project offers a well-structured demonstration of how MCP can be utilized for tool integration and provides practical examples for weather-based applications using modern AI frameworks and protocols.\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"# MCP Weather Client and Server Project\\\\n\\\\n## Overview:\\\\n\\\\nThis project consists of a client-server application designed to interact with the Model Context Protocol (MCP), a protocol that facilitates standardized communication among various AI models and tools. The MCP Weather application provides weather-related functions by utilizing external APIs and integrates these capabilities into a client-server model.\\\\n\\\\n## Project Structure:\\\\n\\\\n- **Client:** \\\\n  - The client contains multiple implementations demonstrating different integrations:\\\\n    - `mcp_client.py`: A simple client application showcasing MCP client-server interactions, calling tools, and handling responses.\\\\n    - `mcp_client_deepseek.py`: Integration with DeepSeek API in the MCP client, handling user queries and processing them via DeepSeek.\\\\n    - `mcp_client_langchain_chat.py`: Integration with DeepSeek API using a LangChain version, illustrating streamlined tool handling and agent creation.\\\\n\\\\n- **Server:**\\\\n  - `weather_server.py`: An MCP server providing weather information services. It includes tools for retrieving weather warnings and forecasts from a weather API and formats the responses for client use.\\\\n\\\\n## Key Features:\\\\n\\\\n- **MCP Client-Server Communication**: Demonstrates how the client communicates with the server using standard input/output, with asynchronous handling for efficiency.\\\\n- **Tool Integration**: Features tools that gather weather data from external APIs and make it easily accessible through standardized MCP protocols.\\\\n- **Flexible Implementations**: Shows different ways to integrate with AI functionalities - starting from a basic model to more sophisticated methods using third-party libraries like DeepSeek and LangChain.\\\\n\\\\n## Getting Started:\\\\n\\\\n### Prerequisites:\\\\n- Install Python 3.7 or later.\\\\n- Install necessary packages using pip (e.g., `asyncio`, `httpx`, `openai`, `langchain_openai`, `dotenv`, and others mentioned in client implementations).\\\\n\\\\n### Environment Configuration:\\\\n- Create a `.env` file at the root of the project to store essential environment variables.\\\\n- **Sample `.env` Configuration**:\\\\n  ```\\\\n  DEEPSEEK_API_KEY=your_deepseek_api_key\\\\n  DEEPSEEK_BASE_URL=https://api.deepseek.com\\\\n  DEEPSEEK_MODEL=deepseek-chat\\\\n  QWEATHER_API_BASE=your_qweather_api_base\\\\n  QWEATHER_API_KEY=your_qweather_api_key\\\\n  ```\\\\n\\\\n### Running the Server:\\\\n1. Navigate to the server directory (e.g., `mcp-demo/server`).\\\\n2. Execute `python weather_server.py` to start the server.\\\\n\\\\n### Running the Client:\\\\n1. Choose a client implementation from the `mcp-demo/client` directory.\\\\n2. Execute the script using Python (e.g., `python mcp_client.py`).\\\\n3. Follow the command prompts to interact with the MCP server.\\\\n\\\\n## Usage:\\\\n- The client applications prompt for user input commands to perform various operations, including fetching weather warnings and forecasts.\\\\n- Server responses are processed and displayed to the user through formatted text, providing valuable weather insights.\\\\n\\\\n## Future Improvements:\\\\n\\\\n- Expand tool capabilities to include more advanced MCP features and integrate additional APIs.\\\\n- Enhance error handling and logging to provide deeper insights into application behavior.\\\\n\\\\n---\\\\nThis project offers a well-structured demonstration of how MCP can be utilized for tool integration and provides practical examples for weather-based applications using modern AI frameworks and protocols.\\\\nTerminating...\", \"timestamp\": \"2025-11-09T00:16:27+0800\"}'}]\n"
     ]
    }
   ],
   "source": [
    "# =============================== 示例：最小可运行 Agent ===============================\n",
    "# 1) 定义智能体目标（Goals）：\n",
    "#    - 读取项目中的每个文件\n",
    "#    - 当已读取完毕时调用 terminate，并在消息中提供 README 的内容（示例环境如为空目录会直接终止）\n",
    "goals = [\n",
    "    Goal(priority=1, name=\"Gather Information\", description=\"Read each file in the project\"),\n",
    "    Goal(priority=1, name=\"Terminate\", description=\"Call the terminate call when you have read all the files \"\n",
    "                                                   \"and provide the content of the README in the terminate message\")\n",
    "]\n",
    "\n",
    "# 2) 指定语言适配器（基于函数调用的 Prompt/解析策略）\n",
    "agent_language = AgentFunctionCallingActionLanguage()\n",
    "\n",
    "# 3) 实现底层动作：读取文件\n",
    "def read_project_file(name: str) -> str:\n",
    "    with open(name, \"r\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# 4) 实现底层动作：列出当前目录及子目录下的 .py 文件（最小示例）\n",
    "def list_project_files() -> List[str]:\n",
    "    \"\"\"\n",
    "    使用 pathlib.Path.rglob 递归查找所有 .py 文件。\n",
    "    \"\"\"\n",
    "    # 1. Path(\".\") 代表当前目录\n",
    "    # 2. .rglob(\"*.py\") 递归地 (r) 查找 (glob) 所有匹配 \"*.py\" 的文件\n",
    "    \n",
    "    # rglob 会返回一个生成器，包含所有匹配的 Path 对象\n",
    "    py_files_paths = Path(\".\").rglob(\"*.py\")\n",
    "    \n",
    "    # 3. 将 Path 对象转换为字符串，并确保它们是文件（排除目录）\n",
    "    #    str(p) 会自动生成正确的相对路径（例如 'my_app/helpers.py'）\n",
    "    files = [str(p) for p in py_files_paths if p.is_file()]\n",
    "    \n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "\n",
    "# 5) 注册动作：将 Python 函数“暴露”为可被 LLM 选择的工具\n",
    "action_registry = ActionRegistry()\n",
    "action_registry.register(Action(\n",
    "    name=\"list_project_files\",\n",
    "    function=list_project_files,\n",
    "    description=\"Lists all files in the project.\",\n",
    "    parameters={},\n",
    "    terminal=False\n",
    "))\n",
    "action_registry.register(Action(\n",
    "    name=\"read_project_file\",\n",
    "    function=read_project_file,\n",
    "    description=\"Reads a file from the project.\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"name\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"required\": [\"name\"]\n",
    "    },\n",
    "    terminal=False\n",
    "))\n",
    "action_registry.register(Action(\n",
    "    name=\"terminate\",\n",
    "    function=lambda message: f\"{message}\\nTerminating...\",\n",
    "    description=\"Terminates the session and prints the message to the user.\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"message\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"required\": []\n",
    "    },\n",
    "    terminal=True\n",
    "))\n",
    "\n",
    "# 6) 准备环境（负责真实执行动作并返回标准化结果）\n",
    "environment = Environment()\n",
    "\n",
    "# 7) 构建 Agent 实例（组装 G/A/M/E 与 LLM 响应函数）\n",
    "agent = Agent(goals, agent_language, action_registry, generate_response, environment)\n",
    "\n",
    "# 8) 运行智能体（输入一个自然语言任务），内部会进入循环直到触发终止或达到最大轮数\n",
    "user_input = \"Write a README for this project.\"\n",
    "final_memory = agent.run(user_input)\n",
    "\n",
    "# 9) 输出最终的记忆（包含用户任务、助手决策、环境执行结果等）\n",
    "print(final_memory.get_memories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e827eKzCZIQQ",
    "outputId": "8bf37f9d-f285-4adf-9f77-44dc6b4c7c09"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# 智能体执行报告\n",
       "\n",
       "## 概览\n",
       "\n",
       "- 总事件数：13\n",
       "\n",
       "\n",
       "## 交互明细\n",
       "\n",
       "\n",
       "### 步骤 1 · 用户输入\n",
       "\n",
       "> Write a README for this project.\n",
       "\n",
       "### 步骤 2 · 助手决策（工具调用）\n",
       "\n",
       "- 工具：`list_project_files`\n",
       "\n",
       "- 参数：\n",
       "\n",
       "```json\n",
       "{}\n",
       "```\n",
       "\n",
       "### 步骤 3 · 环境执行结果\n",
       "\n",
       "执行成功（2025-11-09T00:16:12+0800）\n",
       "\n",
       "可用文件列表：\n",
       "- mcp-demo/client/mcp_client.py\n",
       "- mcp-demo/client/mcp_client_deepseek.py\n",
       "- mcp-demo/client/mcp_client_langchain_chat.py\n",
       "- mcp-demo/server/weather_server.py\n",
       "\n",
       "### 步骤 4 · 助手决策（工具调用）\n",
       "\n",
       "- 工具：`read_project_file`\n",
       "\n",
       "- 参数：\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"name\": \"mcp-demo/client/mcp_client.py\"\n",
       "}\n",
       "```\n",
       "\n",
       "### 步骤 5 · 环境执行结果\n",
       "\n",
       "执行成功（2025-11-09T00:16:14+0800）\n",
       "\n",
       "```text\n",
       "#!/usr/bin/env python3\n",
       "# -*- coding: utf-8 -*-\n",
       "\n",
       "\"\"\"\n",
       "MCP 客户端示例\n",
       "\n",
       "用于与 MCP 服务器交互，调用工具并处理结果\n",
       "\n",
       "Author: FlyAIBox\n",
       "Date: 2025.10.11\n",
       "\"\"\"\n",
       "\n",
       "import asyncio\n",
       "import json\n",
       "import os\n",
       "import subprocess\n",
       "import sys\n",
       "from typing import Dict, Any, List, Optional\n",
       "from contextlib import AsyncExitStack\n",
       "\n",
       "# 使用正确的 MCP 导入\n",
       "from mcp import ClientSession, StdioServerParameters, Tool\n",
       "from mcp.client.stdio import stdio_client\n",
       "\n",
       "class SimpleClientApp:\n",
       "    \"\"\"简单的 MCP 客户端应用\"\"\"\n",
       "    \n",
       "    def __init__(self, server_command: List[str]):\n",
       "        \"\"\"\n",
       "        初始化 MCP 客户端应用\n",
       "        \n",
       "        参数:\n",
       "            server_command: 启动服务器的命令列表\n",
       "        \"\"\"\n",
       "        self.server_command = server_command\n",
       "        self.server_process = None\n",
       "        self.client = None\n",
       "        self.tool_definitions = []\n",
       "        self.exit_stack = AsyncExitStack()\n",
       "    \n",
       "    async def start(self):\n",
       "        \"\"\"启动 MCP 客户端并连接到服务器\"\"\"\n",
       "        print(\"启动 MCP 服务器进程...\")\n",
       "        \n",
       "        # 配置服务器参数\n",
       "        server_params = StdioServerParameters(\n",
       "            command=self.server_command[0],\n",
       "            args=self.server_command[1:],\n",
       "            env=None  # 使用默认环境变量\n",
       "        )\n",
       "        \n",
       "        # 启动服务器并获取通信流\n",
       "        # 通过 stdio_client 协程，以异步方式建立与服务器进程的标准输入/输出通道\n",
       "        # enter_async_context 可以确保后续资源能自动释放（类似 try...finally）\n",
       "        # stdio_client(server_params) 会启动子进程并返回 (reader, writer) 的元组\n",
       "        read_write = await self.exit_stack.enter_async_context(stdio_client(server_params))\n",
       "        # 解包得到 reader（读取服务器输出）、writer（写入服务器输入），用于后续通信\n",
       "        # read_write 是一个包含两个元素的元组，分别对应于与服务器通信的 “读取” (reader) 和 “写入” (writer) 流对象。\n",
       "        read, write = read_write\n",
       "        \n",
       "        # 创建 MCP 客户端会话\n",
       "        self.client = await self.exit_stack.enter_async_context(ClientSession(read, write))\n",
       "        \n",
       "        # 初始化连接\n",
       "        await self.client.initialize()\n",
       "        \n",
       "        # 获取工具定义\n",
       "        response = await self.client.list_tools()\n",
       "        self.tool_definitions = response.tools\n",
       "        \n",
       "        print(f\"已连接到服务器，可用工具: {len(self.tool_definitions)}\")\n",
       "        \n",
       "        for tool in self.tool_definitions:\n",
       "            print(f\"  - {tool.name}: {tool.description}\")\n",
       "        \n",
       "        print(\"\\n使用 'help' 查看帮助，使用 'exit' 退出\")\n",
       "    \n",
       "    async def execute_tool(self, tool_name: str, params: Dict[str, Any]) -> Optional[str]:\n",
       "        \"\"\"\n",
       "        执行工具调用\n",
       "        \n",
       "        参数:\n",
       "            tool_name: 要调用的工具名称\n",
       "            params: 工具参数字典\n",
       "            \n",
       "        返回:\n",
       "            工具执行结果或错误消息\n",
       "        \"\"\"\n",
       "        # 检查工具是否存在\n",
       "        # 遍历工具定义列表，查找名称与 tool_name 匹配的工具对象\n",
       "        # next() 用于返回第一个匹配的工具（如未找到则返回 None）\n",
       "        tool_def = next((t for t in self.tool_definitions if t.name == tool_name), None)\n",
       "        if not tool_def:\n",
       "            return f\"错误: 未找到工具 '{tool_name}'\"\n",
       "        \n",
       "        try:\n",
       "            # 调用工具并等待结果\n",
       "            result = await self.client.call_tool(tool_name, arguments=params)\n",
       "            \n",
       "            # 处理结果\n",
       "            if result and hasattr(result, 'content'):\n",
       "                return result.content\n",
       "            else:\n",
       "                return \"工具执行未返回任何结果\"\n",
       "        \n",
       "        except Exception as e:\n",
       "            return f\"执行过程中出错: {str(e)}\"\n",
       "    \n",
       "    async def stop(self):\n",
       "        \"\"\"停止客户端和服务器\"\"\"\n",
       "        print(\"正在关闭 MCP 客户端...\")\n",
       "        await self.exit_stack.aclose()\n",
       "        print(\"已关闭\")\n",
       "    \n",
       "    def print_help(self):\n",
       "        \"\"\"打印帮助信息\"\"\"\n",
       "        print(\"\\n可用命令:\")\n",
       "        print(\"  help - 显示此帮助信息\")\n",
       "        print(\"  list - 列出可用工具\")\n",
       "        print(\"  call <工具名> <参数JSON> - 调用工具\")\n",
       "        print(\"  exit - 退出程序\")\n",
       "        print(\"\\n示例:\")\n",
       "        print(\"  call get_weather_warning {\\\"location\\\": \\\"101010100\\\"}\")\n",
       "        print(\"  call get_daily_forecast 116.41,39.92\")\n",
       "        print(\"  call get_daily_forecast 101010100 7\")\n",
       "    \n",
       "    def print_tools(self):\n",
       "        \"\"\"打印工具列表和描述\"\"\"\n",
       "        print(\"\\n可用工具:\")\n",
       "        for tool in self.tool_definitions:\n",
       "            print(f\"  {tool.name} - {tool.description}\")\n",
       "            if hasattr(tool, 'inputSchema') and tool.inputSchema:\n",
       "                print(\"    参数:\")\n",
       "                props = tool.inputSchema.get('properties', {})\n",
       "                for param_name, param_info in props.items():\n",
       "                    param_type = param_info.get('type', 'unknown')\n",
       "                    param_desc = param_info.get('description', '')\n",
       "                    print(f\"      {param_name} ({param_type}): {param_desc}\")\n",
       "\n",
       "async def main():\n",
       "    \"\"\"主函数\"\"\"\n",
       "    # 确定服务器路径\n",
       "    server_path = os.path.join(\n",
       "        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\n",
       "        \"server\",\n",
       "        \"weather_server.py\"\n",
       "    )\n",
       "    \n",
       "    # 检查服务器文件是否存在\n",
       "    if not os.path.exists(server_path):\n",
       "        print(f\"错误: 服务器文件不存在: {server_path}\")\n",
       "        return\n",
       "    \n",
       "    # 创建客户端应用\n",
       "    app = SimpleClientApp([\"python\", server_path])\n",
       "    \n",
       "    try:\n",
       "        # 启动客户端\n",
       "        await app.start()\n",
       "        \n",
       "        # 主循环\n",
       "        while True:\n",
       "            try:\n",
       "                # 获取用户输入\n",
       "                command = input(\"\\n> \").strip()\n",
       "                \n",
       "                # 处理命令\n",
       "                if command == \"exit\":\n",
       "                    break\n",
       "                \n",
       "                elif command == \"help\":\n",
       "                    app.print_help()\n",
       "                \n",
       "                elif command == \"list\":\n",
       "                    app.print_tools()\n",
       "                \n",
       "                elif command.startswith(\"call \"):\n",
       "                    # 解析命令\n",
       "                    parts = command[5:].strip().split(\" \", 1)\n",
       "                    if len(parts) < 1:\n",
       "                        print(\"错误: 缺少工具名\")\n",
       "                        continue\n",
       "                    \n",
       "                    tool_name = parts[0]\n",
       "                    \n",
       "                    # 解析参数\n",
       "                    params = {}\n",
       "                    if len(parts) > 1:\n",
       "                        params_str = parts[1]\n",
       "                        \n",
       "                        # 尝试解析为 JSON\n",
       "                        try:\n",
       "                            params = json.loads(params_str)\n",
       "                        except json.JSONDecodeError:\n",
       "                            # 如果不是 JSON，尝试简单解析\n",
       "                            try:\n",
       "                                # 对于 get_weather_warning，只需要一个位置参数\n",
       "                                if tool_name == \"get_weather_warning\":\n",
       "                                    params = {\"location\": params_str}\n",
       "                                \n",
       "                                # 对于 get_daily_forecast，需要位置和可选的天数\n",
       "                                elif tool_name == \"get_daily_forecast\":\n",
       "                                    forecast_args = params_str.split()\n",
       "                                    if len(forecast_args) < 1:\n",
       "                                        print(\"错误: get_daily_forecast 至少需要一个位置参数\")\n",
       "                                        continue\n",
       "                                    \n",
       "                                    params = {\"location\": forecast_args[0]}\n",
       "                                    \n",
       "                                    # 如果提供了天数参数\n",
       "                                    if len(forecast_args) > 1:\n",
       "                                        try:\n",
       "                                            params[\"days\"] = int(forecast_args[1])\n",
       "                                        except ValueError:\n",
       "                                            print(\"错误: days 参数必须是整数\")\n",
       "                                            continue\n",
       "                                else:\n",
       "                                    print(f\"错误: 无法解析 {tool_name} 的参数\")\n",
       "                                    continue\n",
       "                            \n",
       "                            except Exception as e:\n",
       "                                print(f\"参数解析错误: {str(e)}\")\n",
       "                                continue\n",
       "                    \n",
       "                    # 执行工具调用\n",
       "                    print(\"正在调用工具...\")\n",
       "                    result = await app.execute_tool(tool_name, params)\n",
       "                    print(\"\\n结果:\")\n",
       "                    print(result)\n",
       "                \n",
       "                else:\n",
       "                    print(f\"未知命令: {command}\")\n",
       "                    print(\"使用 'help' 查看可用命令\")\n",
       "            \n",
       "            except KeyboardInterrupt:\n",
       "                print(\"\\n操作被中断\")\n",
       "                break\n",
       "            \n",
       "            except Exception as e:\n",
       "                print(f\"错误: {str(e)}\")\n",
       "    \n",
       "    except Exception as e:\n",
       "        print(f\"客户端错误: {str(e)}\")\n",
       "    \n",
       "    finally:\n",
       "        # 停止客户端\n",
       "        await app.stop()\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    try:\n",
       "        asyncio.run(main())\n",
       "    except KeyboardInterrupt:\n",
       "        print(\"\\n程序被中断\")\n",
       "        sys.exit(0) \n",
       "```\n",
       "\n",
       "### 步骤 6 · 助手决策（工具调用）\n",
       "\n",
       "- 工具：`read_project_file`\n",
       "\n",
       "- 参数：\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"name\": \"mcp-demo/client/mcp_client_deepseek.py\"\n",
       "}\n",
       "```\n",
       "\n",
       "### 步骤 7 · 环境执行结果\n",
       "\n",
       "执行成功（2025-11-09T00:16:16+0800）\n",
       "\n",
       "```text\n",
       "\"\"\"\n",
       "# 集成 DeepSeek 的 MCP 客户端\n",
       "#参考官方案例：https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-chatbot/mcp_simple_chatbot/main.py\n",
       "\n",
       "\n",
       "# 本模块实现了一个模型上下文协议（MCP）客户端，该客户端使用 DeepSeek 的 API\n",
       "# 来处理查询并与 MCP 工具进行交互。它演示了如何：\n",
       "# 1. 连接到 MCP 服务器\n",
       "# 2. 使用 DeepSeek 的 API 来处理查询\n",
       "# 3. 处理工具调用和响应\n",
       "# 4. 维护一个交互式聊天循环\n",
       "\n",
       "# 所需环境变量：\n",
       "# DEEPSEEK_API_KEY：DeepSeek API 密钥 (格式：sk-xxxx...)\n",
       "# DEEPSEEK_BASE_URL：DeepSeek API 基础 URL (https://api.deepseek.com)\n",
       "# DEEPSEEK_MODEL：DeepSeek 模型名称 (例如 deepseek-chat)\n",
       "\n",
       "Author: FlyAIBox\n",
       "Date: 2025.10.11\n",
       "\"\"\"\n",
       "\n",
       "import json\n",
       "import asyncio\n",
       "import logging\n",
       "import os\n",
       "from typing import Optional, Dict, Any, List, Tuple\n",
       "from contextlib import AsyncExitStack\n",
       "\n",
       "from openai import OpenAI\n",
       "from dotenv import load_dotenv\n",
       "\n",
       "from mcp import ClientSession, StdioServerParameters\n",
       "from mcp.client.stdio import stdio_client\n",
       "\n",
       "# 配置日志记录\n",
       "logging.basicConfig(\n",
       "    level=logging.INFO,\n",
       "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
       ")\n",
       "logger = logging.getLogger(__name__)\n",
       "\n",
       "load_dotenv()\n",
       "\n",
       "\n",
       "class Configuration:\n",
       "    \"\"\"配置管理类，负责管理和验证环境变量\"\"\"\n",
       "    \n",
       "    def __init__(self) -> None:\n",
       "        \"\"\"初始化配置并加载环境变量\"\"\"\n",
       "        self.load_env()\n",
       "        self._validate_env()\n",
       "        \n",
       "    @staticmethod\n",
       "    def load_env() -> None:\n",
       "        \"\"\"从.env文件加载环境变量\"\"\"\n",
       "        load_dotenv()\n",
       "        \n",
       "    def _validate_env(self) -> None:\n",
       "        \"\"\"验证必需的环境变量是否存在\"\"\"\n",
       "        required_vars = [\"DEEPSEEK_API_KEY\"]\n",
       "        missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
       "        if missing_vars:\n",
       "            raise ValueError(f\"缺少必需的环境变量: {', '.join(missing_vars)}\")\n",
       "    \n",
       "    @property\n",
       "    def api_key(self) -> str:\n",
       "        \"\"\"获取 DeepSeek API 密钥\"\"\"\n",
       "        return os.getenv(\"DEEPSEEK_API_KEY\", \"\")\n",
       "    \n",
       "    @property\n",
       "    def base_url(self) -> str:\n",
       "        \"\"\"获取 DeepSeek API 基础 URL\"\"\"\n",
       "        return os.getenv(\"DEEPSEEK_BASE_URL\", \"https://api.deepseek.com\")\n",
       "    \n",
       "    @property\n",
       "    def model(self) -> str:\n",
       "        \"\"\"获取 DeepSeek 模型名称\"\"\"\n",
       "        return os.getenv(\"DEEPSEEK_MODEL\", \"deepseek-chat\")\n",
       "\n",
       "class Tool:\n",
       "    \"\"\"MCP 工具类，表示一个具有属性的工具\"\"\"\n",
       "    \n",
       "    def __init__(self, name: str, description: str, input_schema: Dict[str, Any]) -> None:\n",
       "        \"\"\"\n",
       "        初始化工具\n",
       "        \n",
       "        Args:\n",
       "            name: 工具名称\n",
       "            description: 工具描述\n",
       "            input_schema: 输入参数模式\n",
       "        \"\"\"\n",
       "        self.name = name\n",
       "        self.description = description\n",
       "        self.input_schema = input_schema\n",
       "        \n",
       "    def to_openai_format(self) -> Dict[str, Any]:\n",
       "        \"\"\"将工具转换为 OpenAI API 格式\"\"\"\n",
       "        return {\n",
       "            \"type\": \"function\",\n",
       "            \"function\": {\n",
       "                \"name\": self.name,\n",
       "                \"description\": self.description,\n",
       "                \"parameters\": self.input_schema\n",
       "            }\n",
       "        }\n",
       "\n",
       "class MCPServer:\n",
       "    \"\"\"MCP 服务器管理类，处理服务器连接和工具执行\"\"\"\n",
       "    \n",
       "    def __init__(self, server_path: str) -> None:\n",
       "        \"\"\"\n",
       "        初始化服务器管理器\n",
       "        \n",
       "        Args:\n",
       "            server_path: 服务器脚本路径\n",
       "        \"\"\"\n",
       "        self.server_path = server_path\n",
       "        self.session: Optional[ClientSession] = None\n",
       "        self.exit_stack = AsyncExitStack()\n",
       "        self._cleanup_lock = asyncio.Lock()\n",
       "        \n",
       "    async def initialize(self) -> None:\n",
       "        \"\"\"初始化服务器连接，包含重试机制\"\"\"\n",
       "        max_retries = 3\n",
       "        retry_delay = 1.0\n",
       "        \n",
       "        for attempt in range(max_retries):\n",
       "            try:\n",
       "                if not os.path.exists(self.server_path):\n",
       "                    raise FileNotFoundError(f\"找不到服务器文件: {self.server_path}\")\n",
       "                \n",
       "                server_params = StdioServerParameters(\n",
       "                    command='python',\n",
       "                    args=[self.server_path],\n",
       "                    env=None\n",
       "                )\n",
       "                \n",
       "                stdio_transport = await self.exit_stack.enter_async_context(\n",
       "                    stdio_client(server_params)\n",
       "                )\n",
       "                stdio, write = stdio_transport\n",
       "                \n",
       "                self.session = await self.exit_stack.enter_async_context(\n",
       "                    ClientSession(stdio, write)\n",
       "                )\n",
       "                await self.session.initialize()\n",
       "                logger.info(\"成功连接到 MCP 服务器\")\n",
       "                break\n",
       "                \n",
       "            except Exception as e:\n",
       "                logger.error(f\"第 {attempt + 1}/{max_retries} 次尝试失败: {str(e)}\")\n",
       "                if attempt < max_retries - 1:\n",
       "                    await asyncio.sleep(retry_delay)\n",
       "                else:\n",
       "                    raise\n",
       "                    \n",
       "    async def list_tools(self) -> List[Tool]:\n",
       "        \"\"\"获取服务器提供的可用工具列表\"\"\"\n",
       "        if not self.session:\n",
       "            raise RuntimeError(\"服务器未初始化\")\n",
       "            \n",
       "        response = await self.session.list_tools()\n",
       "        return [\n",
       "            Tool(tool.name, tool.description, tool.inputSchema)\n",
       "            for tool in response.tools\n",
       "        ]\n",
       "        \n",
       "    async def execute_tool(\n",
       "        self,\n",
       "        tool_name: str,\n",
       "        arguments: Dict[str, Any],\n",
       "        retries: int = 2,\n",
       "        delay: float = 1.0\n",
       "    ) -> Any:\n",
       "        \"\"\"\n",
       "        执行工具，包含重试机制\n",
       "        \n",
       "        Args:\n",
       "            tool_name: 工具名称\n",
       "            arguments: 工具参数\n",
       "            retries: 重试次数\n",
       "            delay: 重试延迟时间（秒）\n",
       "            \n",
       "        Returns:\n",
       "            工具执行结果\n",
       "        \"\"\"\n",
       "        if not self.session:\n",
       "            raise RuntimeError(\"服务器未初始化\")\n",
       "            \n",
       "        for attempt in range(retries):\n",
       "            try:\n",
       "                logger.info(f\"执行工具 {tool_name}，参数: {arguments}\")\n",
       "                result = await self.session.call_tool(tool_name, arguments)\n",
       "                return result\n",
       "                \n",
       "            except Exception as e:\n",
       "                logger.error(f\"工具执行失败 (第 {attempt + 1}/{retries} 次尝试): {str(e)}\")\n",
       "                if attempt < retries - 1:\n",
       "                    await asyncio.sleep(delay)\n",
       "                else:\n",
       "                    raise\n",
       "                    \n",
       "    async def cleanup(self) -> None:\n",
       "        \"\"\"清理服务器资源\"\"\"\n",
       "        async with self._cleanup_lock:\n",
       "            try:\n",
       "                await self.exit_stack.aclose()\n",
       "                self.session = None\n",
       "                logger.info(\"服务器资源清理完成\")\n",
       "            except Exception as e:\n",
       "                logger.error(f\"清理过程中出错: {str(e)}\")\n",
       "\n",
       "class MCPClient:\n",
       "    \"\"\"MCP 客户端实现，集成了 DeepSeek API\"\"\"\n",
       "    \n",
       "    def __init__(self, config: Configuration) -> None:\n",
       "        \"\"\"\n",
       "        初始化 MCP 客户端\n",
       "        \n",
       "        Args:\n",
       "            config: 配置对象\n",
       "        \"\"\"\n",
       "        self.config = config\n",
       "        self.server: Optional[MCPServer] = None\n",
       "        self.client = OpenAI(\n",
       "            api_key=config.api_key,\n",
       "            base_url=config.base_url\n",
       "        )\n",
       "        \n",
       "    async def initialize(self) -> None:\n",
       "        \"\"\"初始化客户端并连接到服务器\"\"\"\n",
       "        server_path = os.path.join(\n",
       "            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\n",
       "            \"server\",\n",
       "            \"weather_server.py\"\n",
       "        )\n",
       "        self.server = MCPServer(server_path)\n",
       "        await self.server.initialize()\n",
       "        \n",
       "\n",
       "    async def process_query(self, query: str) -> str:\n",
       "        \"\"\"\n",
       "        处理用户查询，集成工具调用，支持多轮工具交互\n",
       "\n",
       "        Args:\n",
       "            query: 用户查询字符串\n",
       "\n",
       "        Returns:\n",
       "            处理后的响应结果\n",
       "        \"\"\"\n",
       "        if not self.server:\n",
       "            raise RuntimeError(\"客户端未初始化\")\n",
       "\n",
       "        system_prompt = (\n",
       "            \"You are a helpful assistant specializing in weather information.\\n\"\n",
       "            \"You have access to the MCP Weather Server tool with the following functions:\\n\"\n",
       "            \"- get_weather_warning(city_id=None, latitude=None, longitude=None): Retrieves weather disaster warnings for a specified city ID or coordinates.\\n\"\n",
       "            \"- get_daily_forecast(city_id=None, latitude=None, longitude=None): Retrieves the multi-day weather forecast for a specified city ID or coordinates.\\n\"\n",
       "            \"\\n\"\n",
       "            \"Core Instructions:\\n\"\n",
       "            \"1.  **Carefully analyze the user's request**: Understand all components of the user's query. Determine if the user needs weather warning information, weather forecast information, or both.\\n\"\n",
       "            \"2.  **Identify Information Needs**:\\n\"\n",
       "            \"    * If the user only asks for warnings (e.g., \\\"Are there any warnings in Beijing?\\\"), only use `get_weather_warning`.\\n\"\n",
       "            \"    * If the user only asks for the forecast (e.g., \\\"What's the weather like in Beijing tomorrow?\\\"), only use `get_daily_forecast`.\\n\"\n",
       "            \"    * **If the user's question includes multiple aspects**, such as asking about **warning status** and also asking **if it's suitable for a certain activity** (which implies a query about future weather, like \\\"Have there been high temperature warnings in Beijing in the last week? Is it suitable for outdoor activities?\\\"), you need to **call both tools sequentially**.\\n\"\n",
       "            \"3.  **Call Tools as Needed**:\\n\"\n",
       "            \"    * **Prioritize getting warning information**: If warning information is needed, first call `get_weather_warning`.\\n\"\n",
       "            \"    * **Get the weather forecast**: If the user mentions a specific time period (e.g., \\\"weekend\\\", \\\"next three days\\\", \\\"next week\\\") or asks about activity suitability (which typically concerns the next few days), call `get_daily_forecast` to get the forecast for the corresponding period. For vague phrases like \\\"last week\\\" or \\\"recently\\\", interpret it as asking about *current* conditions and the *upcoming* few days (covered by the forecast). For questions like \\\"Is it suitable for outdoor activities?\\\", you should get the forecast for at least the next 2-3 days (e.g., today, tomorrow, the day after tomorrow, or the upcoming weekend) to support your judgment.\\n\"\n",
       "            \"    * **Ensure tool call order**: When multiple tools need to be called, they should be called in a logical sequence. For example, first get the warning, then get the forecast. Wait for one tool to finish executing before deciding whether to call the next tool or generate a response.\\n\"\n",
       "            \"4.  **Information Integration and Response**:\\n\"\n",
       "            \"    * After obtaining all necessary information (warning, forecast), you **must synthesize and analyze this information**.\\n\"\n",
       "            \"    * **Completely answer the user's question**: Ensure you answer all parts of the user's query.\\n\"\n",
       "            \"    * **Provide advice**: If the user asks about activity suitability, based on the retrieved warning status and forecast information (temperature, weather condition - clear/rainy, wind strength, etc.), provide a clear, data-supported recommendation (e.g., \\\"Currently there are no high temperature warnings, but it's expected to rain this weekend, so it's not very suitable for outdoor activities,\\\" or \\\"It will be sunny for the next few days with no warnings, suitable for outdoor activities.\\\").\\n\"\n",
       "            \"5.  **Tool Usage Details**:\\n\"\n",
       "            \"    * When using the tools, retain the full context of the user's original question.\\n\"\n",
       "            \"    * Unless explicitly requested by the user, do not insert specific times of day (e.g., \\\"3 PM\\\") into the search query or your response.\\n\"\n",
       "            \"    * When city information is needed, if the user provides a city name (e.g., \\\"Beijing\\\"), use the corresponding `city_id` (e.g., Beijing's city_id might be '101010100').\\n\"\n",
       "        )\n",
       "\n",
       "        messages = [\n",
       "            {\"role\": \"system\", \"content\": system_prompt},\n",
       "            {\"role\": \"user\", \"content\": query}\n",
       "        ]\n",
       "\n",
       "        tools = await self.server.list_tools()\n",
       "        available_tools = [tool.to_openai_format() for tool in tools]\n",
       "\n",
       "        max_tool_turns = 5 # 设置一个最大工具调用回合数，防止无限循环\n",
       "\n",
       "        for _ in range(max_tool_turns):\n",
       "            try:\n",
       "                response = self.client.chat.completions.create(\n",
       "                    model=self.config.model,\n",
       "                    messages=messages,\n",
       "                    tools=available_tools,\n",
       "                    tool_choice=\"auto\" # 允许模型决定是否调用工具\n",
       "                )\n",
       "\n",
       "                content = response.choices[0].message\n",
       "                finish_reason = response.choices[0].finish_reason\n",
       "\n",
       "                if finish_reason == \"stop\":\n",
       "                    # 模型生成了最终的文本回复\n",
       "                    messages.append({\"role\": \"assistant\", \"content\": content.content})\n",
       "                    return content.content\n",
       "                elif finish_reason == \"tool_calls\":\n",
       "                    # 模型预测了工具调用\n",
       "                    messages.append(content.model_dump()) # 将 assistant tool_calls 消息添加到历史\n",
       "\n",
       "                    tool_outputs = []\n",
       "                    for tool_call in content.tool_calls:\n",
       "                        tool_name = tool_call.function.name\n",
       "                        tool_args = json.loads(tool_call.function.arguments)\n",
       "\n",
       "                        try:\n",
       "                            # 执行工具\n",
       "                            result = await self.server.execute_tool(tool_name, tool_args)\n",
       "                            logger.info(f\"工具执行完成: {tool_name}\")\n",
       "                            # 将工具结果添加到 tool_outputs 列表\n",
       "                            tool_outputs.append({\n",
       "                                \"tool_call_id\": tool_call.id,\n",
       "                                # MCP tool results have 'content' which is a list of Parts\n",
       "                                # Assuming text content is the first part\n",
       "                                \"output\": result.content[0].text\n",
       "                            })\n",
       "                        except Exception as tool_exec_error:\n",
       "                            logger.error(f\"工具执行失败 {tool_name}: {str(tool_exec_error)}\")\n",
       "                            # 如果工具执行失败，也需要向模型反馈失败信息\n",
       "                            tool_outputs.append({\n",
       "                                \"tool_call_id\": tool_call.id,\n",
       "                                \"output\": f\"Error executing tool {tool_name}: {str(tool_exec_error)}\"\n",
       "                            })\n",
       "\n",
       "                    # 将所有工具结果作为 tool 消息添加到历史\n",
       "                    # 注意：OpenAI API 要求每个 tool_call_id 对应一个 tool 消息\n",
       "                    # 如果模型预测了多个工具调用，这里需要为每个结果添加 tool 消息\n",
       "                    for output in tool_outputs:\n",
       "                        messages.append({\n",
       "                            \"role\": \"tool\",\n",
       "                            \"content\": output[\"output\"],\n",
       "                            \"tool_call_id\": output[\"tool_call_id\"],\n",
       "                        })\n",
       "\n",
       "                    # 继续下一轮循环，将完整的消息历史发送给模型，让它根据工具结果继续\n",
       "                    logger.info(f\"工具调用回合完成，继续与模型交互...\")\n",
       "                    continue # 继续循环，发送包含工具结果的消息历史给模型\n",
       "                else:\n",
       "                    # 发生未知情况或错误\n",
       "                    logger.error(f\"模型返回意外的 finish_reason: {finish_reason}\")\n",
       "                    return f\"处理你的查询时遇到意外情况 ({finish_reason})\"\n",
       "\n",
       "            except Exception as e:\n",
       "                logger.error(f\"处理查询时出错: {str(e)}\")\n",
       "                return f\"处理你的查询时发生错误: {str(e)}\"\n",
       "\n",
       "        # 如果循环达到最大次数仍未得到最终回复\n",
       "        logger.warning(\"达到最大工具调用回合数，未能获得最终回复\")\n",
       "        return \"抱歉，处理你的请求时出现问题，未能获得最终结果。\"\n",
       "\n",
       "    async def chat_loop(self) -> None:\n",
       "        \"\"\"运行交互式聊天循环\"\"\"\n",
       "        logger.info(\"开始聊天会话...\")\n",
       "        \n",
       "        while True:\n",
       "            try:\n",
       "                query = input(\"\\n请输入你的问题 (输入 'quit' 或 'exit' 退出): \").strip()\n",
       "                \n",
       "                if query.lower() in ['quit', 'exit']:\n",
       "                    logger.info(\"结束聊天会话...\")\n",
       "                    break\n",
       "                    \n",
       "                response = await self.process_query(query)\n",
       "                print(f\"\\n助手: {response}\")\n",
       "                \n",
       "            except KeyboardInterrupt:\n",
       "                logger.info(\"\\n收到键盘中断，结束会话...\")\n",
       "                break\n",
       "            except Exception as e:\n",
       "                logger.error(f\"聊天循环中出错: {str(e)}\")\n",
       "                print(f\"\\n发生错误: {str(e)}\")\n",
       "                \n",
       "    async def cleanup(self) -> None:\n",
       "        \"\"\"清理客户端资源\"\"\"\n",
       "        if self.server:\n",
       "            await self.server.cleanup()\n",
       "\n",
       "async def main() -> None:\n",
       "    \"\"\"主程序入口\"\"\"\n",
       "    try:\n",
       "        config = Configuration()\n",
       "        client = MCPClient(config)\n",
       "        \n",
       "        await client.initialize()\n",
       "        await client.chat_loop()\n",
       "        \n",
       "    except Exception as e:\n",
       "        logger.error(f\"致命错误: {str(e)}\")\n",
       "        raise\n",
       "        \n",
       "    finally:\n",
       "        if 'client' in locals():\n",
       "            await client.cleanup()\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    try:\n",
       "        asyncio.run(main())\n",
       "    except KeyboardInterrupt:\n",
       "        logger.info(\"程序被用户终止\")\n",
       "    except Exception as e:\n",
       "        logger.error(f\"程序因错误终止: {str(e)}\")\n",
       "        raise\n",
       "\n",
       "```\n",
       "\n",
       "### 步骤 8 · 助手决策（工具调用）\n",
       "\n",
       "- 工具：`read_project_file`\n",
       "\n",
       "- 参数：\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"name\": \"mcp-demo/client/mcp_client_langchain_chat.py\"\n",
       "}\n",
       "```\n",
       "\n",
       "### 步骤 9 · 环境执行结果\n",
       "\n",
       "执行成功（2025-11-09T00:16:18+0800）\n",
       "\n",
       "```text\n",
       "\"\"\"\n",
       "# 集成 DeepSeek 的 MCP 客户端[LangChain版]\n",
       "#更详细的使用方法请参考：https://github.com/langchain-ai/langchain-mcp-adapters\n",
       "\n",
       "# 所需环境变量：\n",
       "# DEEPSEEK_API_KEY：DeepSeek API 密钥 (格式：sk-xxxx...)\n",
       "# DEEPSEEK_BASE_URL：DeepSeek API 基础 URL (https://api.deepseek.com)\n",
       "# DEEPSEEK_MODEL：DeepSeek 模型名称 (例如 deepseek-chat)\n",
       "\n",
       "Author: FlyAIBox\n",
       "Date: 2025.10.11\n",
       "\"\"\"\n",
       "import asyncio\n",
       "import logging\n",
       "import os\n",
       "from contextlib import AsyncExitStack\n",
       "from typing import List, Optional\n",
       "\n",
       "from dotenv import load_dotenv\n",
       "from langchain_core.messages import SystemMessage\n",
       "from langchain_core.tools import BaseTool\n",
       "from langchain_mcp_adapters.tools import load_mcp_tools\n",
       "from langchain_openai import ChatOpenAI\n",
       "from langgraph.prebuilt import create_react_agent\n",
       "from mcp import ClientSession, StdioServerParameters\n",
       "from mcp.client.stdio import stdio_client\n",
       "\n",
       "# 配置日志\n",
       "logging.basicConfig(\n",
       "    level=logging.INFO,\n",
       "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
       ")\n",
       "logger = logging.getLogger(__name__)\n",
       "\n",
       "\n",
       "# 优先从环境变量加载API密钥等信息\n",
       "load_dotenv()\n",
       "\n",
       "class Configuration:\n",
       "    \"\"\"配置管理类，负责管理和验证环境变量\"\"\"\n",
       "    \n",
       "    def __init__(self) -> None:\n",
       "        \"\"\"初始化配置并加载环境变量\"\"\"\n",
       "        self.load_env()\n",
       "        self._validate_env()\n",
       "        \n",
       "    @staticmethod\n",
       "    def load_env() -> None:\n",
       "        \"\"\"从.env文件加载环境变量\"\"\"\n",
       "        load_dotenv()\n",
       "        \n",
       "    def _validate_env(self) -> None:\n",
       "        \"\"\"验证必需的环境变量是否存在\"\"\"\n",
       "        required_vars = [\"DEEPSEEK_API_KEY\"]\n",
       "        missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
       "        if missing_vars:\n",
       "            raise ValueError(f\"缺少必需的环境变量: {', '.join(missing_vars)}\")\n",
       "    \n",
       "    @property\n",
       "    def api_key(self) -> str:\n",
       "        \"\"\"获取 DeepSeek API 密钥\"\"\"\n",
       "        return os.getenv(\"DEEPSEEK_API_KEY\", \"\")\n",
       "    \n",
       "    @property\n",
       "    def base_url(self) -> str:\n",
       "        \"\"\"获取 DeepSeek API 基础 URL\"\"\"\n",
       "        return os.getenv(\"DEEPSEEK_BASE_URL\", \"https://api.deepseek.com\")\n",
       "    \n",
       "    @property\n",
       "    def model(self) -> str:\n",
       "        \"\"\"获取 DeepSeek 模型名称\"\"\"\n",
       "        return os.getenv(\"DEEPSEEK_MODEL\", \"deepseek-chat\")\n",
       "\n",
       "class MCPServer:\n",
       "    \"\"\"MCP 服务器管理类，处理服务器连接和工具执行\"\"\"\n",
       "    def __init__(self, server_path: str) -> None:\n",
       "        \"\"\"\n",
       "        初始化服务器管理器\n",
       "        \n",
       "        Args:\n",
       "            server_path: 服务器脚本路径\n",
       "        \"\"\"\n",
       "        self.server_path = server_path\n",
       "        self.session: Optional[ClientSession] = None\n",
       "        self.exit_stack = AsyncExitStack()\n",
       "        self._cleanup_lock = asyncio.Lock()\n",
       "\n",
       "\n",
       "    async def initialize(self) -> None:\n",
       "        \"\"\"初始化服务器连接，包含重试机制\"\"\"\n",
       "        max_retries = 3\n",
       "        retry_delay = 1.0\n",
       "        \n",
       "        for attempt in range(max_retries):\n",
       "            try:\n",
       "                if not os.path.exists(self.server_path):\n",
       "                    raise FileNotFoundError(f\"找不到服务器文件: {self.server_path}\")\n",
       "                \n",
       "                server_params = StdioServerParameters(\n",
       "                    command='python',\n",
       "                    args=[self.server_path],\n",
       "                    env=None\n",
       "                )\n",
       "                \n",
       "                stdio_transport = await self.exit_stack.enter_async_context(\n",
       "                    stdio_client(server_params)\n",
       "                )\n",
       "                stdio, write = stdio_transport\n",
       "                \n",
       "                self.session = await self.exit_stack.enter_async_context(\n",
       "                    ClientSession(stdio, write)\n",
       "                )\n",
       "                await self.session.initialize()\n",
       "                logger.info(\"成功连接到 MCP 服务器\")\n",
       "                break\n",
       "                \n",
       "            except Exception as e:\n",
       "                logger.error(f\"第 {attempt + 1}/{max_retries} 次尝试失败: {str(e)}\")\n",
       "                if attempt < max_retries - 1:\n",
       "                    await asyncio.sleep(retry_delay)\n",
       "                else:\n",
       "                    raise\n",
       "\n",
       "    async def list_tools(self) -> List[BaseTool]:\n",
       "        \"\"\"获取服务器提供的可用工具列表\"\"\"\n",
       "        if not self.session:\n",
       "            raise RuntimeError(\"服务器未初始化\")\n",
       "        # LangChain方式获取可用工具列表\n",
       "\n",
       "        # 自动化的工具处理：\n",
       "        # LangChain版本使用load_mcp_tools函数\n",
       "        # 自动适配MCP工具，省去了工具格式转换的工作\n",
       "        tools = await load_mcp_tools(self.session)\n",
       "        logger.info(f\"成功加载工具: {[tool.name for tool in tools]}\")\n",
       "        return tools\n",
       "\n",
       "\n",
       "    async def cleanup(self) -> None:\n",
       "        \"\"\"清理服务器资源\"\"\"\n",
       "        async with self._cleanup_lock:\n",
       "            try:\n",
       "                await self.exit_stack.aclose()\n",
       "                self.session = None\n",
       "                logger.info(\"服务器资源清理完成\")\n",
       "            except Exception as e:\n",
       "                logger.error(f\"清理过程中出错: {str(e)}\")\n",
       "    \n",
       "\n",
       "class MCPClient:\n",
       "    \"\"\"MCP 客户端实现，集成了 DeepSeek API\"\"\"\n",
       "    \n",
       "    def __init__(self, config: Configuration) -> None:\n",
       "        \"\"\"\n",
       "        初始化 MCP 客户端\n",
       "        \n",
       "        Args:\n",
       "            config: 配置对象\n",
       "        \"\"\"\n",
       "        self.config = config\n",
       "        self.server: Optional[MCPServer] = None\n",
       "        self.llm_client = ChatOpenAI(\n",
       "            api_key=config.api_key,\n",
       "            base_url=config.base_url,\n",
       "            model=config.model\n",
       "        )\n",
       "        \n",
       "    async def initialize(self) -> None:\n",
       "        \"\"\"初始化客户端并连接到服务器\"\"\"\n",
       "        server_path = os.path.join(\n",
       "            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\n",
       "            \"server\",\n",
       "            \"weather_server.py\"\n",
       "        )\n",
       "        self.server = MCPServer(server_path)\n",
       "        await self.server.initialize()\n",
       "        \n",
       "\n",
       "    async def process_query(self, query: str):\n",
       "        \"\"\"\n",
       "        处理用户查询，集成工具调用，支持多轮工具交互\n",
       "\n",
       "        Args:\n",
       "            query: 用户查询字符串\n",
       "\n",
       "        Returns:\n",
       "            处理后的响应结果\n",
       "        \"\"\"\n",
       "        if not self.server:\n",
       "            raise RuntimeError(\"客户端未初始化\")\n",
       "\n",
       "        # 创建提示模板\n",
       "        prompt = SystemMessage(content=\"\"\"\"You are a helpful assistant specializing in weather information.\\n\n",
       "                            You have access to the MCP Weather Server tool with the following functions:\\n\n",
       "                            - get_weather_warning(city_id=None, latitude=None, longitude=None): Retrieves weather disaster warnings for a specified city ID or coordinates.\\n\n",
       "                            - get_daily_forecast(city_id=None, latitude=None, longitude=None): Retrieves the multi-day weather forecast for a specified city ID or coordinates.\\n\n",
       "                            \\n\n",
       "                            Core Instructions:\\n\n",
       "                            1.  **Carefully analyze the user's request**: Understand all components of the user's query. Determine if the user needs weather warning information, weather forecast information, or both.\\n\n",
       "                            2.  **Identify Information Needs**:\\n\n",
       "                                * If the user only asks for warnings (e.g., \\Are there any warnings in Beijing?\\), only use `get_weather_warning`.\\n\n",
       "                                * If the user only asks for the forecast (e.g., \\What's the weather like in Beijing tomorrow?\\), only use `get_daily_forecast`.\\n\n",
       "                                * **If the user's question includes multiple aspects**, such as asking about **warning status** and also asking **if it's suitable for a certain activity** (which implies a query about future weather, like \\Have there been high temperature warnings in Beijing in the last week? Is it suitable for outdoor activities?\\), you need to **call both tools sequentially**.\\n\n",
       "                            3.  **Call Tools as Needed**:\\n\n",
       "                                * **Prioritize getting warning information**: If warning information is needed, first call `get_weather_warning`.\\n\n",
       "                                * **Get the weather forecast**: If the user mentions a specific time period (e.g., \\weekend\\, \\next three days\\, \\next week\\) or asks about activity suitability (which typically concerns the next few days), call `get_daily_forecast` to get the forecast for the corresponding period. For vague phrases like \\last week\\ or \\recently\\, interpret it as asking about *current* conditions and the *upcoming* few days (covered by the forecast). For questions like \\Is it suitable for outdoor activities?\\, you should get the forecast for at least the next 2-3 days (e.g., today, tomorrow, the day after tomorrow, or the upcoming weekend) to support your judgment.\\n\n",
       "                                * **Ensure tool call order**: When multiple tools need to be called, they should be called in a logical sequence. For example, first get the warning, then get the forecast. Wait for one tool to finish executing before deciding whether to call the next tool or generate a response.\\n\n",
       "                            4.  **Information Integration and Response**:\\n\n",
       "                                * After obtaining all necessary information (warning, forecast), you **must synthesize and analyze this information**.\\n\n",
       "                                * **Completely answer the user's question**: Ensure you answer all parts of the user's query.\\n\n",
       "                                * **Provide advice**: If the user asks about activity suitability, based on the retrieved warning status and forecast information (temperature, weather condition - clear/rainy, wind strength, etc.), provide a clear, data-supported recommendation (e.g., \\Currently there are no high temperature warnings, but it's expected to rain this weekend, so it's not very suitable for outdoor activities,\\ or \\It will be sunny for the next few days with no warnings, suitable for outdoor activities.\\).\\n\n",
       "                            5.  **Tool Usage Details**:\\n\n",
       "                                * When using the tools, retain the full context of the user's original question.\\n\n",
       "                                * Unless explicitly requested by the user, do not insert specific times of day (e.g., \\3 PM\\) into the search query or your response.\\n\n",
       "                                * When city information is needed, if the user provides a city name (e.g., \\Beijing\\), use the corresponding `city_id` (e.g., Beijing's city_id might be '101010100').\\n                      \n",
       "                            \"\"\")\n",
       "        \n",
       "        \n",
       "        ## 列举工具\n",
       "        tools = await self.server.list_tools()\n",
       "\n",
       "        ## 更简洁的代理创建流程：\n",
       "        # LangChain版本使用create_react_agent函数\n",
       "        # 直接创建代理，并指定模型、工具和提示\n",
       "        # 简化了代码复杂度\n",
       "        \n",
       "        # LangChain方式创建和运行agent\n",
       "        logger.info(\"正在创建agent...\")\n",
       "        agent = create_react_agent(\n",
       "            model=self.llm_client,  \n",
       "            tools=tools,\n",
       "            prompt=prompt\n",
       "        )\n",
       "        logger.info(\"Agent创建成功\")\n",
       "\n",
       "        # 发送查询\n",
       "        logger.info(\"正在发送天气查询...\")\n",
       "        agent_response = await agent.ainvoke({\n",
       "            \"messages\": query\n",
       "        })\n",
       "\n",
       "        # 打印响应\n",
       "        logger.info(\"\\nAgent Response:\")\n",
       "        print(agent_response)\n",
       "\n",
       "        # 遍历消息列表并打印 ToolMessage 的 content\n",
       "        if 'messages' in agent_response:\n",
       "            print(\"\\n--- Tool Message Contents ---\")\n",
       "            for message in agent_response['messages']:\n",
       "                print(f\"\\nTool: {message.name}\") # 可以选择打印工具名称\n",
       "                print(f\"Content:\\n{message.content}\")\n",
       "                print(\"-\" * 20) # 分隔不同 ToolMessage 的内容\n",
       "             \n",
       "\n",
       "    async def chat_loop(self) -> None:\n",
       "        \"\"\"运行交互式聊天循环\"\"\"\n",
       "        logger.info(\"开始聊天会话...\")\n",
       "        \n",
       "        while True:\n",
       "            try:\n",
       "                query = input(\"\\n请输入你的问题 (输入 'quit' 或 'exit' 退出): \").strip()\n",
       "                \n",
       "                if query.lower() in ['quit', 'exit']:\n",
       "                    logger.info(\"结束聊天会话...\")\n",
       "                    break\n",
       "                    \n",
       "                await self.process_query(query)\n",
       "                \n",
       "                \n",
       "            except KeyboardInterrupt:\n",
       "                logger.info(\"\\n收到键盘中断，结束会话...\")\n",
       "                break\n",
       "            except Exception as e:\n",
       "                logger.error(f\"聊天循环中出错: {str(e)}\")\n",
       "                print(f\"\\n发生错误: {str(e)}\")\n",
       "                \n",
       "    async def cleanup(self) -> None:\n",
       "        \"\"\"清理客户端资源\"\"\"\n",
       "        if self.server:\n",
       "            await self.server.cleanup()\n",
       "\n",
       "async def main() -> None:\n",
       "    \"\"\"主程序入口\"\"\"\n",
       "    try:\n",
       "        config = Configuration()\n",
       "        client = MCPClient(config)\n",
       "        \n",
       "        await client.initialize()\n",
       "        await client.chat_loop()\n",
       "        \n",
       "    except Exception as e:\n",
       "        logger.error(f\"致命错误: {str(e)}\")\n",
       "        raise\n",
       "        \n",
       "    finally:\n",
       "        if 'client' in locals():\n",
       "            await client.cleanup()\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    try:\n",
       "        asyncio.run(main())\n",
       "    except KeyboardInterrupt:\n",
       "        logger.info(\"程序被用户终止\")\n",
       "    except Exception as e:\n",
       "        logger.error(f\"程序因错误终止: {str(e)}\")\n",
       "        raise\n",
       "\n",
       "\n",
       "```\n",
       "\n",
       "### 步骤 10 · 助手决策（工具调用）\n",
       "\n",
       "- 工具：`read_project_file`\n",
       "\n",
       "- 参数：\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"name\": \"mcp-demo/server/weather_server.py\"\n",
       "}\n",
       "```\n",
       "\n",
       "### 步骤 11 · 环境执行结果\n",
       "\n",
       "执行成功（2025-11-09T00:16:19+0800）\n",
       "\n",
       "```text\n",
       "#!/usr/bin/env python3\n",
       "# -*- coding: utf-8 -*-\n",
       "\n",
       "\"\"\"\n",
       "MCP 服务器可以提供三种主要类型的功能：\n",
       "\n",
       "资源：客户端可以读取的类似文件的数据（例如 API 响应或文件内容）\n",
       "工具：可由 LLM 调用的函数（经用户批准）\n",
       "提示：预先编写的模板，帮助用户完成特定任务\n",
       "\n",
       "######################################\n",
       "\n",
       "MCP 天气服务器\n",
       "\n",
       "提供两个工具：\n",
       "1. get_weather_warning: 获取指定城市ID或经纬度的天气灾害预警\n",
       "2. get_daily_forecast: 获取指定城市ID或经纬度的天气预报\n",
       "\n",
       "Author: FlyAIBox\n",
       "Date: 2025.10.11\n",
       "\"\"\"\n",
       "\n",
       "from typing import Any, Dict, List, Optional, Union\n",
       "import asyncio\n",
       "import httpx\n",
       "import os\n",
       "from urllib.parse import urljoin\n",
       "from mcp.server.fastmcp import FastMCP\n",
       "from dotenv import load_dotenv\n",
       "from pathlib import Path\n",
       "\n",
       "# 加载 .env 文件中的环境变量\n",
       "dotenv_path = Path(__file__).resolve().parents[1] / '.env'\n",
       "load_dotenv(dotenv_path)\n",
       "\n",
       "# 初始化 FastMCP 服务器\n",
       "mcp = FastMCP(\"weather\",  # 服务器名称\n",
       "              debug=True,  # 启用调试模式，会输出详细日志\n",
       "              host=\"0.0.0.0\") # 监听所有网络接口，允许远程连接\n",
       "\n",
       "# 从环境变量中读取常量\n",
       "QWEATHER_API_BASE = os.getenv(\"QWEATHER_API_BASE\")\n",
       "QWEATHER_API_KEY = os.getenv(\"QWEATHER_API_KEY\")\n",
       "\n",
       "def _normalize_base_url(raw_base: Optional[str]) -> str:\n",
       "    \"\"\"\n",
       "    确保基础 URL 包含协议并以单个斜杠结尾，兼容 .env 中未写协议的情况\n",
       "    \"\"\"\n",
       "    if not raw_base:\n",
       "        raise RuntimeError(\"未配置 QWEATHER_API_BASE 环境变量\")\n",
       "\n",
       "    base = raw_base.strip()\n",
       "    if not base.startswith((\"http://\", \"https://\")):\n",
       "        base = f\"https://{base.lstrip('/')}\"\n",
       "\n",
       "    # urljoin 要求目录风格以斜杠结尾，避免 'v7/weather/7d' 被覆盖\n",
       "    if not base.endswith(\"/\"):\n",
       "        base = f\"{base}/\"\n",
       "\n",
       "    return base\n",
       "\n",
       "try:\n",
       "    _QWEATHER_BASE_URL = _normalize_base_url(QWEATHER_API_BASE)\n",
       "except RuntimeError as err:\n",
       "    print(f\"[配置错误] {err}\")\n",
       "    _QWEATHER_BASE_URL = None\n",
       "\n",
       "async def make_qweather_request(endpoint: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
       "    \"\"\"\n",
       "    向和风天气 API 发送请求\n",
       "    \n",
       "    参数:\n",
       "        endpoint: API 端点路径（不包含基础 URL）\n",
       "        params: API 请求的参数\n",
       "        \n",
       "    返回:\n",
       "        成功时返回 JSON 响应，失败时返回 None\n",
       "    \"\"\"\n",
       "    if not _QWEATHER_BASE_URL:\n",
       "        print(\"QWEATHER_API_BASE 未正确配置，已跳过请求。\")\n",
       "        return None\n",
       "\n",
       "    if not QWEATHER_API_KEY:\n",
       "        print(\"QWEATHER_API_KEY 未设置，已跳过请求。\")\n",
       "        return None\n",
       "\n",
       "    safe_endpoint = endpoint.lstrip(\"/\")\n",
       "    url = urljoin(_QWEATHER_BASE_URL, safe_endpoint)\n",
       "\n",
       "    # 使用 Header 方式认证（和风天气的新版本API）\n",
       "    headers = {\n",
       "        \"X-QW-Api-Key\": QWEATHER_API_KEY\n",
       "    }\n",
       "    \n",
       "    async with httpx.AsyncClient() as client:\n",
       "        try:\n",
       "            print(f\"请求 URL: {url}\")\n",
       "            print(f\"请求参数: {params}\")\n",
       "            response = await client.get(url, params=params, headers=headers, timeout=30.0)\n",
       "            print(f\"响应状态码: {response.status_code}\")\n",
       "            response.raise_for_status()\n",
       "            result = response.json()\n",
       "            print(f\"响应内容: {result}\")\n",
       "            return result\n",
       "        except httpx.HTTPStatusError as e:\n",
       "            print(f\"HTTP 状态错误: {e.response.status_code} - {e.response.text}\")\n",
       "            return None\n",
       "        except Exception as e:\n",
       "            print(f\"API 请求错误: {type(e).__name__}: {e}\")\n",
       "            return None\n",
       "\n",
       "def format_warning(warning: Dict[str, Any]) -> str:\n",
       "    \"\"\"\n",
       "    将天气预警数据格式化为可读字符串\n",
       "    \n",
       "    参数:\n",
       "        warning: 天气预警数据对象\n",
       "        \n",
       "    返回:\n",
       "        格式化后的预警信息\n",
       "    \"\"\"\n",
       "    return f\"\"\"\n",
       "预警ID: {warning.get('id', '未知')}\n",
       "标题: {warning.get('title', '未知')}\n",
       "发布时间: {warning.get('pubTime', '未知')}\n",
       "开始时间: {warning.get('startTime', '未知')}\n",
       "结束时间: {warning.get('endTime', '未知')}\n",
       "预警类型: {warning.get('typeName', '未知')}\n",
       "预警等级: {warning.get('severity', '未知')} ({warning.get('severityColor', '未知')})\n",
       "发布单位: {warning.get('sender', '未知')}\n",
       "状态: {warning.get('status', '未知')}\n",
       "详细信息: {warning.get('text', '无详细信息')}\n",
       "\"\"\"\n",
       "\n",
       "@mcp.tool()\n",
       "async def get_weather_warning(location: Union[str, int]) -> str:\n",
       "    \"\"\"\n",
       "    获取指定位置的天气灾害预警\n",
       "    \n",
       "    参数:\n",
       "        location: 城市ID或经纬度坐标（经度,纬度）\n",
       "                例如：'101010100'（北京）或 '116.41,39.92'\n",
       "                也可以直接传入数字ID，如 101010100\n",
       "        \n",
       "    返回:\n",
       "        格式化的预警信息字符串\n",
       "    \"\"\"\n",
       "    # 确保 location 为字符串类型\n",
       "    location = str(location)\n",
       "    \n",
       "    params = {\n",
       "        \"location\": location,\n",
       "        \"lang\": \"zh\"\n",
       "    }\n",
       "    \n",
       "    data = await make_qweather_request(\"v7/warning/now\", params)\n",
       "    \n",
       "    if not data:\n",
       "        return \"无法获取预警信息或API请求失败。\"\n",
       "    \n",
       "    if data.get(\"code\") != \"200\":\n",
       "        return f\"API 返回错误: {data.get('code')}\"\n",
       "    \n",
       "    warnings = data.get(\"warning\", [])\n",
       "    \n",
       "    if not warnings:\n",
       "        return f\"当前位置 {location} 没有活动预警。\"\n",
       "    \n",
       "    formatted_warnings = [format_warning(warning) for warning in warnings]\n",
       "    return \"\\n---\\n\".join(formatted_warnings)\n",
       "\n",
       "def format_daily_forecast(daily: Dict[str, Any]) -> str:\n",
       "    \"\"\"\n",
       "    将天气预报数据格式化为可读字符串\n",
       "    \n",
       "    参数:\n",
       "        daily: 天气预报数据对象\n",
       "        \n",
       "    返回:\n",
       "        格式化后的预报信息\n",
       "    \"\"\"\n",
       "    return f\"\"\"\n",
       "日期: {daily.get('fxDate', '未知')}\n",
       "日出: {daily.get('sunrise', '未知')}  日落: {daily.get('sunset', '未知')}\n",
       "最高温度: {daily.get('tempMax', '未知')}°C  最低温度: {daily.get('tempMin', '未知')}°C\n",
       "白天天气: {daily.get('textDay', '未知')}  夜间天气: {daily.get('textNight', '未知')}\n",
       "白天风向: {daily.get('windDirDay', '未知')} {daily.get('windScaleDay', '未知')}级 ({daily.get('windSpeedDay', '未知')}km/h)\n",
       "夜间风向: {daily.get('windDirNight', '未知')} {daily.get('windScaleNight', '未知')}级 ({daily.get('windSpeedNight', '未知')}km/h)\n",
       "相对湿度: {daily.get('humidity', '未知')}%\n",
       "降水量: {daily.get('precip', '未知')}mm\n",
       "紫外线指数: {daily.get('uvIndex', '未知')}\n",
       "能见度: {daily.get('vis', '未知')}km\n",
       "\"\"\"\n",
       "\n",
       "@mcp.tool()\n",
       "async def get_daily_forecast(location: Union[str, int], days: int = 3) -> str:\n",
       "    \"\"\"\n",
       "    获取指定位置的天气预报\n",
       "    \n",
       "    参数:\n",
       "        location: 城市ID或经纬度坐标（经度,纬度）\n",
       "                例如：'101010100'（北京）或 '116.41,39.92'\n",
       "                也可以直接传入数字ID，如 101010100\n",
       "        days: 预报天数，可选值为 3、7、10、15、30，默认为 3\n",
       "        \n",
       "    返回:\n",
       "        格式化的天气预报字符串\n",
       "    \"\"\"\n",
       "    # 确保 location 为字符串类型\n",
       "    location = str(location)\n",
       "    \n",
       "    # 确保 days 参数有效\n",
       "    valid_days = [3, 7, 10, 15, 30]\n",
       "    if days not in valid_days:\n",
       "        days = 3  # 默认使用3天预报\n",
       "    \n",
       "    params = {\n",
       "        \"location\": location,\n",
       "        \"lang\": \"zh\"\n",
       "    }\n",
       "    # 和风天气API文档 https://dev.qweather.com/docs/api/weather/weather-daily-forecast/\n",
       "    endpoint = f\"v7/weather/{days}d\"\n",
       "    data = await make_qweather_request(endpoint, params)\n",
       "    \n",
       "    if not data:\n",
       "        return \"无法获取天气预报或API请求失败。\"\n",
       "    \n",
       "    if data.get(\"code\") != \"200\":\n",
       "        return f\"API 返回错误: {data.get('code')}\"\n",
       "    \n",
       "    daily_forecasts = data.get(\"daily\", [])\n",
       "    \n",
       "    if not daily_forecasts:\n",
       "        return f\"无法获取 {location} 的天气预报数据。\"\n",
       "    \n",
       "    formatted_forecasts = [format_daily_forecast(daily) for daily in daily_forecasts]\n",
       "    return \"\\n---\\n\".join(formatted_forecasts)\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    print(\"正在启动 MCP 天气服务器...\")\n",
       "    print(\"提供工具: get_weather_warning, get_daily_forecast\")\n",
       "    print(\"请确保环境变量 QWEATHER_API_KEY 已设置\")\n",
       "    print(\"使用 Ctrl+C 停止服务器\")\n",
       "    \n",
       "    # 初始化并运行服务器\n",
       "    mcp.run(transport='stdio') \n",
       "\n",
       "```\n",
       "\n",
       "### 步骤 12 · 助手决策（工具调用）\n",
       "\n",
       "- 工具：`terminate`\n",
       "\n",
       "- 参数：\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"message\": \"# MCP Weather Client and Server Project\\n\\n## Overview:\\n\\nThis project consists of a client-server application designed to interact with the Model Context Protocol (MCP), a protocol that facilitates standardized communication among various AI models and tools. The MCP Weather application provides weather-related functions by utilizing external APIs and integrates these capabilities into a client-server model.\\n\\n## Project Structure:\\n\\n- **Client:** \\n  - The client contains multiple implementations demonstrating different integrations:\\n    - `mcp_client.py`: A simple client application showcasing MCP client-server interactions, calling tools, and handling responses.\\n    - `mcp_client_deepseek.py`: Integration with DeepSeek API in the MCP client, handling user queries and processing them via DeepSeek.\\n    - `mcp_client_langchain_chat.py`: Integration with DeepSeek API using a LangChain version, illustrating streamlined tool handling and agent creation.\\n\\n- **Server:**\\n  - `weather_server.py`: An MCP server providing weather information services. It includes tools for retrieving weather warnings and forecasts from a weather API and formats the responses for client use.\\n\\n## Key Features:\\n\\n- **MCP Client-Server Communication**: Demonstrates how the client communicates with the server using standard input/output, with asynchronous handling for efficiency.\\n- **Tool Integration**: Features tools that gather weather data from external APIs and make it easily accessible through standardized MCP protocols.\\n- **Flexible Implementations**: Shows different ways to integrate with AI functionalities - starting from a basic model to more sophisticated methods using third-party libraries like DeepSeek and LangChain.\\n\\n## Getting Started:\\n\\n### Prerequisites:\\n- Install Python 3.7 or later.\\n- Install necessary packages using pip (e.g., `asyncio`, `httpx`, `openai`, `langchain_openai`, `dotenv`, and others mentioned in client implementations).\\n\\n### Environment Configuration:\\n- Create a `.env` file at the root of the project to store essential environment variables.\\n- **Sample `.env` Configuration**:\\n  ```\\n  DEEPSEEK_API_KEY=your_deepseek_api_key\\n  DEEPSEEK_BASE_URL=https://api.deepseek.com\\n  DEEPSEEK_MODEL=deepseek-chat\\n  QWEATHER_API_BASE=your_qweather_api_base\\n  QWEATHER_API_KEY=your_qweather_api_key\\n  ```\\n\\n### Running the Server:\\n1. Navigate to the server directory (e.g., `mcp-demo/server`).\\n2. Execute `python weather_server.py` to start the server.\\n\\n### Running the Client:\\n1. Choose a client implementation from the `mcp-demo/client` directory.\\n2. Execute the script using Python (e.g., `python mcp_client.py`).\\n3. Follow the command prompts to interact with the MCP server.\\n\\n## Usage:\\n- The client applications prompt for user input commands to perform various operations, including fetching weather warnings and forecasts.\\n- Server responses are processed and displayed to the user through formatted text, providing valuable weather insights.\\n\\n## Future Improvements:\\n\\n- Expand tool capabilities to include more advanced MCP features and integrate additional APIs.\\n- Enhance error handling and logging to provide deeper insights into application behavior.\\n\\n---\\nThis project offers a well-structured demonstration of how MCP can be utilized for tool integration and provides practical examples for weather-based applications using modern AI frameworks and protocols.\"\n",
       "}\n",
       "```\n",
       "\n",
       "### 步骤 13 · 环境执行结果\n",
       "\n",
       "执行成功（2025-11-09T00:16:27+0800）\n",
       "\n",
       "生成内容：\n",
       "\n",
       "```markdown\n",
       "# MCP Weather Client and Server Project\n",
       "\n",
       "## Overview:\n",
       "\n",
       "This project consists of a client-server application designed to interact with the Model Context Protocol (MCP), a protocol that facilitates standardized communication among various AI models and tools. The MCP Weather application provides weather-related functions by utilizing external APIs and integrates these capabilities into a client-server model.\n",
       "\n",
       "## Project Structure:\n",
       "\n",
       "- **Client:** \n",
       "  - The client contains multiple implementations demonstrating different integrations:\n",
       "    - `mcp_client.py`: A simple client application showcasing MCP client-server interactions, calling tools, and handling responses.\n",
       "    - `mcp_client_deepseek.py`: Integration with DeepSeek API in the MCP client, handling user queries and processing them via DeepSeek.\n",
       "    - `mcp_client_langchain_chat.py`: Integration with DeepSeek API using a LangChain version, illustrating streamlined tool handling and agent creation.\n",
       "\n",
       "- **Server:**\n",
       "  - `weather_server.py`: An MCP server providing weather information services. It includes tools for retrieving weather warnings and forecasts from a weather API and formats the responses for client use.\n",
       "\n",
       "## Key Features:\n",
       "\n",
       "- **MCP Client-Server Communication**: Demonstrates how the client communicates with the server using standard input/output, with asynchronous handling for efficiency.\n",
       "- **Tool Integration**: Features tools that gather weather data from external APIs and make it easily accessible through standardized MCP protocols.\n",
       "- **Flexible Implementations**: Shows different ways to integrate with AI functionalities - starting from a basic model to more sophisticated methods using third-party libraries like DeepSeek and LangChain.\n",
       "\n",
       "## Getting Started:\n",
       "\n",
       "### Prerequisites:\n",
       "- Install Python 3.7 or later.\n",
       "- Install necessary packages using pip (e.g., `asyncio`, `httpx`, `openai`, `langchain_openai`, `dotenv`, and others mentioned in client implementations).\n",
       "\n",
       "### Environment Configuration:\n",
       "- Create a `.env` file at the root of the project to store essential environment variables.\n",
       "- **Sample `.env` Configuration**:\n",
       "  ```\n",
       "  DEEPSEEK_API_KEY=your_deepseek_api_key\n",
       "  DEEPSEEK_BASE_URL=https://api.deepseek.com\n",
       "  DEEPSEEK_MODEL=deepseek-chat\n",
       "  QWEATHER_API_BASE=your_qweather_api_base\n",
       "  QWEATHER_API_KEY=your_qweather_api_key\n",
       "  ```\n",
       "\n",
       "### Running the Server:\n",
       "1. Navigate to the server directory (e.g., `mcp-demo/server`).\n",
       "2. Execute `python weather_server.py` to start the server.\n",
       "\n",
       "### Running the Client:\n",
       "1. Choose a client implementation from the `mcp-demo/client` directory.\n",
       "2. Execute the script using Python (e.g., `python mcp_client.py`).\n",
       "3. Follow the command prompts to interact with the MCP server.\n",
       "\n",
       "## Usage:\n",
       "- The client applications prompt for user input commands to perform various operations, including fetching weather warnings and forecasts.\n",
       "- Server responses are processed and displayed to the user through formatted text, providing valuable weather insights.\n",
       "\n",
       "## Future Improvements:\n",
       "\n",
       "- Expand tool capabilities to include more advanced MCP features and integrate additional APIs.\n",
       "- Enhance error handling and logging to provide deeper insights into application behavior.\n",
       "\n",
       "---\n",
       "This project offers a well-structured demonstration of how MCP can be utilized for tool integration and provides practical examples for weather-based applications using modern AI frameworks and protocols.\n",
       "Terminating...\n",
       "```\n",
       "\n",
       "\n",
       "## 生成的 README（若已终止并返回）\n",
       "\n",
       "\n",
       "```markdown\n",
       "# MCP Weather Client and Server Project\n",
       "\n",
       "## Overview:\n",
       "\n",
       "This project consists of a client-server application designed to interact with the Model Context Protocol (MCP), a protocol that facilitates standardized communication among various AI models and tools. The MCP Weather application provides weather-related functions by utilizing external APIs and integrates these capabilities into a client-server model.\n",
       "\n",
       "## Project Structure:\n",
       "\n",
       "- **Client:** \n",
       "  - The client contains multiple implementations demonstrating different integrations:\n",
       "    - `mcp_client.py`: A simple client application showcasing MCP client-server interactions, calling tools, and handling responses.\n",
       "    - `mcp_client_deepseek.py`: Integration with DeepSeek API in the MCP client, handling user queries and processing them via DeepSeek.\n",
       "    - `mcp_client_langchain_chat.py`: Integration with DeepSeek API using a LangChain version, illustrating streamlined tool handling and agent creation.\n",
       "\n",
       "- **Server:**\n",
       "  - `weather_server.py`: An MCP server providing weather information services. It includes tools for retrieving weather warnings and forecasts from a weather API and formats the responses for client use.\n",
       "\n",
       "## Key Features:\n",
       "\n",
       "- **MCP Client-Server Communication**: Demonstrates how the client communicates with the server using standard input/output, with asynchronous handling for efficiency.\n",
       "- **Tool Integration**: Features tools that gather weather data from external APIs and make it easily accessible through standardized MCP protocols.\n",
       "- **Flexible Implementations**: Shows different ways to integrate with AI functionalities - starting from a basic model to more sophisticated methods using third-party libraries like DeepSeek and LangChain.\n",
       "\n",
       "## Getting Started:\n",
       "\n",
       "### Prerequisites:\n",
       "- Install Python 3.7 or later.\n",
       "- Install necessary packages using pip (e.g., `asyncio`, `httpx`, `openai`, `langchain_openai`, `dotenv`, and others mentioned in client implementations).\n",
       "\n",
       "### Environment Configuration:\n",
       "- Create a `.env` file at the root of the project to store essential environment variables.\n",
       "- **Sample `.env` Configuration**:\n",
       "  ```\n",
       "  DEEPSEEK_API_KEY=your_deepseek_api_key\n",
       "  DEEPSEEK_BASE_URL=https://api.deepseek.com\n",
       "  DEEPSEEK_MODEL=deepseek-chat\n",
       "  QWEATHER_API_BASE=your_qweather_api_base\n",
       "  QWEATHER_API_KEY=your_qweather_api_key\n",
       "  ```\n",
       "\n",
       "### Running the Server:\n",
       "1. Navigate to the server directory (e.g., `mcp-demo/server`).\n",
       "2. Execute `python weather_server.py` to start the server.\n",
       "\n",
       "### Running the Client:\n",
       "1. Choose a client implementation from the `mcp-demo/client` directory.\n",
       "2. Execute the script using Python (e.g., `python mcp_client.py`).\n",
       "3. Follow the command prompts to interact with the MCP server.\n",
       "\n",
       "## Usage:\n",
       "- The client applications prompt for user input commands to perform various operations, including fetching weather warnings and forecasts.\n",
       "- Server responses are processed and displayed to the user through formatted text, providing valuable weather insights.\n",
       "\n",
       "## Future Improvements:\n",
       "\n",
       "- Expand tool capabilities to include more advanced MCP features and integrate additional APIs.\n",
       "- Enhance error handling and logging to provide deeper insights into application behavior.\n",
       "\n",
       "---\n",
       "This project offers a well-structured demonstration of how MCP can be utilized for tool integration and provides practical examples for weather-based applications using modern AI frameworks and protocols.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 将智能体运行结果以 Markdown 形式美化展示\n",
    "from IPython.display import display, Markdown\n",
    "import json\n",
    "\n",
    "def _format_env_result(env_json_str: str) -> str:\n",
    "    try:\n",
    "        obj = json.loads(env_json_str)\n",
    "    except Exception:\n",
    "        return env_json_str\n",
    "    if isinstance(obj, dict) and obj.get(\"tool_executed\") is True:\n",
    "        result = obj.get(\"result\", \"\")\n",
    "        ts = obj.get(\"timestamp\", \"\")\n",
    "        if isinstance(result, list):\n",
    "            body = \"\\n\".join([f\"- {x}\" for x in result])\n",
    "            return f\"执行成功（{ts}）\\n\\n可用文件列表：\\n{body}\"\n",
    "        if isinstance(result, str):\n",
    "            if result.strip().startswith(\"# \") or \"\\n## \" in result:\n",
    "                return f\"执行成功（{ts}）\\n\\n生成内容：\\n\\n```markdown\\n{result}\\n```\"\n",
    "            return f\"执行成功（{ts}）\\n\\n```text\\n{result}\\n```\"\n",
    "        return f\"执行成功（{ts}）\\n\\n```json\\n{json.dumps(result, ensure_ascii=False, indent=2)}\\n```\"\n",
    "    if isinstance(obj, dict) and obj.get(\"tool_executed\") is False:\n",
    "        err = obj.get(\"error\", \"\")\n",
    "        tb = obj.get(\"traceback\", \"\")\n",
    "        return f\"执行失败\\n\\n错误：`{err}`\\n\\n<details><summary>Traceback</summary>\\n\\n```text\\n{tb}\\n```\\n\\n</details>\"\n",
    "    return f\"```json\\n{json.dumps(obj, ensure_ascii=False, indent=2)}\\n```\"\n",
    "\n",
    "md_lines = [\n",
    "    \"# 智能体执行报告\",\n",
    "]\n",
    "\n",
    "if 'final_memory' not in globals():\n",
    "    display(Markdown(\"> 未检测到 final_memory 变量，请先运行上方示例执行智能体。\"))\n",
    "else:\n",
    "    memories = final_memory.get_memories()\n",
    "\n",
    "    # 概览\n",
    "    md_lines.append(\"## 概览\")\n",
    "    md_lines.append(f\"- 总事件数：{len(memories)}\")\n",
    "\n",
    "    # 逐步展示\n",
    "    md_lines.append(\"\\n## 交互明细\\n\")\n",
    "    for idx, item in enumerate(memories, 1):\n",
    "        typ = item.get(\"type\", \"unknown\")\n",
    "        content = item.get(\"content\", \"\")\n",
    "        if typ == \"user\":\n",
    "            md_lines.append(f\"### 步骤 {idx} · 用户输入\")\n",
    "            md_lines.append(f\"> {content}\")\n",
    "        elif typ == \"assistant\":\n",
    "            md_lines.append(f\"### 步骤 {idx} · 助手决策（工具调用）\")\n",
    "            try:\n",
    "                call = json.loads(content)\n",
    "                tool = call.get(\"tool\", \"?\")\n",
    "                args = call.get(\"args\", {})\n",
    "                md_lines.append(f\"- 工具：`{tool}`\")\n",
    "                md_lines.append(\"- 参数：\")\n",
    "                md_lines.append(f\"```json\\n{json.dumps(args, ensure_ascii=False, indent=2)}\\n```\")\n",
    "            except Exception:\n",
    "                md_lines.append(\"- 文本回复：\")\n",
    "                md_lines.append(f\"```text\\n{content}\\n```\")\n",
    "        elif typ == \"environment\":\n",
    "            md_lines.append(f\"### 步骤 {idx} · 环境执行结果\")\n",
    "            md_lines.append(_format_env_result(content))\n",
    "        else:\n",
    "            md_lines.append(f\"### 步骤 {idx} · 其他\")\n",
    "            md_lines.append(f\"```text\\n{content}\\n```\")\n",
    "\n",
    "    # 摘取 README 内容（若存在 terminate 消息）\n",
    "    md_lines.append(\"\\n## 生成的 README（若已终止并返回）\\n\")\n",
    "    readme_blocks = []\n",
    "    for item in memories[::-1]:\n",
    "        if item.get(\"type\") == \"environment\":\n",
    "            try:\n",
    "                obj = json.loads(item[\"content\"]) if isinstance(item.get(\"content\"), str) else item[\"content\"]\n",
    "                if obj.get(\"tool_executed\") and isinstance(obj.get(\"result\"), str) and obj[\"result\"].lstrip().startswith(\"# \"):\n",
    "                    readme_blocks.append(obj[\"result\"].replace(\"\\nTerminating...\", \"\").strip())\n",
    "                    break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if readme_blocks:\n",
    "        md_lines.append(\"```markdown\\n\" + readme_blocks[0] + \"\\n```\")\n",
    "    else:\n",
    "        md_lines.append(\"> 本次执行未生成 README 内容或未调用终止工具。\")\n",
    "\n",
    "    display(Markdown(\"\\n\\n\".join(md_lines)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 执行流程回顾\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    Start([用户输入任务]) --> Init[初始化Memory]\n",
    "    Init --> AddTask[将用户任务添加到Memory]\n",
    "    \n",
    "    AddTask --> LoopStart{开始循环<br/>max_iterations=50}\n",
    "    \n",
    "    LoopStart --> Step1[步骤1: 构造Prompt]\n",
    "    \n",
    "    Step1 --> Step1a[格式化Goals为system消息]\n",
    "    Step1 --> Step1b[格式化Memory为对话历史]\n",
    "    Step1 --> Step1c[格式化Actions为tools Schema]\n",
    "    \n",
    "    Step1a --> Step2\n",
    "    Step1b --> Step2\n",
    "    Step1c --> Step2\n",
    "    \n",
    "    Step2[步骤2: 调用LLM] --> Step2a[发送Prompt到OpenAI API]\n",
    "    Step2a --> Step2b{是否有tools?}\n",
    "    \n",
    "    Step2b -->|有| Step2c[启用函数调用模式]\n",
    "    Step2b -->|无| Step2d[普通对话模式]\n",
    "    \n",
    "    Step2c --> Step3\n",
    "    Step2d --> Step3\n",
    "    \n",
    "    Step3[步骤3: 获取响应] --> Step3a[LLM返回tool_calls或文本]\n",
    "    Step3a --> Step4[步骤4: 解析响应]\n",
    "    \n",
    "    Step4 --> Step4a[解析JSON获取tool和args]\n",
    "    Step4a --> Step4b{解析成功?}\n",
    "    \n",
    "    Step4b -->|成功| Step5\n",
    "    Step4b -->|失败| Step4c[降级为terminate工具]\n",
    "    Step4c --> Step5\n",
    "    \n",
    "    Step5[步骤5: 获取Action对象] --> Step5a[从ActionRegistry查找]\n",
    "    Step5a --> Step6[步骤6: 执行动作]\n",
    "    \n",
    "    Step6 --> Step6a[在Environment中执行]\n",
    "    Step6a --> Step6b{执行成功?}\n",
    "    \n",
    "    Step6b -->|成功| Step6c[格式化结果+时间戳]\n",
    "    Step6b -->|失败| Step6d[捕获异常+traceback]\n",
    "    \n",
    "    Step6c --> Step7\n",
    "    Step6d --> Step7\n",
    "    \n",
    "    Step7[步骤7: 更新Memory] --> Step7a[添加assistant消息<br/>包含决策内容]\n",
    "    Step7a --> Step7b[添加environment消息<br/>包含执行结果]\n",
    "    \n",
    "    Step7b --> Step8{步骤8: 判断是否终止}\n",
    "    \n",
    "    Step8 -->|action.terminal=True| End([结束循环])\n",
    "    Step8 -->|继续| IterCheck{迭代次数<br/>是否达到上限?}\n",
    "    \n",
    "    IterCheck -->|否| LoopStart\n",
    "    IterCheck -->|是| End\n",
    "    \n",
    "    End --> Return[返回final_memory]\n",
    "    \n",
    "    style Start fill:#e1f5e1\n",
    "    style End fill:#ffe1e1\n",
    "    style Return fill:#e1e5ff\n",
    "    style Step2 fill:#fff4e1\n",
    "    style Step6 fill:#fff4e1\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (agent101)",
   "language": "python",
   "name": "agent101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
