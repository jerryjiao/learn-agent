{
      "cells": [
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# SGLang\n",
                        "\n",
                        "[SGLang](https://github.com/sgl-project/sglang) 是一个用于大型语言模型（LLM）和视觉语言模型（VLM）的高性能服务框架。\n",
                        "\n",
                        "欲了解更多关于 SGLang 的信息，请参阅[官方文档](https://docs.sglang.ai/)。\n",
                        "\n",
                        "**部署环境说明**：\n",
                        "- 平台：Google Colab\n",
                        "- 操作系统：Ubuntu 22.04\n",
                        "- GPU：Tesla T4 (15GB 显存)\n",
                        "- 目标受众：智能体中高级学习者，关注高性能推理部署"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## 环境设置 (Environment Setup)\n",
                        "\n",
                        "默认情况下，您可以在纯净环境中使用 pip 安装 `sglang`："
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "!pip install \"sglang[all]>=0.4.6.post1\""
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "如果您在安装过程中遇到问题，请查阅[官方安装文档](https://docs.sglang.ai/start/install.html)以获取更多帮助。"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## API 服务 (API Service)\n",
                        "\n",
                        "使用 SGLang 构建兼容 OpenAI 的 API 服务非常简单。默认情况下，服务会在 `http://localhost:30000` 启动。您可以通过 `--host` 和 `--port` 参数指定地址。\n",
                        "\n",
                        "由于 Google Colab 的 T4 GPU 显存限制 (约 15GB)，我们将使用 **AWQ 量化版本** 的模型 `Qwen/Qwen3-8B-AWQ`。这能显著降低显存占用，确保模型在 T4 上顺利运行。"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import time\n",
                        "import requests\n",
                        "import os\n",
                        "\n",
                        "# 启动 SGLang 服务器（后台运行）\n",
                        "# --model-path: 指定模型路径，这里使用 Qwen/Qwen3-8B-AWQ 适配 T4 显存\n",
                        "# --port: 指定端口为 30000\n",
                        "# > server.log 2>&1: 将日志重定向到文件，避免阻塞 Notebook\n",
                        "\n",
                        "print(\"正在启动 SGLang 服务器... (预计需要几分钟下载和加载模型)\")\n",
                        "get_ipython().system_raw('nohup python -m sglang.launch_server --model-path Qwen/Qwen3-8B-AWQ --port 30000 --host 127.0.0.1 > server.log 2>&1 &')\n",
                        "\n",
                        "# 等待服务器就绪的辅助函数\n",
                        "def wait_for_server(url, timeout=900): # 设置较长超时时间以等待模型下载\n",
                        "    start_time = time.time()\n",
                        "    print(\"等待服务就绪...\", end=\"\")\n",
                        "    while time.time() - start_time < timeout:\n",
                        "        try:\n",
                        "            response = requests.get(f\"{url}/health\")\n",
                        "            if response.status_code == 200:\n",
                        "                print(\"\\n服务器已启动并就绪！\")\n",
                        "                return True\n",
                        "        except requests.ConnectionError:\n",
                        "            pass\n",
                        "        time.sleep(10)\n",
                        "        print(\".\", end=\"\", flush=True)\n",
                        "    print(\"\\n服务器启动超时。请运行 `!cat server.log` 查看错误日志。\")\n",
                        "    return False\n",
                        "\n",
                        "wait_for_server(\"http://127.0.0.1:30000\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### 基本用法 (Basic Usage)\n",
                        "\n",
                        "服务启动后，您可以使用标准的 OpenAI 聊天接口与模型进行交互。以下是使用 `curl` 的示例："
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "!curl http://localhost:30000/v1/chat/completions \\\n",
                        "  -H \"Content-Type: application/json\" \\\n",
                        "  -d '{\n",
                        "  \"model\": \"Qwen/Qwen3-8B-AWQ\",\n",
                        "  \"messages\": [\n",
                        "    {\"role\": \"user\", \"content\": \"请简要介绍一下大型语言模型。\"}\n",
                        "  ],\n",
                        "  \"temperature\": 0.6,\n",
                        "  \"top_p\": 0.95,\n",
                        "  \"top_k\": 20,\n",
                        "  \"max_tokens\": 1024\n",
                        "}'"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### 思考模式与非思考模式 (Thinking & Non-Thinking Modes)\n",
                        "\n",
                        "Qwen3 模型具备“思考”能力，即在回答前会输出推理过程。这一行为可以通过参数控制。\n",
                        "\n",
                        "您可以通过 API 调用中的 `chat_template_kwargs` 来禁用思考模式（硬开关）："
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "!curl http://localhost:30000/v1/chat/completions \\\n",
                        "  -H \"Content-Type: application/json\" \\\n",
                        "  -d '{\n",
                        "  \"model\": \"Qwen/Qwen3-8B-AWQ\",\n",
                        "  \"messages\": [\n",
                        "    {\"role\": \"user\", \"content\": \"请简要介绍一下大型语言模型。\"}\n",
                        "  ],\n",
                        "  \"temperature\": 0.7,\n",
                        "  \"top_p\": 0.8,\n",
                        "  \"top_k\": 20,\n",
                        "  \"max_tokens\": 1024,\n",
                        "  \"presence_penalty\": 1.5,\n",
                        "  \"chat_template_kwargs\": {\"enable_thinking\": false}\n",
                        "}'"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "**注意**：`enable_thinking` 参数并非 OpenAI API 标准参数。不同框架的具体实现可能有所不同。\n",
                        "\n",
                        "**提示**：\n",
                        "1. 如果希望完全禁用思考（即使用户使用 `/think` 指令也不思考），可以在启动服务器时指定自定义的“非思考”聊天模板。\n",
                        "2. 建议针对思考模式和非思考模式设置不同的采样参数。"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### 高级功能介绍\n",
                        "\n",
                        "#### 1. 解析思考内容 (Parsing Thinking Content)\n",
                        "SGLang 支持将思考内容解析为结构化消息。若需启用，需在启动命令中添加 `--reasoning-parser qwen3`。\n",
                        "启用后，响应消息将在 `content` 外包含 `reasoning_content` 字段。\n",
                        "\n",
                        "#### 2. 解析工具调用 (Parsing Tool Calls)\n",
                        "支持将工具调用内容解析为结构化消息，启动命令需添加 `--tool-call-parser qwen25`。\n",
                        "\n",
                        "#### 3. 结构化/JSON 输出 (Structured/JSON Output)\n",
                        "SGLang 支持结构化输出，详情请参考 [SGLang 文档](https://docs.sglang.ai/backend/structured_outputs.html#OpenAI-Compatible-API)。\n",
                        "\n",
                        "#### 4. 上下文长度与 YaRN\n",
                        "Qwen3 预训练上下文长度可达 32,768 token。SGLang 支持 [YaRN](https://arxiv.org/abs/2309.00071) 技术来扩展上下文长度（例如扩展至 131,072），但需注意这可能会影响短文本的性能，建议仅在确实需要处理超长文本时启用。"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### 服务资源清理\n",
                        "\n",
                        "完成实验后，建议停止后台运行的 SGLang 服务器，以释放系统资源。"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# 终止 SGLang 服务器进程\n",
                        "!pkill -f sglang.launch_server\n",
                        "print(\"SGLang 服务器已停止。\")"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "Python 3",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.10.12"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 5
}